{"meta":{"title":"鲈鱼柳的博客","subtitle":"为学日益，为道日损","description":null,"author":"鲈鱼柳","url":"https://zs-liu.github.io"},"pages":[{"title":"","date":"un33fin33","updated":"un22fin22","comments":true,"path":"google462fce11ee28ad9d.html","permalink":"https://zs-liu.github.io/google462fce11ee28ad9d.html","excerpt":"","text":"google-site-verification: google462fce11ee28ad9d.html"},{"title":"About","date":"un33fin33","updated":"un33fin33","comments":false,"path":"about/index.html","permalink":"https://zs-liu.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"un33fin33","updated":"un33fin33","comments":false,"path":"categories/index.html","permalink":"https://zs-liu.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"un33fin33","updated":"un33fin33","comments":false,"path":"tags/index.html","permalink":"https://zs-liu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Max Weight, Back Pressure and Utility Maximization","slug":"Max Weight, Back Pressure and Utility Maximization","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2019/09/16/Max Weight, Back Pressure and Utility Maximization/","link":"","permalink":"https://zs-liu.github.io/2019/09/16/Max Weight, Back Pressure and Utility Maximization/","excerpt":"Queue Control Problem \\(A_1(t),A_2(t),Q_1(t),Q_2(t),\\mu_1(t),\\mu_2(t)\\) \\(A_i(t)\\) is iid, \\(E\\{A_i(t)\\}=\\lambda_i\\), \\(A_i(t)\\in[0,A_{\\max}]\\) \\(S_i(t)\\) is condition of link \\(i\\), \\(S_i(t)\\in\\{0,1\\}\\) \\(P_{xy}=P_r\\{S_1(t)=x,S_2(t)=y\\}\\) At every time, serve \\(Q_1(t)\\) or \\(Q_2(t)\\) \\(\\mu_i(t)=1\\text{ if } S_i(t)=1,Q_i(t)&gt;0\\quad0\\text{ otherwise}\\) Goal: stabilize both queues \\(~\\)","text":"Queue Control Problem \\(A_1(t),A_2(t),Q_1(t),Q_2(t),\\mu_1(t),\\mu_2(t)\\) \\(A_i(t)\\) is iid, \\(E\\{A_i(t)\\}=\\lambda_i\\), \\(A_i(t)\\in[0,A_{\\max}]\\) \\(S_i(t)\\) is condition of link \\(i\\), \\(S_i(t)\\in\\{0,1\\}\\) \\(P_{xy}=P_r\\{S_1(t)=x,S_2(t)=y\\}\\) At every time, serve \\(Q_1(t)\\) or \\(Q_2(t)\\) \\(\\mu_i(t)=1\\text{ if } S_i(t)=1,Q_i(t)&gt;0\\quad0\\text{ otherwise}\\) Goal: stabilize both queues \\(~\\) Linear Program \\[\\max~e\\] \\[\\begin{split}s.t.~\\lambda_1+e&amp;\\leq{}P_{10}+P_{11}\\theta\\\\\\lambda_2+e&amp;\\leq{}P_{01}+(1-\\theta)P_{11}\\end{split}\\] optimal value \\(e^\\star&gt;0\\) LP policy is obvious If we know all \\(\\lambda_i\\) and \\(P_{xy}\\) ALG Max-Weight Policy is simple \\(S(t)=\\{1,0\\}\\), serve \\(Q_1\\) \\(S(t)=\\{0,1\\}\\), serve \\(Q_2\\) \\(S(t)=\\{1,1\\}\\), serve \\(\\max\\{Q_1,Q_2\\}\\) Without any infomation \\[L(t)=\\frac{1}{2}Q_1^2(t)+\\frac{1}{2}Q_2^2(t)\\] \\[Q_i(t+1)=(Q_i(t)-\\mu_i(t)+A_i(t))^+\\] \\[\\begin{split}\\Delta(t)&amp;=E\\{L(t+1)-L(t)|Q(t)\\}\\\\&amp;=\\frac{1}{2}E\\{Q_1^2(t+1)-Q_1^2(t)+Q_2^2(t+1)-Q_2^2(t)|Q(t)\\}\\\\&amp;\\leq{}B-E\\{Q_1(t)[\\mu_1(t)-A_1(t)]+Q_2(t)[\\mu_2(t)-A_2(t)]|Q(t)\\}\\\\&amp;=B+\\lambda_1Q_1(t)+\\lambda_2Q_2(t)-E\\{Q_1(t)\\mu_1(t)+Q_2(t)\\mu_2(t)|Q(t)\\}\\\\&amp;\\leq{}B+\\lambda_1Q_1(t)+\\lambda_2Q_2(t)-Q_1(t)[P_{10}+P_{11}\\theta]-Q_2(t)[P_{01}+P_{11}(1-\\theta)]\\\\&amp;=B-eQ_1(t)-eQ_2(t)\\end{split}\\] Capacity Region \\(\\lambda=(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)\\) Def: The capacity region \\(\\Lambda\\) is the closure of \\(\\lambda\\) under which exists an algorithm that can stabilize the system Claim: \\(\\Lambda=\\{(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)\\text{ s.t. }e(\\lambda)\\geq0\\}\\) Corollary: Max-weight stabilizes the system whenever \\(\\exists{}e\\geq0\\text{ s.t. } \\lambda+e\\in\\Lambda\\) So, in some way, Max-weight is &quot;Throughput Optimal&quot; Caratheodory's Theorem Let \\(X\\) be a subset of \\(R^d\\). If \\(x\\in\\text{conv}(X)\\), then \\(\\exists~d+1\\) points in \\(X\\) (i.e. \\(x_1,\\dots,x_{d+1}\\)), s.t. \\[x=\\sum_{i=1}^{d+1}\\alpha_ix_i,\\alpha_i\\geq0,\\sum_{i=1}^{d+1}\\alpha_i=1\\] Multi-Hop \\(V=\\{1,2,\\dots,7\\}\\) \\(E\\) are linkes \\(A_n^{(k)(t)}\\) is packages entering \\(n\\) for destination \\(k\\), \\(E\\{A_n^{(k)}(t)\\}=\\lambda_n^{(k)}\\in[0,A_{\\max}]\\) \\(S(t)=(S_{nm}(t),[n,m]\\in{}E)\\), \\(S_{n,m}\\in\\{0,1\\}\\), iid \\(\\mu_{nm}(t)=1\\text{ if }S_{nm}(t)=1\\text{ and use link }[n,m]\\quad0\\text{ otherwise}\\) No interference Queueing: \\(Q_n^{(k)}(t)\\) \\(Q_n^{(k)}(t+1)\\leq{}(Q_n^{(k)}(t)-\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t))^+{}+\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)+A_n^{(k)}(t)\\) \\(Q_n^{(0)}(t)=0\\) Goal: stabilize all queues Back Pressure At time \\(t\\), observe \\(Q_n^{(k)}(t)\\) for all \\(n,k\\) For each \\([n,m]\\), define weight \\(W_{nm}(t)=\\max_k[Q_n^{(k)}(t)-Q_m^{(k)}(t)]^+\\) and let \\(k^\\star=\\arg\\max{}W_{nm}^{(k)}(t)\\) If \\(W_{nm}(t)&gt;0\\), \\(\\mu_{nm}^{(k)}(t)=\\mu_{nm}(t)\\text{ if }k=k^\\star\\quad0\\text{ otherwise}\\) If \\(W_{nm}(t)=0\\), \\(\\mu_{nm}^{(k)}(t)=0\\) Analysis \\(Q(t)=(Q_n^{(k)}(t)),L(Q(t))=\\frac{1}{2}\\sum_{n,k}Q_n^{(k)}(t)^2\\) \\[\\begin{split}\\Delta(t)&amp;=E\\{L(t+1)-L(t)|Q(t)\\}\\\\&amp;\\leq{}B-E\\{\\sum_{n,k}Q_n^{(k)}(t)[\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t)-\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)-A_n^{(k)}(t)]|Q(t)\\}\\\\&amp;=B-\\sum_{n,m,k}E\\{\\mu_{nm}^{(k)}(t)[Q_n^{(k)}(t)-Q_m^{(k)}(t)]|Q(t)\\}+\\sum_{n,k}Q_n^{(k)}(t)\\lambda_n^{(k)}\\\\&amp;\\leq{}B-e\\sum_{nm}Q_n^{(k)}(t)\\end{split}\\] \\[\\max{}e\\] \\[\\begin{split}s.t.~\\lambda_n^{(k)}+e+\\sum_{[a,n]}f_{an}^{(k)}&amp;\\leq\\sum_{[n,m]}f_{nm}^{(k)}\\\\\\sum_kf_{nm}^{(k)}&amp;\\leq\\mu_{nm}\\\\f_{nm}^{(k)}&amp;\\geq0\\end{split}\\] Interference Problem \\[\\max~\\sum_{n,m,k}\\mu_{nm}^{(k)}(t)[Q_n^{(k)}(t)-Q_m^{(k)}(t)]^+\\] \\[s.t.~\\sum_k\\mu_{nm}^{(k)}(t)\\leq\\mu_{nm}(I(t),S(t)),I(t)\\text{ feasible}\\] Utility Maximization in Networks \\(A_n^{(k)}(t)\\): # of pkts arrivals to \\(n\\) for \\(k\\) at t \\(R_n^{(k)}(t)\\): # of pkts admitted (admission control), \\(0\\leq{}R_n^{(k)}(t)\\leq{}R_{\\max}\\) \\(S(t)=(S_{nm}(t),[n,m]\\in{}E)\\), \\(S_{n,m}\\in\\{0,1\\}\\), iid \\(Q_n^{(k)}(t+1)\\leq{}(Q_n^{(k)}(t)-\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t))^+{}+\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)+R_n^{(k)}(t)\\) \\(\\bar{r}_n^{(k)}=\\lim_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1}E\\{R_n^{(k)}(t)\\}\\) \\(g_n^{(k)}(\\cdot)\\) is utility function which is usually concave increasing Goal: \\[\\max\\sum_{n,k}g_n^{(k)}(\\bar{r}_n^{(k)}),\\text{ s.t. stability}\\] Lyapunor function \\[L(Q(t))=\\frac{1}{2}\\sum_{n,k}Q_n^{(k)}(t)^2\\] \\[\\begin{split}\\Delta(t)&amp;\\leq{}B-E\\{\\sum_{n,k}Q_n^{(k)}(t)[\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t)-\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)-R_n^{(k)}(t)]|Q(t)\\}\\\\\\text{subtract from both sides the term}&amp; \\\\ &amp;V\\sum_{n,k}E\\{g_n^{(k)}(R_n^{(k)}(t))|Q_n(t)\\}\\\\\\Delta(t)-V\\sum_{n,k}E\\{g_n^{(k)}(R_n^{(k)}(t))|Q_n(t)\\}&amp; \\leq{}B-\\sum_{n,k}E\\{Vg_n^{(k)}(R_n^{(k)}(t))-Q_n^{(k)}(t)R_n^{(k)}(t)|Q(t)\\}\\\\&amp;-\\sum_{n,m,k}E\\{\\mu_{nm}^{(k)}(t)[Q_n^{(k)}(t)-Q_m^{(k)}(t)]|Q(t)\\}\\end{split}\\] Cross-Longer Control At time \\(t\\), observe \\(S(t)\\) and \\(Q(t)\\) Admission control (Application): Choose \\(R_n^{(k)}(t)\\) \\[\\max{}Vg_n^{(k)}(R_n^{(k)}(t))-Q_n^{(k)}(t)R_n^{(k)}(t),\\text{ s.t. }0\\leq{}R_n^{(k)}(t)\\leq{}R_{\\max}\\] Scheduling and Routing (Transport): Define \\(W_{nm}(t)=\\max_k[Q_n^{(k)}(t)-Q_m^{(k)}(t)]^+\\), choose \\[\\mu_{nm}^{(k)}(t)=\\mu_{nm}(t)\\text{ if }k=k^\\star\\quad0\\text{ otherwise}\\] Resource Allocation (Physical): Choose \\(\\mu(t)\\) \\[\\max\\sum_{n,m}W_{nm}(t)\\mu_{nm}(t),\\text{ s.t. }\\mu(t)\\text{ is feasible}\\] Queue Independent Policy \\[\\begin{split}\\max&amp;\\sum_{n,k}g_n^{(k)}(r_n^{(k)})\\\\\\text{s.t. }r_n^{(k)}+\\sum_{a,n}f_{an}^{(k)}&amp;\\leq{}f_{nm}^{(k)}\\\\\\sum_kf_{nm}^{(k)}&amp;\\leq\\mu_{nm}\\\\\\mu=(\\mu_{nm},[n,m]\\in{}E)&amp;\\in\\sum_{S}\\beta_S\\text{conv}(\\Gamma_S)\\end{split}\\] \\(r_n^{(k)\\star}\\), \\(f_{nm}^{(k)\\star}\\), \\(\\mu_{nm}^{(k)\\star}\\) refers to a stationary randomized policy \\[\\Rightarrow{}\\Delta-V\\sum_{n,k}E\\{g_n^{(k)}(R_n^{(k)}(t))|Q_n(t)\\}\\leq{}B-V\\cdot\\text{opt}\\] Sum over \\(t=0,\\cdots,T-1\\), take expectation \\[\\begin{split}E\\{L(t)\\}-E\\{L(0)\\}-\\sum_tV\\sum_{n,k}E\\{g_n^{(k)}(R_n^{(k)}(t))|Q(t)\\}&amp;\\leq{}BT-VT\\cdot{}\\text{opt}\\\\\\sum_tV\\sum_{n,k}E\\{g_n^{(k)}(R_n^{(k)}(t))|Q(t)\\}&amp;\\geq{}-BT+VT\\cdot{}\\text{opt}-E\\{L(0)\\}\\end{split}\\] If consider \\(g_n^{(k)}(\\cdot)\\) as concave increasing, we can use Jensen Inequality to put the expectation sign inside. Utility Performance \\[\\sum_{n,k}g_n^{(k)}(r_n^{(k)}(t))\\geq{}\\text{opt}-\\frac{B}{V}\\] Delay Bound Assume \\(r_n^{(k)\\star}\\geq{}e&gt;0\\) \\[r_n^{(k)&#39;}=r_n^{(k)\\star}-e,~f_{nm}^{(k)&#39;}=f_{nm}^{(k)\\star}\\] \\[\\begin{split}\\Delta(t)-VE\\{\\sum_{n,k}g_n^{(k)}(R_n^{(k)}(t))|Q(t)\\}&amp;\\leq{}B-e\\sum_{n,k}Q_n^{(k)}(t)\\\\\\Delta(t)&amp;\\leq{}B+Vg_{\\max}-e\\sum_{n,k}Q_n^{(k)}(t)\\\\\\bar{Q}(t)&amp;\\leq\\frac{B+Vg_{\\max}}{e}\\sim\\mathcal{O}(V)\\end{split}\\]"},{"title":"Introduction to Queue and Lyapunov Analysis","slug":"Introduction to Queue and Lyapunov Analysis","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2019/09/09/Introduction to Queue and Lyapunov Analysis/","link":"","permalink":"https://zs-liu.github.io/2019/09/09/Introduction to Queue and Lyapunov Analysis/","excerpt":"Queue Definition Arrival: \\(A(t)\\) Cumulative arrival: \\[X[t_1,t_2]=\\int_{t_1}^{t_2}A(t)dt\\] Service: \\(\\mu(t)\\) Cumulative departure: \\[Y[t_1,t_2]=\\int_{t_1}^{t_2}Y(t)dt\\leq\\int_{t_1}^{t_2}\\mu(t)dt\\] \\(Y(t)=Q(t)~if~Q(t)&gt;0\\quad0~otherwise\\) \\(Q(t)=X[0,t]-Y[0,t],Q(0)=0\\)","text":"Queue Definition Arrival: \\(A(t)\\) Cumulative arrival: \\[X[t_1,t_2]=\\int_{t_1}^{t_2}A(t)dt\\] Service: \\(\\mu(t)\\) Cumulative departure: \\[Y[t_1,t_2]=\\int_{t_1}^{t_2}Y(t)dt\\leq\\int_{t_1}^{t_2}\\mu(t)dt\\] \\(Y(t)=Q(t)~if~Q(t)&gt;0\\quad0~otherwise\\) \\(Q(t)=X[0,t]-Y[0,t],Q(0)=0\\) Interval and Conserving Def: An interval \\(I=[t_1,t_2]\\) is a busy period if \\(Y(t)&gt;0,\\forall{}t\\in{}I\\) and \\(Y(t_1^-)=Y(t_2^+)=0\\) Def: A work conserving single server system is one where \\(Y(t)=\\mu(t)\\) wherever \\(Q(t)&gt;0\\) If we start from an empty system, and \\(Q(t)&gt;0\\), then \\[\\exists{}t^* \\text{ with } Q(t^*)=0 \\text{ s.t. } Q(t)=X[t^*,t]-\\int_{t^*}^t\\mu(t)dt\\] Stability Def: \\(X[0,t]\\) has a rate \\(\\lambda\\) if \\(\\lim_{t\\rightarrow\\infty}\\frac{X[0,t]}{t}=\\lambda\\) w.p.1 Def: \\(\\mu(t)\\) has rate \\(\\mu\\) if \\(\\lim_{t\\rightarrow\\infty}\\frac{\\int_0^t\\mu(t)}{t}=\\mu\\) w.p.1 Def: \\(Q(t)\\) is rate stable if \\(\\lim_{t\\rightarrow\\infty}\\frac{Q(t)}{t}=0\\) w.p.1 Def: \\(Q(t)\\) is mean rate stable if \\(\\lim_{t\\rightarrow\\infty}\\frac{E[Q(t)]}{t}=0\\) Theorem-Rate Stability Suppose \\(Q(t)=X[0,t]-Y[0,t]\\), and \\(X[0,t]\\) has rate \\(\\lambda\\) and \\(\\mu(t)\\) has rate \\(\\mu\\), then \\(Q(t)\\) is rate stable if and only if \\(\\lambda\\leq\\mu\\) \\[Q(t)\\text{ stable } \\rightarrow\\lambda\\leq\\mu\\] \\[\\frac{Q(t)}{t}=\\frac{X[0,t]}{t}-\\frac{Y[0,t]}{t}\\geq\\frac{X[0,t]}{t}-\\frac{\\int_0^t\\mu(t)}{t}\\] \\[Q(t)\\text{ stable } \\leftarrow\\lambda\\leq\\mu\\] Little's law \\[Q_{av}=\\lim_{t\\rightarrow\\infty}\\frac{1}{t}\\int_{\\tau=0}^tQ(\\tau)d\\tau,D_{av}=\\lim_{k\\rightarrow\\infty}\\frac{1}{k}\\sum_{k=1}^kD_k\\] \\[\\Rightarrow{}Q_{av}=\\lambda{}D_{av}\\] Lyapunor Analysis Definition Denote \\(\\vec{Q}(t)=(Q_1(t),\\dots,Q_k(t))\\), Def \\(L(\\vec{Q}(t))\\) is a lyapunor-function if \\(L(\\vec{Q}(t))\\geq0\\) \\(L(0)=0\\) Define one-slot conditional lyapunor drift \\[\\Delta(t)=E\\{L(\\vec{Q}(t+1))-L(\\vec{Q}(t))|\\vec{Q}(t)\\}\\] Theorem Suppose \\(L(\\vec{Q}(t))\\) is a lyapunor function and satisifes \\(\\Delta(t)\\leq{}B-e\\sum_kQ_k(t)\\) and \\(L(\\vec{Q}(0))&lt;\\infty\\), then \\(e&gt;0\\rightarrow{}Q(t)\\) is strongly stable \\[\\lim_{T\\rightarrow\\infty}\\sup\\frac{1}{T}\\sum_{t=0}^{T-1}\\sum_kE\\{Q_k(t)\\}\\leq\\frac{B}{e}\\] \\(e\\geq0\\) and \\(L(\\vec{Q})=\\sum_k{}w_kQ_k\\), then \\[\\lim_{t\\rightarrow\\infty}\\frac{E\\{Q_k(t)\\}}{t}=0,\\forall{}k\\] Proof is made by list the inequality from \\(t=0\\) to \\(t=T\\) and sum them up. For the first one, divide by \\(T\\) then is obvious. For the second one, put exception into square, the left side is \\(O(\\sqrt{T})\\). Example \\(A(t), \\mu(t), \\sim{}Bernoulli~\\lambda,\\mu\\) \\[Q(t+1)=(Q(t)-\\mu(t)+A(t))^+,L(Q(t))=\\frac{1}{2}Q^2(t)\\] \\[\\Delta(t)=\\frac{1}{2}E\\{Q^2(t+1)-Q^2(t)|Q(t)\\}\\leq\\cdots=E\\{\\frac{1}{2}(\\mu(t)-A(t))^2-Q(t)(\\mu(t)-A(t))|Q(t)\\}\\leq\\frac{1}{2}-Q(t)(\\mu-\\lambda)\\] \\[\\bar{Q}(t)\\leq\\frac{1}{2(\\mu-\\lambda)}\\]"},{"title":"Optimal Transport","slug":"Optimal Transport","date":"un66fin66","updated":"un22fin22","comments":true,"path":"2019/05/25/Optimal Transport/","link":"","permalink":"https://zs-liu.github.io/2019/05/25/Optimal Transport/","excerpt":"Wasserstein Distance Definition Consider, general functions \\(f\\) and \\(g\\), the Wasserstein distance is \\[\\min_{\\text{all map }T}\\{\\sum_{\\text{all movements of }T}\\text{distance moved}\\times\\text{amount moved}\\}\\] For \\(f:X\\rightarrow{}R^+,g:Y\\rightarrow{}R^+\\), the distance can be formulated as \\[W_p(f,g)=\\left(\\inf_{T\\in\\mathcal{M}}\\int{}|x-T(x)|^pf(x)dx\\right)^{1/p}\\] where \\(\\mathcal{M}\\) is the set of all maps that rearrange the distribution \\(f\\) into \\(g\\).","text":"Wasserstein Distance Definition Consider, general functions \\(f\\) and \\(g\\), the Wasserstein distance is \\[\\min_{\\text{all map }T}\\{\\sum_{\\text{all movements of }T}\\text{distance moved}\\times\\text{amount moved}\\}\\] For \\(f:X\\rightarrow{}R^+,g:Y\\rightarrow{}R^+\\), the distance can be formulated as \\[W_p(f,g)=\\left(\\inf_{T\\in\\mathcal{M}}\\int{}|x-T(x)|^pf(x)dx\\right)^{1/p}\\] where \\(\\mathcal{M}\\) is the set of all maps that rearrange the distribution \\(f\\) into \\(g\\). Quadratic Wasserstein distance: \\(p=2\\) \\[W_2^2(f,g)=\\inf_{T\\in\\mathcal{M}}\\int{}|x-T(x)|^pf(x)dx\\] Kantorovich Problem Definition \\[\\inf_\\gamma\\left\\{\\int_{X\\times{}Y}c(x,y)d\\gamma|\\gamma\\geq0,\\gamma\\in\\Pi(\\mu,\\nu)\\right\\}\\] where \\(\\Pi(\\mu,\\nu)=\\{\\gamma\\in\\mathcal{P}(X\\times{}Y)|(P_X)\\#\\gamma=\\mu,(P_Y)\\#\\gamma=\\nu\\}\\), \\(P_X\\) and \\(P_Y\\) are two projections Kantorovich Dual Problem Consider \\(\\varphi\\in{}L^1(\\mu)\\) and \\(\\psi\\in{}L^1(\\nu)\\), the Kantorovich dual problem is formulated as the following: \\[\\sup_{\\varphi,\\psi}\\left(\\int_X\\varphi{}d\\mu+\\int_Y\\psi{}d\\nu\\right)\\] subject to \\(\\varphi(x)+\\psi(y)\\leq{}c(x,y)\\), for any \\((x,y)\\in{}X\\times{}Y\\). Note that this dual formulation is a linear optimization problem which is solvable by linear programming. Dynamic Formulation The Benamou-Brenier formula identifies the squared quadratic Wasserstein metric between \\(\\mu\\) and \\(\\nu\\) by: \\[W_2^2(\\mu,\\nu)=\\inf\\int_0^1\\int|v(t,x)|^2\\rho(t,x)dxdt\\] where infimum is taken among all the Borel fields \\(v(t,x)\\) that transports \\(\\mu\\) to \\(\\nu\\) continuously in time, satisfying the zero flux condition on the boundary: \\[\\begin{split}\\frac{\\partial\\rho}{\\partial{}t}+\\nabla(v\\rho)&amp;=0\\\\\\text{subject to }\\rho(0,x)=d\\mu,\\rho(1,x)&amp;=d\\nu\\end{split}\\] Monge-Ampere Equation \\[\\left\\{\\begin{split}&amp;det(D^2u(x))=f(x)/g(\\nabla{}u(x))\\\\&amp;\\nabla{}u:X\\rightarrow{}Y\\\\&amp;u\\text{ is convex}\\end{split}\\right.\\] The optimal map is \\(\\nabla{}u\\). Thus, the square of the quadratic Wasserstein distance has the form: \\[W_2^2(f,g)=\\int{}|x-\\nabla{}u(x)|^2f(x)dx\\]"},{"title":"Proximal Mapping","slug":"Proximal Mapping","date":"un44fin44","updated":"un22fin22","comments":true,"path":"2019/04/25/Proximal Mapping/","link":"","permalink":"https://zs-liu.github.io/2019/04/25/Proximal Mapping/","excerpt":"Closed Closed set A set \\(\\mathcal{C}\\) is closed if it contains its boundary: \\[x^k\\in\\mathcal{C},x^k\\rightarrow\\bar{x}\\Rightarrow\\bar{x}\\in\\mathcal{C}\\] the intersection of closed sets is closed the union of a finite number of closed sets is closed inverse under linear mapping \\[\\{x|Ax\\in\\mathcal{C}\\}\\] is closed if \\(\\mathcal{C}\\) is closed","text":"Closed Closed set A set \\(\\mathcal{C}\\) is closed if it contains its boundary: \\[x^k\\in\\mathcal{C},x^k\\rightarrow\\bar{x}\\Rightarrow\\bar{x}\\in\\mathcal{C}\\] the intersection of closed sets is closed the union of a finite number of closed sets is closed inverse under linear mapping \\[\\{x|Ax\\in\\mathcal{C}\\}\\] is closed if \\(\\mathcal{C}\\) is closed Closed function a function is closed if its epigraph is a closed set or if all its sublevel set is a closed set If \\(f\\) is continuous and \\(dom~f\\) is closed, then \\(f\\) is closed If \\(f\\) is continuous and \\(dom~f\\) is open, then \\(f\\) is closed iff it converges to \\(\\infty\\) along every sequence converging to a boundary point of \\(dom~f\\) Properties sublevel sets: \\(f\\) is closed iff all its subsevel sets are closed minimum: if \\(f\\) is closed with bounded sublevel sets then it has a minimizer Conjugate function Conjugate functions: recall the conjugate of a function \\(f\\) is always closed and convex The indicator function of convex set \\(\\mathcal{C}\\): conjugate is support function of \\(\\mathcal{C}\\) Norm: conjugate is indicator of unit dual norm ball Second conjugate \\[f^{**}(x)=\\sup_{y\\in{}dom~f^*}(x^\\top{}y-f^*(y)))\\] \\(f^{**}\\) is closed and convex \\(f^{**}\\leq{}f(x)\\) if \\(f\\) is closed and convex, then \\(f^{**}\\leq{}f(x)\\) Calculus rules Separable sum \\[f(x_1,x_2)=g(x_1)+h(x_2),f^*(y_1,y_2)=g^*(y_1)+h^*(y_2)\\] Scalar multiplication \\[\\alpha{}&gt;0,f(x)=\\alpha{}g(x),f^*(y)=\\alpha{}g^*(y/\\alpha{})\\] Addition to affine function \\[f(x)=g(x)+a^\\top+b,f^*(y)=g^*(y-a)-b\\] Infimal convolution \\[f(x)=\\inf_{u+v=x}(g(u)+h(v)),f^*(y)=g^*(y)+h^*(y)\\] Proximal mapping Definition: the proximal mapping of a closed convex function \\(f\\) is: \\[\\text{prox}_f(x)=\\arg\\min_u\\left(f(u)+\\frac{1}{2}||u-x||_2^2\\right)\\] Example \\[u=\\text{prox}_f(x)\\Leftrightarrow{}0\\in\\partial{}f(u)+u-x\\Leftrightarrow{}x-u\\in\\partial{}f(u)\\Leftrightarrow{}f(z)\\geq{}f(u)+(x-u)^\\top(z-u)\\] If \\(f(x)=\\delta_C(x)\\), \\(C\\) is closed and convex \\(\\Rightarrow{}(x-u)^\\top(z-u)\\leq0\\) Calculus rules \\[f(x)=\\lambda{}g(x/\\lambda)\\Rightarrow{}\\text{prox}_f(x)=\\lambda\\text{prox}_{\\lambda^{-1}g}(x/\\lambda)\\] Proof \\[\\text{prox}_f(x)=\\arg\\min_y\\{f(y)+\\frac{1}{2}||y-x||^2\\}=\\arg\\min_{y,z}\\{\\lambda{}g(z)+\\frac{1}{2}||\\lambda{}z-x||_2^2\\left|z=\\frac{y}{\\lambda}\\}\\right.\\] \\[L(y,z;\\mu)=\\lambda{}g(z)+\\frac{1}{2}||\\lambda{}z-x||_2^2+&lt;\\lambda{}z-y,\\mu&gt;\\] \\[\\partial_yL=0,\\partial_zL=0\\Rightarrow{}z=\\text{prox}_{\\lambda^{-1}g}(x/\\lambda)\\] Moreau decomposition \\[x=\\text{prox}_f(x)+\\text{prox}_{f^*}(x)\\quad{}\\text{for all }x\\] Composition with affine mapping for general \\(A\\), prox-operator of \\(f\\) does not follow easily from prox-operator of \\(g\\) however, if \\(AA^\\top=(1/\\alpha)I\\) \\[f(x)=g(Ax+b)\\Rightarrow{}\\text{prox}_f(x)=x-\\alpha{}A^\\top(Ax+b-\\text{prob}_{\\alpha^{-1}g}(Ax+b))\\] Projections Affine sets \\(C=\\{x|Ax=b\\}\\), with \\(A\\in{}R^{p\\times{}n}\\) and \\(\\textbf{rank}(A)=p\\) \\[P_C(x)=x+A^\\top(AA^\\top)^{-1}(b-Ax)\\] inexpensive if \\(p\\ll{}n\\), or \\(AA^\\top=I\\) Simple polyhedral sets Halfspace \\(C=\\{x|a^\\top{}x\\leq{}b\\}\\) \\[P_C(x)=x+\\frac{b-a^\\top{}x}{||a||_2^2}a\\quad\\text{if }a^\\top{}x&gt;b,\\quad{}P_C(x)=x\\quad\\text{if }a^\\top{}x\\leq{}b\\] Probability simplex \\(C=\\{x|\\textbf{1}^\\top{}x=1,x\\succeq0\\}\\) \\[P_C(x)=(x-\\lambda\\textbf{1})_{+}\\] where \\(\\lambda\\) is the solution of the equation \\[\\textbf{1}^\\top(x-\\lambda\\textbf{1})_{+}=\\sum_{i=1}^n\\max\\{0,x_k-\\lambda\\}=1\\] Support function, Norm and Distance Support function conjugate of support function of closed convex set is indicator function \\[f(x)=\\sup_{y\\in{}C}x^\\top{}y,f^*(y)=\\delta_C(y)\\] prox-operator of support function follows from Moreau decomposition \\[\\text{prox}_{tf}(x)=x-t\\text{prox}_{t^{-1}f^*}(x/t)=x-tP_C(x/t)\\] Norm conjugate of norm is indicator function of dual norm \\[f(x)=||x||,f^*(y)=\\delta_B(y),B=\\{y|~||y||_*\\leq1\\}\\] prox-operator of norm follows from Moreau decomposition \\[\\text{prox}_{tf}(x)=-P_{tB}(x)\\] Euclidean distance to a set \\[d(x)=\\inf_{y\\in{}C}||x-y||_2\\] \\[\\text{prox}_{td}(x)=\\left\\{\\begin{aligned}&amp;x+\\frac{t}{d(x)}(P_C(x)-x)&amp;d(x)\\geq{}t\\\\&amp;P_C(x)&amp;~\\text{otherwise}\\end{aligned}\\right.\\]"},{"title":"Subgradient Method","slug":"Subgradient Method","date":"un22fin22","updated":"un22fin22","comments":true,"path":"2019/04/16/Subgradient Method/","link":"","permalink":"https://zs-liu.github.io/2019/04/16/Subgradient Method/","excerpt":"Subgradient \\(g\\) is a subgradient of a convex function \\(f\\) at \\(x\\in{}dom~f\\) if \\[f(y)\\geq{}f(x)+g^\\top(y-x)\\quad\\text{for all }y\\in{}dom~f\\]","text":"Subgradient \\(g\\) is a subgradient of a convex function \\(f\\) at \\(x\\in{}dom~f\\) if \\[f(y)\\geq{}f(x)+g^\\top(y-x)\\quad\\text{for all }y\\in{}dom~f\\] Properties \\(f(x)+g^\\top(y-x)\\) is a global lower bound on \\(f(y)\\) \\(g\\) defines non-vertical supporting hyperplane to \\(epi~f\\) at \\((x, f (x))\\) if \\(f\\) is convex and differentiable, then \\(\\nabla{}f(x)\\) is a subgradient of \\(g\\) at \\(x\\) Subdifferential the subdifferential \\(\\partial{}f(x)\\) of \\(f\\) at \\(x\\) is the set of all subgradients: \\[\\partial{}f(x)=\\{g|g^\\top(y-x)\\leq{}f(y)-f(x),\\forall{}y\\in{}dom~f\\}\\] Monotonicity the subdifferential of a convex function is a monotone operator: \\[(u-v)^\\top(x-y)\\geq0,\\text{for all }x,y,u\\in\\partial{}f(x),v\\in\\partial{}f(y)\\] Directional Derivative Introduction \\[f(x+d)=f(x)+\\nabla{}f(x)^\\top{}d\\] \\[\\text{If }\\nabla{}f(x)^\\top{}d&lt;0,d\\text{ is a descent direction}\\] Definition The directional derivative of \\(f\\) at \\(x\\) in the direction \\(y\\) is \\[f^{&#39;}(x;y)=\\lim_{a\\searrow{}0}\\frac{f(x+\\alpha{}y)-f(x)}{\\alpha}=\\lim_{t\\rightarrow\\infty}(tf(x+\\frac{y}{t})-tf(x))\\] \\(f^{&#39;}(x;y)\\) is the right derivative of \\(g(\\alpha)=f(x+\\alpha{}y)\\) at \\(\\alpha=0\\) Proof:Existence \\[h(\\alpha)=\\frac{f(x+\\alpha{}y)-f(x)}{\\alpha}\\] \\[x+\\alpha_1{}y=x+\\frac{\\alpha_1}{\\alpha_2}x-\\frac{\\alpha_1}{\\alpha_2}x+\\frac{\\alpha_1}{\\alpha_2}\\alpha_2y=(1-\\frac{\\alpha_1}{\\alpha_2})x+\\frac{\\alpha_1}{\\alpha_2}(x+\\alpha_2y)\\] \\[f(x+\\alpha_1{}y)\\leq(1-\\frac{\\alpha_1}{\\alpha_2})f(x)+\\frac{\\alpha_1}{\\alpha_2}f(x+\\alpha_2y)\\text{ (convex)}\\] \\[h(\\alpha_1)\\leq{}h(\\alpha_2)\\] Proof:Homogeneous \\[\\text{If }x\\in\\text{int dom}f\\Rightarrow{}\\text{dom}f^{&#39;}(x,y)=R^n\\] \\[\\text{Let }\\beta=\\alpha\\lambda,\\text{ it is easy to prove}\\] Directional derivative and subgradients For convex \\(f\\) and \\(x\\in\\text{int dom}f\\): \\[f^{&#39;}(x;y)=\\sup_{g\\in\\partial{}f(x)}g^\\top{}y\\] generalize \\(f^{&#39;}(x;y)\\) for differentiable functions implies that \\(f^{&#39;}(x;y)\\) exists for all \\(x\\in\\text{int dom}f\\), all \\(y\\) Proof \\[f^{&#39;}(x;y)=\\lim_{a\\searrow{}0}\\frac{f(x+\\alpha{}y)-f(x)}{\\alpha}\\geq\\lim_{a\\searrow{}0}\\frac{}{}\\] \\[\\text{Let }\\hat{g}\\in\\partial_yf^{&#39;}(x;y)\\] \\[\\lambda{}f^{&#39;}(x;v)=f^{&#39;}(x,\\lambda{}v)\\geq{}f^{&#39;}(x;y)+\\hat{g}^\\top(\\lambda{}v-y)\\]\\[\\Leftrightarrow{}\\lambda(f^{&#39;}(x,v)-\\hat{g}^\\top{}v)\\geq{}f^{&#39;}(x;y)-\\hat{g}^\\top{}y\\] \\[\\Leftrightarrow{}^{\\lambda\\rightarrow\\infty}f^{&#39;}(x,v)\\geq\\hat{h}^\\top{}v\\] \\[\\Leftrightarrow{}f(x,v)\\geq{}f(x)+f^{&#39;}(x,v)\\geq{}f(x)+\\hat{g}^\\top{}v\\Rightarrow{}\\hat{g}\\in\\partial{}f(x)\\] \\[\\Leftrightarrow{}^{\\lambda\\rightarrow0}f^{&#39;}(x,y)\\leq{}g^\\top{}y\\] Steepest descent direction Steepest descent direction at \\(x\\in\\text{int dom} f\\) is \\[\\Delta{}x_{nsd}=\\arg\\min_{||y||_2\\leq1}f^{&#39;}(x;y)\\] \\[\\text{(P) }\\min_yf^{&#39;}(x;y),s.t.||y||_2\\leq1\\] \\[\\text{(D) }\\max_g-||g||_2,s.t.g\\in\\partial{}f(x)\\] Proof if \\(f\\) is convex, \\(f(y)&lt;f(x),g\\in\\partial{}f(x)\\), then for small \\(t&gt;0\\): \\[||x-tg-y||_2^2=||x-y||_2^2-2tg^\\top(x-y)+t^2||g||_2^2\\leq||x-y||_2^2-2t(f(x)-f(y))+t^2||g||_2^2&lt;||x-y||_2^2\\] \\[f^\\star(x^\\star)=\\min{}f(x),x_{k+1}=x_k-tg_k,g_k\\in\\partial{}f(x_k)\\] \\[f(x_k)&gt;f(x^\\star),\\text{but }||x_{k+1}-x^\\star||&lt;||x_k-x^\\star||\\] Lemma 1 Let \\(x_0\\in{}int~dom~f\\), then \\[f \\text{ is convex }\\Rightarrow{}f\\text{ is continuous at }x_0\\] Proof \\[g(x)=f(x_0+x)-f(x_0), g(0)=0, g\\text{ is convex}, 0\\in~int~dom~g\\] \\[\\exists\\alpha~s.t.~x_0+\\alpha{}y_i\\in~dom~f\\] \\[\\{y_1,\\dots,y_{2n}\\}=\\{e_1,\\dots,e_n,-e_1,\\dots,-e_n\\}\\] ...... Subgradient Method to minimize a nondifferentiable convex function \\(f\\): choose \\(x^{(0)}\\) and repeat \\[x^{(k)}=x^{(k-1)}-t_kg^{(k-1)},k=1,2,\\dots\\] \\(g^{(k-1)}\\) is any subgradient of \\(f\\) at \\(x^{(k−1)}\\) Assumptions \\(f\\) has finite optimal value \\(f^\\star\\), minimizer \\(x^\\star\\) \\(f\\) is convex, \\(dom~f = R^n\\) \\(f\\) is Lipschitz continuous with constant \\(G &gt; 0\\) \\[|f(x)-f(y)|\\leq{}G||x-y||_2\\quad\\forall{}x,y\\] this is equivalent to \\(||g||_2\\leq{}G\\) for all \\(x\\) and \\(g\\in\\partial{}f(x)\\) Analysis the subgradient method is not a descent method the key quantity in the analysis is the distance to the optimal set with \\(x^+=x^{(i)},x=x^{(i-1)},g=g^{(i-1)},t=t_i\\): \\[||x^+-x^\\star||_2^2=||x-tg-x^\\star||_2^2\\] \\[=||x-x^\\star||_2^2-2tg^\\top(x-x^\\star)+t^2||g_2^2||\\leq||x-x^\\star||_2^2-2t(f(x)-f^\\star)+t^2||g_2^2||\\] combine inequalities for \\(i=1,\\dots,k\\), and define \\(f^{(k)}_{\\text{best}}=\\min_{0\\leq{}i\\leq{}k}f(x^{(i)})\\) \\[(f^{(k)}_{\\text{best}}-f^\\star)\\leq\\frac{||x^{(0)}-x^\\star||_2^2+\\sum_{i=1}^kt_i^2||g^{(i-1)}||_2^2}{2\\sum_{i=1}^kt_i}\\] Fixed step size: \\(t_i=t\\) \\[f^{(k)}_{\\text{best}}-f^\\star\\leq\\frac{||x^{(0)}-x^\\star||_2^2}{2kt}+\\frac{G^2t}{2}\\] does not guarantee convergence of \\(f^{(k)}_{\\text{best}}\\) Fixed step length: \\(t_i=s/||g^{(i-1)}||_2\\) \\[f^{(k)}_{\\text{best}}-f^\\star\\leq\\frac{G||x^{(0)}-x^\\star||_2^2}{2ks}+\\frac{Gs}{2}\\] does not guarantee convergence of \\(f^{(k)}_{\\text{best}}\\) Diminishing step size: \\(t_i\\rightarrow0\\) \\(f^{(k)}_{\\text{best}}\\) converges to \\(f^\\star\\) currently we don't know the speed of convergence large \\(t_i\\) may converge fast at first, but then smaller \\(t_i\\) should be used Optimal step size when \\(f^\\star\\) is known \\[t_i=\\frac{f(x^{(i-1)})-f^\\star}{||g^{(i-1)}||_2^2}\\] applying recursively (with \\(||x^{(0)}-x^\\star||_2\\leq{}R\\) and \\(||g^{(i)}||_2\\leq{}G\\)) gives \\[f^{(k)}_{\\text{best}}-f^\\star\\leq\\frac{GR}{\\sqrt{k}}\\]"},{"title":"故障模式影响及危害分析","slug":"故障模式影响及危害分析","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2019/04/15/故障模式影响及危害分析/","link":"","permalink":"https://zs-liu.github.io/2019/04/15/故障模式影响及危害分析/","excerpt":"概述 定义 Failure Mode, Effects and Criticality analysis 归纳分析方法：分析系统中每个设备所有可能的故障模式及对 系统造成的所有可能影响，并按每个故障模式的严重程度及发 生概率予以分类。 一种自下而上的归纳分析方法 FMEA和CA","text":"概述 定义 Failure Mode, Effects and Criticality analysis 归纳分析方法：分析系统中每个设备所有可能的故障模式及对 系统造成的所有可能影响，并按每个故障模式的严重程度及发 生概率予以分类。 一种自下而上的归纳分析方法 FMEA和CA 目的 在设计、生产和使用阶段发现影响系统可靠性的各薄弱环节 对薄弱环节提出改进措施，提高系统可靠性水平 步骤 系统定义 确定系统中进行FMECA的设备范围 系统功能任务的描述 确定系统及设备的故障判据 FMEA 代码：采用统一的编码体系对每个设备的每个故障模式进行编码 设备或功能标识：记录被分析设备或功能的名称标识 功能：简要描述设备的主要功能 故障模式：故障的表现形式，每一个产品有可能具有多种故障模式 损坏型、退化型、松脱型、失调型、堵塞或渗漏型、功能型等 故障原因 直接原因：导致设备功能故障的设备自身的那些物理、化学或生物变化过程等 间接原因：其他设备故障、环境因素和人为因素等外部原因 任务阶段与工作方式 任务可能包含多个阶段，设备可能需要参与一个或多个阶段 工作方式：可替换、有冗余等 故障影响 局部影响：某设备的故障模式对自身和与其所在约定层次相同的其他设备的使用、功能或状态的影响 高一层次影响：某设备的故障模式对其所在约定层次的高一层次设备的使用、功能或状态的影响。 最终影响：指系统中某设备的故障模式对初始约定层次设备的使用、功能或状态的影响。 严酷度等级 严酷度：设备故障模式所产生后果的严重程度 严酷度等级：应考虑故障造成的最坏潜在后果，根据最终可能出现的人员伤亡、系统损坏或经济损失的程度等来确定。 故障检测方法 包括事前检测与事后检测 目视检查、离机检测、原位测试等手段 补偿措施 设计补偿措施 设备发生故障时，能继续工作的冗余设备 安全或保险装置(如监控及报警装置) 可替换的工作方式(如备用或辅助设备) 可以消除或减轻故障影响的设计或工艺改进 操作人员补偿措施 特殊的使用和维护规程，尽量避免或预防故障的发生 一旦出现某故障后操作人员应采取的最恰当的补救措施 CA 危害性分析CA：按每个故障模式的严重程度及其发生概率所产生的综合影响对系统中的设备分级 分析方法 风险优先数法 危害性矩阵法 风险优先数法 \\[RPN=OPR\\times{}ESR\\] \\(OPR\\)：发生概率等级 \\(ESR\\)：影响严酷度等级 危害性矩阵法 故障模式频数比 \\(\\alpha\\)：设备某故障模式占其全部故障模式的百分比率 故障影响概率 \\(\\beta\\)：某故障模式发生后，导致确定的严酷度等级的最终影响的条件概率 故障模式危害度：评价设备单一故障模式危害性 \\[C_m(j)=\\alpha\\beta\\lambda{}pt\\] 设备危害度：评价设备的危害性 \\[C_r(j)=\\sum{}C_{mi}(j)\\] 结果呈现 可靠性关键设备清单 RPN 值大于某规定值的设备 危害性矩阵中落在某一规定区域之内的设备 严重故障模式清单 严酷度为 Ⅰ、Ⅱ 类的故障模式 故障影响严重程度被评为 9-10 分的故障模式 单点故障模式清单 某一设备或故障模式发生后将直接导致系统故障 小结 FMECA 中各故障模式的相关数据是定量化分析的基础 FMECA 是静态的、单一因素的分析方法，在动态方面还很不完善"},{"title":"LP, SOP and SOCP","slug":"LP, SDP and SOCP","date":"un22fin22","updated":"un22fin22","comments":true,"path":"2019/04/02/LP, SDP and SOCP/","link":"","permalink":"https://zs-liu.github.io/2019/04/02/LP, SDP and SOCP/","excerpt":"Linear Programming \\[\\text{(P) }\\min_xc^Tx,s.t.Ax=b,x\\geq0\\] \\[\\text{(D) }\\min_yb^Ty,s.t.A^Ty+s=c,s\\geq0\\] Strong duality \\[c^Tx=b^T\\Leftrightarrow{}x^Ts=0\\Leftrightarrow{}x_is_i=0\\]","text":"Linear Programming \\[\\text{(P) }\\min_xc^Tx,s.t.Ax=b,x\\geq0\\] \\[\\text{(D) }\\min_yb^Ty,s.t.A^Ty+s=c,s\\geq0\\] Strong duality \\[c^Tx=b^T\\Leftrightarrow{}x^Ts=0\\Leftrightarrow{}x_is_i=0\\] KKT system in LP Primal feasibility \\[Ax=b\\land{}x\\geq0\\] Dual feasibility \\[A^Ty+s=c\\land{}s\\geq0\\] Complementarity \\[x_is_i=0\\] Algebraic Characterization Define \\(x\\circ{}s=(x_1s_1,\\dots,x_ns_n)^\\top\\) and \\[L_x:y\\mapsto(x_1y_1,\\dots,x_ny_n)^\\top,\\text{ i.e. }L_x={\\rm Diag}(x)\\] Semidefinite programming (SDP) \\[\\text{(P) }\\min\\langle{}C_1,X_1\\rangle{}+\\dots+\\langle{}C_n,X_n\\rangle{}\\] \\[s.t.~\\langle{}A_{i1},X_1\\rangle{}+\\dots+\\langle{}A_{in},X_n\\rangle{}=b_i,X_i\\succeq0\\] \\[\\text{(D) }\\max{}b^\\top{}y\\] \\[s.t.~A_{1i}b_1+\\dots+A_{ni}b_n+S_i=C_i,S_i\\succeq0\\] Strong duality \\[\\langle{}X,S\\rangle{}=0\\Leftrightarrow{}X\\succeq0\\land{}S\\succeq0\\land\\frac{XS+SX}{2}=0\\] Second order cone programming (SOCP) \\[\\text{(P) }\\min{}c^\\top{}x,s.t.Ax=b,x\\succeq_{\\mathcal{Q}}0\\] \\[\\text{(Q) }\\min{}b^\\top{}y,s.t.A^\\top{}y+s=c,s\\succeq_{\\mathcal{Q}}0\\] Example \\[x\\in{}S,\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_n\\] \\[(\\lambda_1+\\dots+\\lambda_K)(X)\\] \\[\\Leftrightarrow\\] \\[\\min_{Y,t}Tr(Y)+Kt,~s.t.tI+Y\\succeq{}X,Y\\succeq0\\] SDP Relaxation Consider QCQP \\[\\min~x^TA_0x+2b_0^Tx+c_0\\text{ assume }A_i\\in{}S^n\\] \\[\\text{s.t. }x^TA_ix+2b_i^Tx+c_i\\leq0,i=1,\\dots,m\\] Max Cut For graph \\((V,E)\\) and weights \\(w_{ij}=w_{ji}\\geq0\\), the maxcut problem is: \\[(Q)~\\max_x\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-x_ix_j),~\\text{ s.t. }x_i\\in\\{-1,1\\}\\] where \\(x_i=-1\\) means \\(x\\) in set \\(X_1\\) and \\(x_i=1\\) means \\(x\\) in set \\(X_2\\). And its relaxation is: \\[(P)~\\max_{v_i\\in{}R^n}\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-v_iv_j),~\\text{ s.t. }||v_i||_2=1\\] The equivalent SDP of \\((P)\\) is : \\[(SDP)~\\max_{X\\in{}S^n}\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-X_{ij}),~\\text{ s.t. }X_{ii}=1,X\\succeq0\\] where \\(X=V^TV=(v_1,\\dots,v_n)^T(v_1,\\dots,v_n)\\). Add \\(rank(X)=1\\), then \\(X=xx^T\\Rightarrow{}x_i^2=1\\), which means that it is equivalent to the original problem. Greedy Way Every time we pick most distant one, then \\[Z^*\\geq\\frac{1}{2}Z_{opt}\\] Rounding Procedure Generate a vector \\(r\\) uniformly distributed on the unit sphere, i.e. \\(||r||_2=1\\) Set \\(x_i=1(v_i^Tr\\geq0),-1(\\text{otherwise})\\) Theoretical Result Let \\(W\\) be the objective function value of \\(x\\) and \\(E(W)\\) be the expected value. Then \\[E(W)=\\frac{1}{\\pi}\\sum_{i&lt;j}w_{ij}\\arccos(v_i^Tv_j)\\] Goemans and Williamson showed: \\[E(W)\\geq\\alpha\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-v_i^Tv_j),\\alpha=\\min_{0\\leq\\theta\\leq\\pi}\\frac{2}{\\pi}\\frac{\\theta}{1-\\cos\\theta}&gt;0.878\\] SDP Representablity A set \\(X\\subseteq{}R^n\\) is SDP-representable (or SDP-Rep for short) if it can be expressed linearly as the feasible region of an SDP: \\[X=\\{x|(\\exists{}u\\in{}R^k)(\\exists{}A_i,B_j,C\\in{}R^{m\\times{}m})\\sum_ix_iA_i+\\sum_ju_jB_j+C\\succeq0\\}\\] A function \\(f(x)\\) is SDP-Rep if its epigraph is SDP-representable: \\[\\text{epi}(f)=\\{(x_0,x)|f(x)\\leq{}x_0\\}\\] If \\(X\\) is SDP-Rep, then \\(\\min_{x\\in{}X}c^Tx\\) is an SDP If \\(f(x)\\) is SDP-Rep, then \\(\\min_xf(x)\\) is an SDP 'Calculus' of SDP-Rep Sets If \\(X\\) and \\(Y\\) are SDP-Rep then so are Minkowski sum \\(X+Y\\) Intersection \\(X\\cap{}Y\\) Affine pre-image \\(A^{-1}(X)\\) if \\(A\\) is affine Affine map \\(A(X)\\) if \\(A\\) is affine Cartesian Product \\(X\\times{}Y=\\{(x,y)|x\\in{}X,y\\in{}Y\\}\\) Proof of Affine map \\[Y=\\{Fx+f|F\\in{}R^{m\\times{}n},x\\in{}X\\}\\] \\[m\\geq{}n,rank(F)=n\\] \\[y=Fx+f\\Rightarrow{}x=(F^TF)^{-1}F^T(y-f)=F^+(y-f)\\] \\[A(F^+(y-f))+B(u)+C\\geq0\\Rightarrow{}A(F^+y)+B(u)+C-A(F^+f)\\geq0\\] \\[m\\leq{}n,rank(F)=m\\] \\[y=Fx\\Rightarrow{}y=[F_1,F_2][x_1,x_2]^T\\Rightarrow{}x_1=F^{-1}(y-F_2x_2)\\] 'Calculus' of SDP-Rep Functions If functions \\(f_i,i=1,\\dots,m\\) and \\(g\\) are SDP-Rep. Then the following are SDP-Rep: nonnegative sum \\(\\sum\\alpha_if_i\\) for \\(\\alpha_i\\geq0\\) maximum \\(\\max_if_i\\) composition \\(g(f_1(x),\\dots,f_m(x))\\) Legendre transform \\(f^*(y)=\\max_xy^Tx-f(x)\\) Positive Polynomials The set of nonnegative polynomials of a given degree forms a proper cone \\[P_n=\\{(p_0,\\dots,p_n)|(\\forall{}t\\in{}I)p_o+p_1t+\\dots+p_nt^n&gt;0\\}\\] The cone of positive polynomials is SDP-Rep"},{"title":"可修复系统的可靠性分析","slug":"可修复系统的可靠性分析","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2019/04/01/可修复系统的可靠性分析/","link":"","permalink":"https://zs-liu.github.io/2019/04/01/可修复系统的可靠性分析/","excerpt":"维修性特征量 维修分布函数/维修度 产品从故障开始到修理完毕经历的时间 \\(Y\\) \\[M(t)=P(Y\\leq{}t)\\] 修复率 尚未修复的产品在单位时间内修复完成 \\[\\mu(t)=\\frac{m(t)}{1-M(t)}\\] 平均修复时间 \\[MTTR=\\int_0^\\infty{}tm(t)dt=\\int_0^\\infty{}tdM(t)\\]","text":"维修性特征量 维修分布函数/维修度 产品从故障开始到修理完毕经历的时间 \\(Y\\) \\[M(t)=P(Y\\leq{}t)\\] 修复率 尚未修复的产品在单位时间内修复完成 \\[\\mu(t)=\\frac{m(t)}{1-M(t)}\\] 平均修复时间 \\[MTTR=\\int_0^\\infty{}tm(t)dt=\\int_0^\\infty{}tdM(t)\\] 可用性 也称有效性，综合反映可靠性和维修性，即可维修产品使用效率的广义可靠性 规定条件包括工作条件和维修条件 特征量 瞬时可用度 可维修产品在某时刻具有或维持功能的概率 瞬时可用度常用于理论分析 平均可用度 \\[\\bar{A}(t_1,t_2)=\\frac{1}{t_2-t_1}\\int_{t_1}^{t_2}A(t)dt\\] 稳态可用度 \\[A=A(\\infty)=\\lim_{t\\rightarrow\\infty}A(t)=\\frac{MTBF}{MTBF+MTTR}\\] 当可靠度函数和维修度函数均是指数函数： \\[A=\\frac{\\mu}{\\mu+\\lambda}\\] 预防维修与事后维修 预防维修 计划性的维修活动，最常用的形式是定期检修，使设备总是 保持良好的工作状态 按照规定程序，假定起始时间为 \\(0\\)，检修时间间隔为 \\(T\\)，则产品工作时间 \\(t=jT+\\tau\\) 理想预防维修系统的可用度 平均预防维修时间 \\(T_p\\) ；平均事后维修时间 \\(T_f\\) 系统平均停工时间 \\(MDT=R(T)T_p+(1-R(T))T_f\\) 系统平均工作时间 \\(MUT=\\int_0^TR(t)dt\\) \\[A=\\frac{MUT}{MUT+MDT}\\] 事后维修 系统投入运行后，一旦发生故障，立即开始修理 系统只存在运行状态 \\(S\\) 和失效状态 \\(F\\) 状态转移关系 \\[P_F(t+\\Delta{}t)=P_S(t)\\lambda(t)\\Delta{}t+P_F(t)[1-\\mu(t)\\Delta{}t]\\] \\[P_F(t)=\\exp\\{-Q(t)\\}\\int_0^t\\exp\\{Q(t)\\}\\lambda(t)dt,Q(t)=\\int_0^t[\\lambda(t)+mu(t)]dt\\] 马尔科夫模型求解可修复系统可靠性 马尔科夫过程 \\[P(X(t_n)=x_n|X(t_1)=x_1,\\dots,X(t_{n-1})=x_{n-1})=P(X(t_n)=x_n|X(t_{n-1})=x_{n-1})\\] 随机过程 \\(\\{X(t),t\\geq0\\}\\) 是连续时间和离散状态空间 \\(S=\\{0,1,2,\\dots,n\\}\\) 的马尔科夫过程，若满足如下条件（即转移密度为常数），则称此过程为齐次马尔科夫过程： \\[P(X(t+\\Delta{}t)=j|X(t)=i)\\equiv{}p_{ij}(\\Delta{}t)\\] 由归一化条件，显然有： \\[p_{ii}+\\sum_{j\\neq{}i}p_{ij}=1\\] 转移率 \\[q_{ij}=\\lim_{\\Delta{}t\\rightarrow0}\\frac{p_{ij}(\\Delta{}t)}{\\Delta{}t},q_{i}=\\lim_{\\Delta{}t\\rightarrow0}\\frac{1-p_{ii}(\\Delta{}t)}{\\Delta{}t}\\] 由全概率条件，显然有： \\[q_i=\\sum_{j\\neq{}i}q_{ij}\\] 转移率矩阵为： \\[a_{ij}=q_{ij}~(i\\neq{}j)~\\text{or}~-q_i~(i=j)\\] 利用转移率矩阵求解系统可靠性参数 状态概率：系统在 \\(t\\) 时刻处于状态 \\(i\\) 的概率 可用度：瞬时可用度、稳态可用度 系统可靠度、平均首次故障时间 状态频率、状态持续时间 系统状态概率 令 \\[P(t)=[p_1(t),p_2(t),\\dots,p_n(t)]\\] 则有： \\[P^{&#39;}(t)=P(t)A\\] 等式两边使用拉普拉斯变换 \\[sP^*(s)-P(0)=P^*(s)A\\] \\[P^*(s)=P(0)(sI-A)^{-1}\\] 然后进行分式分解（\\(s_i\\) 为矩阵 \\(sI-A\\) 第 \\(i\\) 个特征值） \\[P^*_j(s)=\\sum_{i=0}^Nb_{ji}(s-s_i)^{-1}\\] 从而可以得到原函数 \\[P_j(t)=\\sum_{i=0}^Nb_{ji}\\exp(s_it)\\] 系统可用度 系统的瞬时可用度 \\(A(t)\\) \\[A(t)=\\sum_{j\\in{}W}p_j(t)\\] 系统稳态可用度 \\(A(\\infty)\\) \\[(\\pi_0,\\dots,\\pi_n)A=(0,\\dots,0),\\pi_0+\\dots+\\pi_n=1\\] \\[A(\\infty)=\\sum_{j\\in{}W}\\pi_j\\] 系统可靠度与平均首次故障前时间MTTFF 状态频率和持续时间"},{"title":"Duality","slug":"Duality","date":"un22fin22","updated":"un22fin22","comments":true,"path":"2019/03/26/Duality/","link":"","permalink":"https://zs-liu.github.io/2019/03/26/Duality/","excerpt":"Lagrange dual problem Lagrangian \\[\\min{}f_0(x),s.t.f_i(x)\\leq0,h_i(x)=0\\] variable \\(x\\in{}R^n\\), domain \\(\\mathcal{D}\\), optimal value \\(p^\\star\\) \\[L:R^n\\times{}R^m\\times{}R^p\\rightarrow{}R,dom~L=\\mathcal{D}\\times{}R^m\\times{}R^p\\] \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\]","text":"Lagrange dual problem Lagrangian \\[\\min{}f_0(x),s.t.f_i(x)\\leq0,h_i(x)=0\\] variable \\(x\\in{}R^n\\), domain \\(\\mathcal{D}\\), optimal value \\(p^\\star\\) \\[L:R^n\\times{}R^m\\times{}R^p\\rightarrow{}R,dom~L=\\mathcal{D}\\times{}R^m\\times{}R^p\\] \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] Lagrange dual function \\[g:R^m\\times{}R^p\\rightarrow{}R\\] \\[g(\\lambda,\\nu)=\\inf_{x\\in{}D}L(x,\\lambda,\\nu)=\\inf_{x\\in{}D}(f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x))\\] lower bound property: if \\(\\lambda\\geq0\\), then \\(g(\\lambda,\\nu)\\leq{}p^\\star\\) Lagrange dual problem \\[\\max{}g(\\lambda,\\nu),s.t.\\lambda\\succeq0\\] optimal value \\(d^\\star\\) Weak and strong duality weak duality: \\(d^\\star\\leq{}p^\\star\\) always holds (for convex and nonconvex problems) strong duality: \\(d^\\star=p^\\star\\) (usually) holds for convex problems Slater's constraint qualification \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }f_i(x)\\leq0,Ax=b\\] strong duality holds if it is strictly feasible, i.e., \\[\\exists{}x\\in{}\\text{relint }D:~f_i(x)&lt;0,Ax=b\\] there exist many other types of constraint qualifications Geometric interpretation Omit Complementary slackness Assume strong duality holds, \\(x^\\star\\) is primal optimal, \\((\\lambda^\\star,\\nu^\\star)\\) is dual optimal - \\(x^\\star\\) minimizes \\(L(x,\\lambda^\\star,\\nu^\\star)\\) - \\(\\lambda_i^\\star{}f_i(x^\\star)=0\\) for \\(i=1,\\dots,m\\) KKT conditions primal constraints: \\(f_i(x)\\leq0,i=1,\\dots,m,h_i(x)=0,i=1,\\dots,p\\) dual constraints: \\(\\lambda\\succeq0\\) complementary slackness: \\(\\lambda_if_i(x)=0\\) gradient of Lagrangian with respect to \\(x\\) vanishes: \\[\\nabla{}f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla{}f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla{}h_i(x)=0\\] KKT conditions for convex problem If \\(\\tilde{x},\\tilde{\\lambda},\\tilde{\\nu}\\) satisfy KKT for a convex problem, then they are optimal. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda, \\nu\\) that satisfy KKT conditions. Perturbation and sensitivity analysis \\[f_i(x)\\leq0\\Rightarrow{}f_i(x)\\leq{}u_i\\] \\[h_i(x)=0\\Rightarrow{}h_i(x)=v_i\\] \\[g(\\lambda,\\nu)\\Rightarrow{}g(\\lambda,\\nu)-u^T\\lambda-v^T\\nu\\] \\[p^\\star(u,v)\\geq{}g(\\lambda^\\star,\\nu^\\star)-u^T\\lambda^\\star-v^T\\nu^\\star=p^\\star(0,0)-\\lambda^\\star-\\nu^\\star\\] Local sensitivity If \\(p^\\star(u,v)\\) is differentiable at \\((0,0)\\), then \\[\\lambda_i^\\star=-\\frac{\\partial{}p^\\star(0,0)}{\\partial{}u_i},\\nu_i^\\star=-\\frac{\\partial{}p^\\star(0,0)}{\\partial{}v_i}\\] Duality and problem reformulations Introducing new variables and equality constraints \\[\\text{minimize }f_0(Ax+b)\\] dual function is constant we have strong duality, but dual is quite useless \\[\\Rightarrow\\text{minimize }f_0(y)\\text{, subject to }Ax+b-y=0\\] Implicit constraints \\[\\text{minimize }c^Tx\\text{, subject to }Ax=b,-1\\preceq{}x\\preceq{}1\\] \\[\\Rightarrow\\text{minimize }f_0(x)=c^Tx(-1\\preceq{}x\\preceq{}1),\\infty(\\text{otherwise})\\text{, subject to }Ax=b\\] Problems with generalized inequalities \\[\\text{minimize }f_0(x)\\text{, subject to }f_i(x)\\preceq{}_{K_i}0,h_i(x)=0\\] Semidefinite program \\[\\text{minimize }c^Tx,\\text{, subject to }x_1F_1+\\dots+x_nF_n\\preceq{}G\\] Lagrange multiplier is matrix \\(Z\\in{}S^k\\) Lagranigian \\[L(x,Z)=c^Tx+tr(Z(x_1F_1+\\dots+x_nF_n-G))\\] dual function \\[g(Z)=\\inf_xL(x,Z)=-tr(GZ)(\\text{for }tr(F_iZ)+c_i=0),-\\infty(\\text{otherwise})\\]"},{"title":"Simplex Algorithm","slug":"Simplex Algorithm","date":"un55fin55","updated":"un22fin22","comments":true,"path":"2019/03/22/Simplex Algorithm/","link":"","permalink":"https://zs-liu.github.io/2019/03/22/Simplex Algorithm/","excerpt":"Convert LP to a Standard Form \\[\\min{}c^\\top{}x,\\text{subject to }Ax=b\\land{}x\\geq0\\] For Inequality \\(x+y\\geq{}a\\rightarrow{}x+y-z=a,z\\geq0\\) \\(x+y\\leq{}a\\rightarrow{}x+y+s=a,s\\geq0\\) Unrestricted \\(\\rightarrow{}x=y-z,y\\geq0,z\\geq0\\)","text":"Convert LP to a Standard Form \\[\\min{}c^\\top{}x,\\text{subject to }Ax=b\\land{}x\\geq0\\] For Inequality \\(x+y\\geq{}a\\rightarrow{}x+y-z=a,z\\geq0\\) \\(x+y\\leq{}a\\rightarrow{}x+y+s=a,s\\geq0\\) Unrestricted \\(\\rightarrow{}x=y-z,y\\geq0,z\\geq0\\) Preview of Simplex Algorithm Basic solutions \\[Ax=b,A\\in{}R^{m\\times{}n},x\\in{}R^n,n\\geq{}m\\] Set \\(n-m\\) variables to \\(0\\) and solving for the remaining \\(m\\) variables. The columns for the remaining \\(m\\) variables are linear independent. \\[\\text{Total number: }C_n^m\\] Basic feasible solutions Feasible region for any LP problem is a convex set If a LP has an optimal solution, there must be an extreme point of the feasible region that is optimal Proof \\[\\text{Suppose }x^\\star=\\sum_{i=1}^n\\alpha_ix_i\\text{ is an optimal solution}\\] \\[c^\\top{}x^\\star=c^\\top{}\\sum_{i=1}^n\\alpha_ix_i&gt;\\sum_{i=1}^n\\alpha_ic^\\top{}x^\\star=c^\\top{}x^\\star\\] Adjacent Basic Feasible Solutions Two basic feasible solutions are said to be adjacent if their sets of basic variables have \\(m-1\\) basic variables in common General Description of the Simplex Algorithm Find an initial bfs of LP Change bfs to its adjacent bfs Recalculate by Gaussian elimination For maximum, until the first row is all nonnegative; For minimum, until the first row is all nonpositive Example Standard form: \\[\\max{}z=3x_1+5x_2\\] \\[\\begin{split} \\text{subject to }\\quad{}x_1+s_1&amp;=8\\\\ 2x_2+s_2&amp;=12\\\\ 3x_1+4x_2+s_3&amp;=36\\\\ x_1,x_2,s_1,s_2,s_3&amp;\\geq0 \\end{split} \\label{p1s}\\] Choose \\((s_1,s_2,s_3)\\) as BFS, we have table: \\(z\\) \\(x_1\\) \\(x_2\\) \\(s_1\\) \\(s_2\\) \\(s_3\\) rhs Basic Variable 1 -3 -5 0 0 0 0 \\(z=0\\) 0 1 0 1 0 0 8 \\(s_1=8\\) 0 0 2 0 1 0 12 \\(s_2=12\\) 0 3 4 0 0 1 36 \\(s_3=36\\) Choose \\((s_1,x_2,s_3)\\) as BFS, we have table: \\(z\\) \\(x_1\\) \\(x_2\\) \\(s_1\\) \\(s_2\\) \\(s_3\\) rhs Basic Variable 1 0 0 0 2.5 0 30 \\(z=30\\) 0 1 0 1 0 0 8 \\(s_1=8\\) 0 0 1 0 0.5 0 6 \\(x_2=6\\) 0 3 0 0 -2 1 12 \\(s_3=12\\) Choose \\((s_1,x_2,x_1)\\) as BFS, we have table: \\(z\\) \\(x_1\\) \\(x_2\\) \\(s_1\\) \\(s_2\\) \\(s_3\\) rhs Basic Variable 1 0 0 0 0.5 0 42 \\(z=42\\) - - - - - - - - 0 0 1 0 0.5 0 12 \\(x_2=6\\) 0 1 0 0 -2/3 1/3 4 \\(x_1=4\\) So the final result is \\[x_1=4,x_2=6,z_{max}=42\\] Degeneracy An LP is degenerate if it has at least one basic feasible solution in which a basic feasible variable is equal to \\(0\\) pivoting may not improve the objective value simplex method may end up in cycles Two BFS Methods Big M Method \\[\\min{}f(x),\\text{ subject to }Ax=b(b\\succeq0)\\] \\[\\Rightarrow\\min{}f(x)+M^\\top{}a,\\text{ subject to }Ax+Ia=b\\] Introduce \\(a_i\\) to every row which has a negative coefficient (on \\(s\\) or \\(z\\)), then we can get bfs easily \\[(x=0,a=b)\\] If we can't reduce all parameters of \\(a\\) to 0, then the original problem is infeasible Two-Phase Method \\[\\min{}f(x),\\text{ subject to }Ax=b(b\\succeq0)\\] \\[\\Rightarrow\\min{}I^\\top{}a,\\text{ subject to }Ax+Ia=b,a\\succeq0\\] If \\(a^\\star\\neq0\\), then the original problem is infeasible"},{"title":"Convex Optimization Problems","slug":"Convex Optimization Problems","date":"un22fin22","updated":"un22fin22","comments":true,"path":"2019/03/19/Convex Optimization Problems/","link":"","permalink":"https://zs-liu.github.io/2019/03/19/Convex Optimization Problems/","excerpt":"Optimization problem in standard form \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }f_i(x)\\leq0\\quad{}\\land{}\\quad{}Ax=b\\]","text":"Optimization problem in standard form \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }f_i(x)\\leq0\\quad{}\\land{}\\quad{}Ax=b\\] Theorem: Optimal and locally optimal options Any locally optimal point of a convex problem is (globally) optimal. #### Proof Suppose \\(x\\) is locally optimal, but there exists a feasible \\(y\\) with \\(f_0(y)&lt;f_0(x)\\), for \\(z\\) in the small neighbour of \\(x\\), contradiction must exist Theorem \\(x\\) is optimal if and only if it is feasible and \\[\\nabla{}f_0(x)^T(y-x)\\geq0\\quad\\text{for all feasible }y\\] Proof \\[f_0(y)\\geq{}f_0(x)+\\nabla{}f_0(x)^T(y-x)\\geq{}f_0(x)\\] \\[\\text{Suppose }\\exists{}y,~s.t.~\\nabla{}f_0(x)^T(y-x)&lt;0\\] \\[g(t)=f(x+y(y-x)),g^{&#39;}(0)=\\nabla{}f_0(x)^T(y-x)&lt;0\\] \\[\\exists{}\\bar{t},g(\\bar{t})&lt;g(0)=f_0(x)\\] Also, we can view this with Taylor expansion Equivalent convex problems Eliminating equality constrains \\[Ax=b\\rightarrow{}x=Fz+x_0~\\text{for some }z\\] Introducing equality constraints \\[y_i=A_ix+b\\] Introducing slack variables \\[a_i^Tx\\leq{}b_i\\rightarrow{}a_i^Tx+s_i=b_i\\land{}s_i\\geq0\\] Quasiconvex optimization If \\(f_0\\) is quasiconvex, there exists a family of functions \\(\\phi_t\\) such that: \\(\\phi_t(x)\\) is convex in \\(x\\) for fixed \\(t\\) t-sublevel set of \\(f_0\\) is 0-sublevel set of \\(\\phi_t\\), i.e. \\[f_0(x)\\leq{}t\\leftrightarrow{}\\phi_t(x)\\leq0\\] Linear program (LP) \\[\\text{minimize }c^Tx+d\\] \\[\\text{subject to }Gx\\leq{}h\\land{}Ax=b\\] convex problem with affine objective and constraint functions feasible set is a polyhedron Linear fractional program \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }Gx\\leq{}h\\land{}Ax=b\\] \\[f_0(x)=\\frac{c^Tx+d}{c^Tx+f}, dom~f_0(x)=\\{c^Tx+f&gt;0\\}\\] a quasiconvex optimization problem; can be solved by bisection also equivalent to the LP (variables \\(y\\), \\(z\\)) \\[\\text{minimize }c^Ty+dz\\] \\[\\text{subject to }Gy\\leq{}hz,Ay=bz,e^Ty+fz=1,z\\geq0\\] Proof \\[\\forall{}x\\in{}X_1,y=\\frac{x}{e^Tx+f},z=\\frac{1}{e^Tx+f}\\rightarrow{}(y,z)\\in{}X_2\\rightarrow{}p_1^*\\geq{}p_2^*\\] \\[\\forall{}(y,z)\\in{}X_2, z\\neq0,x=\\frac{y}{z}\\in{}X_1\\] \\[\\forall{}(y,z)\\in{}X_2, z=0,\\text{Choose }x_0\\in{}X_1,x_0+ty\\in{}X_1(t\\geq0)\\] \\[\\lim_{t\\rightarrow\\infty}\\frac{c^T(x_0+ty)+d}{e^T(x_0+ty)+f}=c^Ty,p_2^*\\geq{}p_1^*\\] Quadratic program (QP) \\[A\\in{}R^{m\\times{}n},A=U\\Sigma{}V^T,\\Sigma=diag(\\lambda_1,...,\\lambda_r,0,...)\\] \\[A^\\dagger=V\\Sigma^\\dagger{}U^T,\\Sigma^\\dagger=diag(1/\\lambda_1,...,1/\\lambda_r,0,...)\\] Quadratically constrained quadratic program (QCQP) \\[\\text{minimize }(1/2)x^TP_0x+q_0^Tx+r_0\\] \\[\\text{subject to }(1/2)x^TP_ix+q_i^Tx+r_i\\leq0, Ax=b\\] \\(P_i\\in{}S_{+}^{n}\\) objective and constraints are convex quadratic Least squares \\[\\text{minimize }||Ax-b||_2^2\\] analytical solution \\(x^\\star=A^\\dagger{}b\\) Robust linear program \\[\\text{minimize }c^Tx\\] \\[\\text{subject to }prob(a_i^Tx\\leq{}b_i)\\geq\\eta,i=1,...,m\\] Semidefinite program (SDP) \\[\\text{minimize }c^Tx\\] \\[\\text{subject to }x_1F_1+x_2F_2+\\dots+x_nF_n+G\\preceq0,Ax=b\\] inequality constraint is called linear matrix inequality (LMI) includes problems with multiple LMI constraints"},{"title":"不可修复系统的可靠性分析","slug":"不可修复系统的可靠性分析","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2019/03/18/不可修复系统的可靠性分析/","link":"","permalink":"https://zs-liu.github.io/2019/03/18/不可修复系统的可靠性分析/","excerpt":"可靠性逻辑框图 系统可靠性 在元件故障数据和系统结构已知的情况下，预测系统的可靠性 硬件可靠性/人员操作可靠性/软件可靠性","text":"可靠性逻辑框图 系统可靠性 在元件故障数据和系统结构已知的情况下，预测系统的可靠性 硬件可靠性/人员操作可靠性/软件可靠性 系统可靠性框图 描述系统与其组成单元之间的故障逻辑关系，以系统工程图为基础。 方框:单元或功能 逻辑关系:功能布局 连线:系统功能流程的方向 节点(节点可以在需要时才加以标注) 输入节点:系统功能流程的起点 输出节点:系统功能流程的终点 中间节点 典型不可修系统的可靠性分析 可靠性分析假设 系统及组成单元只有故障与正常两种状态 不同方框表示的不同单元的故障概率是相互独立的 不考虑输入错误引起的系统故障 假设系统的整个软件是完全可靠的 假设人员操作是完全可靠的。 串联系统 \\[S=x_1\\cap{}x_2\\cap{}\\dots\\cap{}x_n\\] \\[\\bar{S}=\\bar{x}_1\\cup\\bar{x}_2\\cup\\dots\\cup\\bar{x}_n\\] \\[R_S(t)=\\prod_{i=1}^nR_i(t)\\] 当各单元的寿命分布均为指数分布时，系统的寿命也服从指数分布 串联系统中提高可靠度最小的设备的可靠性，对系统可靠性的提高贡献最大 并联系统 \\[S=x_1\\cup{}x_2\\cup{}\\dots\\cup{}x_n\\] \\[\\bar{S}=\\bar{x}_1\\cap\\bar{x}_2\\cap\\dots\\cap\\bar{x}_n\\] \\[R_S(t)=1-\\prod_{i=1}^n(1-e^{-\\lambda_it})\\] \\[MTTF_S=\\sum_{i=1}^n(i\\lambda)^{-1}\\] 三并联到四并联基本上可以满足要求 并联系统中提高可靠度最大的设备的可靠性，对系统可靠性的提高贡献最大 \\(k/n\\)表决系统 \\[R_S(t)=\\sum_{i=0}^{n-k}C_n^i[1-R(t)]^i[R(t)]^{n-i}\\] \\[\\text{For exponential distribution, }MTTF_S=\\sum_{i=k}^n\\frac{1}{i\\lambda}\\] 储备系统 组成系统的各单元只有一个单元工作，当工作单元故障时，通过转换装置接到另一个单元继续工作，直到所有单元都故障时系统才故障，称为非工作贮备系统(又可称为旁联系统) 卷积方法 \\[f_{12}(t)=f_1(t)*f_2(t)=\\int_0^tf_2(t-t_1)f_1(t)dt_1\\] \\(f_1(t)=\\lambda_1e^{-\\lambda_1t},f_2(t)=\\lambda_2e^{-\\lambda_2t}\\) \\[L[f_{12}(t)]=L[f_1(t)*f_2(t)]=L[f_1(t)]\\cdot{}L[f_2(t)]=\\frac{\\lambda_1}{s+\\lambda_1}\\frac{\\lambda_2}{s+\\lambda_2}\\] \\[\\Rightarrow{}f_{12}(t)=\\frac{\\lambda_1\\lambda_2}{\\lambda_1-\\lambda_2}(e^{-\\lambda_2t}-e^{-\\lambda_1t})\\] 复合事件概率法 两设备的失效可认为是独立的 计算储备系统时需考虑两者产生的非独立性（在时间上） \\[R_{12}(t)=R_1(t)+\\int_0^tR_2(t-t_1)f_1(t_1)dt_1\\] 考虑冗余设备的备用失效 \\(R_2^{-}(\\cdot)\\) 考虑不完全切换 \\(R_{SW}\\) \\[R_{12}(t)=R_1(t)+R_{SW}\\int_0^tR_2(t-t_1)R_2^{-}(t_1)f_1(t_1)dt_1\\] 网络系统 全概率分解法分析复杂系统可靠性 \\[R_S(t)=P(S)=P(x)P(S|x)+P(\\bar{x})P(S|\\bar{x})\\] \\(S(x)\\) 表示把网络 \\(S\\) 中设备 \\(x\\) 的两端节点合成一个节点而产生的新网络 \\(S(\\bar{x})\\) 表示把网络 \\(S\\) 中设备 \\(x\\) 去掉(即两个端点之间不存在经由 \\(x\\) 的联系)而产生的新网络 \\[R_S(t)=P(S)=P(x)P(S(x))+P(\\bar{x})P(S(\\bar{x}))\\] 选用分解弧的原则 对任意无向弧可以作为分解弧 与输入节点与输出节点相连的弧可以作为分解弧 对任意有向弧，其两端点的任何一个只有流入或流出的弧，可以作为分解弧，若弧的两端都有流入和流出的弧不可作为分解弧（因为不可以引入原本不存在的通路） 最小路集/最小割集法分析系统可靠性 结构函数 \\[Y=\\varphi(X)=\\varphi(x_1,x_2,\\dots,x_n)\\] \\[\\bar{\\varphi}(X)=1-\\varphi(\\overline{1-X})\\] \\[C_1(X)=\\{i|x_i=1\\},C_0(X)=\\{i|x_i=0\\}\\] 路集中任何一个设备变成失效则系统失效，此路集为最小路集 割集中任何一个设备变成成功则系统成功，此割集为最小割集 单调关联系统的表示 设最小路集为 \\(p_1,\\dots,p_m\\)，最小割集为 \\(k_1,\\dots,k_m\\) \\[\\varphi(x)=\\bigcup_{j=1}^m\\bigcap_{i\\in{}p_j}x_i=\\bigcap_{j=1}^l\\bigcup_{i\\in{}k_j}x_i\\] 最小路集/最小割集求解 联络矩阵法 \\[C:\\{C_{ij}=x\\text{ if arc }x\\text{ exists between }i,j\\text{; }0\\text{ otherwise}\\}\\] \\[C^r=C\\times{}C^{r-1}\\] 如果研究 \\(I\\) 到 \\(L\\) 的可靠性 只需求出\\(C^2,C^3,\\dots,C^{n-1}\\)中第 \\(L\\) 列 \\(C^{n-1}\\)只需求出第 \\(I\\) 行 布尔行列式 给定联络矩阵 \\(C\\) ，令 \\(D=C+I\\) 删去输入节点列，输出节点行，得到 \\(S\\) \\(|S|\\) 展开并且各项取正 最小割集 \\(S\\) 最小割集为 \\(\\bar{S}\\) 最小路集 利用最小路集/最小割集求解系统可靠度 精确解 最小路集/最小割集一般相交（可以直接用容斥原理求解），可对其进行“不交化”，得到相互独立的最小路集/最小割集 \\[K_1\\cup{}K_2=K_1+\\bar{K}_1K_2\\] 近似解 区间估计：当设备可靠度较高，容斥原理展式的首项或前两项起主要作用 点估计 特殊情况：三角形与星形"},{"title":"Convex Functions","slug":"Convex Functions","date":"un44fin44","updated":"un22fin22","comments":true,"path":"2019/03/07/Convex Functions/","link":"","permalink":"https://zs-liu.github.io/2019/03/07/Convex Functions/","excerpt":"Basic properties and examples Definition \\(f:R^n\\rightarrow{}R\\) is convex if dom\\(f\\) is a convex set and \\[f(\\theta{}x+(1-\\theta)y)\\leq\\theta{}f(x)+(1-\\theta)f(y)\\] for all \\(x,y\\in\\) dom\\(f\\), \\(0\\leq\\theta\\leq1\\)","text":"Basic properties and examples Definition \\(f:R^n\\rightarrow{}R\\) is convex if dom\\(f\\) is a convex set and \\[f(\\theta{}x+(1-\\theta)y)\\leq\\theta{}f(x)+(1-\\theta)f(y)\\] for all \\(x,y\\in\\) dom\\(f\\), \\(0\\leq\\theta\\leq1\\) strictly convex if we choose \\(&lt;\\) instead of \\(\\leq\\) Examples on \\(R\\), \\(R^n\\) and \\(R^{m\\times{}n}\\) Restriction of a convex function to a line \\(f:R^n\\rightarrow{}R\\) is convex if and only if the function \\(g:R\\rightarrow{}R\\) \\[g(t)=f(x+tv), dom g=\\{\\}\\] #### Proof \\[\\forall{}t_1,t_2,x+t_1v\\in{}dom~f,x+t_2v\\in{}dom~f\\] \\[g(\\theta{}t_1+(1-\\theta)t_2)=f(x+(\\theta{}t_1+(1-\\theta{})t_2)v)\\] \\[=f(\\theta(x+t_1v)+(1-\\theta)(x+t_2v))\\] \\[\\leq{}\\theta{}f(x+t_1v)+(1-\\theta)f(x+t_2v)\\] \\[let~x_1,x_2\\in{}dom~f, de\\!{}fine~x=x_1,v=x_2-x_1,g(t)=f(x+tv)\\] \\[f(\\theta{}x_1+(1-\\theta)x_2)=f(x_1+(1-\\theta)(x_2-x_1))=g(1-\\theta)\\] \\[\\leq{}\\theta{}g(0)+(1-\\theta)g(1)=\\theta{}f(x_1)+(1-\\theta)f(x_2)\\] Example \\(f:S^n\\rightarrow{}R\\) with \\(f(X)=\\log\\det{}X\\), dom \\(f=S^n_{++}\\) ### First-order condition Differentiable \\(f\\) with convex domain is convex iff \\[f(y)\\geq{}f(x)+\\nabla{}f(x)^T(y-x)\\quad\\forall{}x,y\\in{}dom~f\\] #### Proof \\[t&gt;0,~f(x+t(y-x))\\leq{}(1-t)f(x)+tf(y)\\] \\[f(y)\\geq{}f(x)+\\frac{f(x+t(y-x))-f(x)}{t}\\] \\[f(y)\\geq{}f(x)+f^{&#39;}(x)(y-x)\\] \\[f(x)\\geq{}f^{&#39;}(z)(x-z),f(y)\\geq{}f^{&#39;}(z)(y-z)\\] \\[let~z=\\theta{}x+(1-\\theta{})y\\] \\[\\theta{}f(x)+(1-\\theta)f(y)\\geq{}f(z)\\] ### Second-order condition Twice differentiable \\(f\\) with convex domain is convex iff \\[\\nabla^2f(x)\\succeq0\\quad\\forall{}x\\in{}dom~f\\] if \\(\\nabla^2f(x)\\succ0\\) for all \\(x\\in{}dom~f\\), then \\(f\\) is strictly convex ### Epigraph and sublevel set \\(\\alpha\\)-sublevel set of \\(f:R^n\\rightarrow{}R\\) \\[C_\\alpha=\\{x\\in{}dom~f|f(x)\\leq\\alpha\\}\\] sublevel sets of convex functions are convex (converse is false) epigraph of \\(f:R^n\\rightarrow{}R\\) \\[epi~f=\\{(x,t)\\in{}R^{n+1}|x\\in{}dom~f,f(x)\\leq{}t\\}\\] \\(f\\) is convex iff \\(epi~f\\) is a convex set ## Operations that preserve convexity ### Positive weighted sum &amp; composition with affine function nonnegative multiple sum composition with affine function Pointwise Maximum if \\(f_1,...,f_m\\) are convex, then \\[f(x)=\\max\\{f_1(x),...,f_m(x)\\}\\] is convex ### Pointwise Supremum if \\(f(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\[g(x)=\\sup_{y\\in\\mathcal{A}}f(x,y)\\] is convex ### Theorem Let \\(f:R^n\\rightarrow{}R\\) convex and \\(dom~f:R^n\\), then \\[f(x)=\\sup\\{g(x)|g~is~af\\!{}fine,g(z)\\leq{}f(z),\\forall{}z\\}\\] #### Proof \\(\\geq\\), obvious \\(\\leq\\) \\[(x,f(x))\\in{}bd~epi~f\\] \\[\\exists{}(a,b)\\neq0~s.t.~a^Tz+bt\\leq{}a^Tx+bf(x)\\] \\[a^Tz+b(f(z)+s)\\leq{}a^Tx+bf(x)\\] \\[b\\leq0,\\text{otherwise }bs\\rightarrow{}\\infty\\] \\[b=0,z=x+a\\rightarrow{}a=0,\\text{contradiction}\\] \\[b&lt;0,s=0\\rightarrow{}f(z)\\geq{}f(x)+\\frac{a^T(x-z)}{b}=g(z)\\] ### Jensen's inequality if \\(f\\) is convex, then\\[f(Ez)\\leq{}Ef(z)\\] #### Proof \\[Let~x_0=\\int_{\\Omega}x\\rho{}(x)dx\\] \\[\\exists{}a,b~s.t.~ax+b\\leq{}f(x)~\\text{and}~ax_0+b=f(x_0)\\] \\[\\int_{\\Omega}f(x)\\rho{}(x)dx\\geq{}\\int_{\\Omega}(ax+b)\\rho(x)dx=ax_0+b=f(x_0)\\] ### Composition with scalar functions composition of \\(g:R^n\\rightarrow{}R\\) and \\(h:R\\rightarrow{}R\\) \\[f(x)=h(g(x))\\] \\(f\\) is convex if \\(g\\) convex, \\(h\\) convex, \\(\\tilde{h}\\) nondecreasing \\(g\\) concave, \\(h\\) convex, \\(\\tilde{h}\\) nonincreasing Minimization if \\(f(x,y)\\) is convex in \\((x,y)\\) and \\(C\\) is a convex set, then\\[g(x)=\\inf_{y\\in{}C}f(x,y)\\] is convex ### Perspective the perspective of a function \\(f:R^n\\rightarrow{}R\\) is the function \\(g:R^n\\times{}R\\rightarrow{}R\\) \\[g(x,t)=tf(x/t),~dom~g=\\{(x,t)|x/t\\in{}dom~f,t&gt;0\\}\\] \\(g\\) is convex if \\(f\\) is convex #### Proof use epi graph \\[(x,t,s)\\in{}epi~g\\] \\[s\\geq{}tf(x/t)\\] \\[s/t\\geq{}f(x/t)\\] \\[(x/t,s/t)\\in{}epi~f\\] ## Conjugate function the conjugate of a function \\(f\\) is \\[f^*(y)=\\sup_{x\\in{}dom~f}(y^Tx-f(x))\\] \\(f^*\\) is convex (even if \\(f\\) is not) \\(f(x)+f(y)\\geq{}x^Ty\\) \\(f\\) is convex and closed (\\(epi~f\\) is closed), then \\(f^{**}=f\\) Proof \\[f(x)\\geq{}x^Ty-f(y)\\] \\[f(x)\\geq{}\\sup_{y\\in{}dom~f^*}(y^Tx-f(x))=f^{**}(x)\\] \\[epi~f\\subseteq{}epi~f^{**}\\] \\[Let~(x,f^{**}(x))\\notin{}epi~f\\] \\[\\exists{}[a,b]\\neq0,~s.t.[a,b][z-x,t-f^{**}(x)]^T\\leq{}c&lt;0,\\forall(z,t)\\in{}epi~f\\] \\[t=f(z)+s,s\\geq0\\] \\[b&lt;0,a^T(z-x)+b(f(z)-f^{**}(x)+s)\\leq{}c\\] \\[\\text{Define }y=-\\frac{a}{b},~y^Tz-f(z)-s-y^Tx+f^{**}(x)\\leq-\\frac{c}{b}\\] \\[\\text{Let }s=0,\\text{make }\\sup\\] \\[b=0,Let~y\\in{}dom~f^*~and~\\varepsilon&gt;0\\] \\[[a+\\varepsilon{}\\hat{y},-\\varepsilon][z-x,t-f^{**}(x)^T\\] \\[=a^T(z-x)+\\varepsilon\\hat{y}^T(z-x)-\\varepsilon(t-f^{**}(x))\\] \\[\\leq{}c+\\varepsilon(f^*(\\hat{y})+f^{**}(x)-\\hat{y}^Tx)=\\tilde{c}&lt;0\\] #### Example \\[f(X)=\\log\\det{}X,X\\in{}S_+^n\\] \\[f^*(Y)=\\sup_{X&gt;0}\\{&lt;X,Y&gt;-f(X)\\}\\] \\[Y\\geq0,f^*(Y)=+\\infty\\] \\[Y&lt;0,Y-(X^{*})^{-1}=0\\rightarrow{}f^*(Y)=Tr(I)-\\log\\det{}Y^{-1}\\] \\[Y \\ngeq \\&amp;\\nleq0\\rightarrow{}f^*(Y)=+\\infty(\\exists{}u,s.t.Yu=\\lambda{}u,\\text{choose }X=tuu^T,t&gt;0)\\] ## Quasiconvex functions ### Definition \\(f:R^n\\rightarrow{}R\\) is quasiconvex if dom \\(f\\) is convex and its sublevel sets are all convex ### Modified Jensen inequality \\[0\\leq\\theta\\leq1\\rightarrow{}f(\\theta{}x+(1-\\theta)y)\\leq\\max\\{f(x),f(y)\\}\\] #### Proof \\[\\alpha=f(x)&gt;f(y)\\rightarrow{}S_\\alpha=\\{x|f(x)\\leq\\alpha\\}\\] \\[[x,y]\\subset{}S_\\alpha\\rightarrow{}f([x,y])\\leq\\alpha\\] ### First-order condition Differentiable \\(f\\) with convex domain is quasiconvex iff \\[f(y)\\leq{}f(x)\\rightarrow\\triangledown{}f(x)^T(y-x)\\leq0\\] #### Proof \\[g(t)=f(x+t(y-x))\\rightarrow{}g^{&#39;}(0)=\\triangledown{}f(x)^T(y-x)\\leq0\\] \\[\\text{Assume } z\\in[x,y],f(z)&gt;f(x)\\&amp;{}f(z)&gt;f(y)\\] \\[\\triangledown{}f(z)^T(x-z)\\leq0~\\&amp;~\\triangledown{}f(z)^T(y-z)\\leq0\\] \\[\\triangledown{}f(z)=0\\] \\(z_0\\) in the neighbour of \\(z\\), \\(\\triangledown{}f(z)\\neq0,f(z_0)&gt;f(x),f(z_0)&gt;f(y)\\), contradict ### Second-order condition Differentiable \\(f\\) with convex domain is quasiconvex iff \\[f^{&#39;}(x)=0\\rightarrow{}f^{&#39;&#39;}(x)\\geq0\\] #### Proof \\[\\exists{}c\\in[a,b],f^{&#39;}(c)=0\\land{}f^{&#39;&#39;}(c)&lt;0\\] \\[\\exists{}\\varepsilon{}f(c)&gt;f(c-\\varepsilon),f(c)&gt;f(c+\\varepsilon)\\] \\[\\exists{}\\delta&gt;0,f(c)-\\delta&gt;f(c-\\varepsilon),f(c)+\\delta&gt;f(c+\\varepsilon)\\] \\[c-\\varepsilon,c+\\varepsilon\\in\\{x|f(x)\\leq{}f(c)-\\delta\\},c\\notin\\{x|f(x)\\leq{}f(c)-\\delta\\}\\] \\(\\{x|f(x)\\leq{}f(c)-\\delta\\}\\) is not convex, contradict \\[f^{&#39;}\\neq0\\rightarrow{}f\\uparrow{}or\\downarrow\\rightarrow{}f\\in\\text{quasiconvex}\\] \\[d=\\sup\\{c|f^{&#39;}(c)=0\\}\\] \\[\\forall{}x\\geq{}d,f{&#39;}(x)\\geq0\\rightarrow{}f(x)\\geq{}f(d)\\] \\[\\forall{}x\\leq{}d,f{&#39;}(x)\\leq0\\rightarrow{}f(x)\\leq{}f(d)\\] ## Log-concave and Log-convex functions ### Definition A positive function \\(f\\) is log-concave if \\(\\log{}f\\) is concaveA positive function \\(f\\) is log-convex if \\(\\log{}f\\) is convex powers:\\(x^a\\) on \\(R_{++}\\) is log-convex for \\(a\\leq0\\), log-concave for \\(a\\geq0\\) many common probability densities are log-concave cumulative Gaussian distribution function is log-concave Second-order condition Twice differentiable \\(f\\) with convex domain is log-concave if and only if \\[\\forall{}x,f(x)\\triangledown^2f(x)\\preceq\\triangledown{}f(x)\\triangledown{}f(x)^T\\] ### Properties product of log-concave functions is log-concave sum of log-concave functions is not always log-concave if \\(f:R^n\\times{}R^m\\rightarrow{}R\\) is log-concave, then \\[g(x)=\\int{}f(x,y)dy\\] is log-concave (not easy to show) convolution \\(f*g\\) of log-concave functions \\(f,g\\) is log-concave \\[(f*g)(x)=\\int{}f(x-y)g(y)dy\\]"},{"title":"基本参数与参数估计基础","slug":"基本参数与参数估计基础","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2019/03/04/基本参数与参数估计基础/","link":"","permalink":"https://zs-liu.github.io/2019/03/04/基本参数与参数估计基础/","excerpt":"基本参数 可靠度与不可靠度 \\(n(t)\\):在\\(0\\sim{}t\\)时刻的工作时间内产品的累计失效数 \\(N_0\\):\\(t=0\\)时在规定条件下进行工作的产品数 \\[Reliability(t)=R(t)=\\frac{N_0-n(t)}{N_0}\\] \\[Fallibility(t)=F(t)=\\frac{n(t)}{N_0}\\]","text":"基本参数 可靠度与不可靠度 \\(n(t)\\):在\\(0\\sim{}t\\)时刻的工作时间内产品的累计失效数 \\(N_0\\):\\(t=0\\)时在规定条件下进行工作的产品数 \\[Reliability(t)=R(t)=\\frac{N_0-n(t)}{N_0}\\] \\[Fallibility(t)=F(t)=\\frac{n(t)}{N_0}\\] 失效率 \\[\\lambda(t)=\\lim_{\\Delta{}t\\rightarrow0}\\frac{P(t&lt;T\\leq{}t+\\Delta{}t|T&gt;t)}{\\Delta{}t}=\\frac{F^{&#39;}(t)}{1-F(t)}=-\\frac{R^{&#39;}(t)}{R(t)}\\] \\[\\hat{\\lambda}(t)=\\frac{n(t+\\Delta{}t)-n(t)}{\\Delta{}t(N_0-n(t))}\\] 单位:\\(1Fit=10^{-9}/h\\) \\(R(t)=\\exp\\{-\\int_0^t\\lambda(t)dt\\}\\) \\(f(t)=\\lambda(t)\\exp\\{-\\int_0^t\\lambda(t)dt\\}\\) 浴盆曲线 大多数产品的失效率随时间的变化曲线形似浴盆 由于产品失效机理的不同，失效率随时间的变化大致可以分为三个阶段：早期故障、偶然故障和耗损故障 平均寿命 平均故障前时间(MTTF, Mean Time To Failure) 设\\(n\\)个不可修复产品在同样条件下进行试验，测得其全部故障时间为\\(t_1,t_2,...,t_n\\)，则其平均故障前时间为: \\[T_{TF}=\\frac{1}{n}\\sum_{i=1}^nt_i\\] 平均故障间隔时间(MTBF, Mean Time Between Failure) 设\\(1\\)个可修复产品在使用过程中发生了\\(n\\)次故障，每次故障修复后又重新投入使用，测得其每次工作持续时间为\\(t_1,t_2,...,t_n\\)，则其平均故障间隔时间为： \\[T_{BF}=\\frac{1}{n}\\sum_{i=1}^nt_i\\] 平均寿命 \\[\\theta=\\int_0^\\infty{}tf(t)dt=\\int_0^\\infty{}R(t)dt\\] 可靠寿命 \\[t_R=R^{-1}(r)\\] 常用的可靠性定量指标 指数分布:\\(\\lambda=const\\) \\(R(t)=\\exp\\{-\\lambda{}t\\}\\) \\(f(t)=\\lambda\\exp\\{-\\lambda{}t\\}\\) \\(\\theta=MTTF=\\lambda^{-1}\\) 常用分布 对数正态分布 \\(T\\)的对数\\(\\ln{}T\\)服从正态分布，则\\(T\\)服从对数正态分布，即\\(X=\\ln{}T\\sim{}N(\\mu,\\sigma^2)\\) \\[f(t)=\\frac{1}{\\sigma{}t\\sqrt{2\\pi}}\\exp\\{-\\frac{(\\ln{}t-\\mu)^2}{2\\sigma^2}\\}\\] 均值:\\(E(T)=\\exp\\{\\mu+\\frac{\\sigma^2}{2}\\}\\) 方差:\\(D(T)=\\exp\\{2(\\mu+\\frac{\\sigma^2}{2})\\}(e^{\\sigma^2}-1)\\) 威布尔分布 \\[f(t)=\\frac{m}{t_0}(t-\\gamma)^{m-1}\\exp\\{-(t-\\gamma)\\frac{m}{t_0}\\}\\] 均值:\\(E(T)=\\gamma+t_0^{\\frac{1}{m}}\\Gamma(1+\\frac{1}{m})\\) 方差:\\(D(T)=t_0^{\\frac{2}{m}}[\\Gamma(1+\\frac{2}{m})-\\Gamma^2(1+\\frac{1}{m})]\\) 参数估计基础 截尾实验 无替换定数截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到出现事先指定的\\(r\\)个产品失效停止试验。 总试验时间:\\[T_r=\\sum_{i=1}^rt_i(n-r)t_r\\] 点估计:\\(\\lambda=r/T_r\\) 区间估计:\\(2\\lambda{}T_r\\sim{}\\chi^2(2r)\\) 有替换定数截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到规定失效数\\(r\\)时停止试验。\\(r\\)个失效样品的失效时间记为:\\(t_1\\leq{}t_2\\leq\\dots\\leq{}t_r\\) 总试验时间:\\(T_r=nt_r\\) 无替换定时截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到出现事先规定的时间\\(t_0\\)停止试验，共有\\(r\\)个失效 总试验时间:\\[T_0=\\sum_{i=1}^rt_i(n-r)t_0\\] 点估计:\\(\\lambda=r/T_0\\) 有替换定时截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到出现事先规定的时间\\(t_0\\)停止试验，共有\\(r\\)个失效 总试验时间:\\(T_0=nt_0\\) 贝叶斯估计简介 略"},{"title":"无限阶段折扣问题","slug":"无限阶段折扣问题","date":"un44fin44","updated":"un22fin22","comments":true,"path":"2019/01/24/无限阶段折扣问题/","link":"","permalink":"https://zs-liu.github.io/2019/01/24/无限阶段折扣问题/","excerpt":"引言——成本总和的最小化 无限阶段问题的成本总和 定义非时变离散时间动态系统： \\[x_{k+1}=f(x_k,u_k,w_k)\\] \\[x_k\\in{}S,u_k\\in{}U(x_k)\\subset{}C,w_k\\sim{}P(\\cdot{}|x_k,u_k)\\]","text":"引言——成本总和的最小化 无限阶段问题的成本总和 定义非时变离散时间动态系统： \\[x_{k+1}=f(x_k,u_k,w_k)\\] \\[x_k\\in{}S,u_k\\in{}U(x_k)\\subset{}C,w_k\\sim{}P(\\cdot{}|x_k,u_k)\\] 给定初始状态\\(x_0\\)，成本函数\\(g(\\cdot,\\cdot,\\cdot):S\\times{}C\\times{}D\\rightarrow\\mathcal{R}\\)和折扣因子\\(\\alpha\\)，现需要找到\\(\\pi=\\{\\mu_0,\\mu_1,...\\},\\mu_k\\in{}U(x_k)\\)，使得如下定义的成本最小： \\[J_\\pi(x_0)=\\lim_{N\\rightarrow{}\\infty}E_{w_k}\\{\\sum_{k=0}^{N-1}\\alpha^kg(x_k,\\mu_k(x_k),w_k)\\}\\] 对于所有合法的\\(\\pi\\)构成的集合记为\\(\\Pi\\)，则记最优成本函数为： \\[J^*(x)=\\min_{\\pi\\in\\Pi}J_\\pi(x)\\quad{}x\\in{}S\\] 如果对于所有初始状态，其最优策略相同，则称该问题为稳定问题，该最优策略为最优稳定策略： \\[J_\\mu(x)=J^*(x)\\] 有限阶段问题的DP算法 对于\\(N\\)个阶段的成本问题，DP算法为： \\[J_{N-k}(x)=\\min_{u\\in{}U(x)}E\\{\\alpha^{N-k}g(x,u,w)+J_{N-k+1}(f(x,u,w))\\}\\] \\[J_N(x)=\\alpha^NJ(x)\\] 重新作如下记号： \\[V_k(x)=\\frac{J_{N-k}(x)}{\\alpha^{N-k}},\\quad{}V_0(x)=J(x)\\] \\[V_{k+1}(x)=\\min_{u\\in{}U(x)}E\\{g(x,u,w)+\\alpha{}V_k(f(x,u,w))\\}\\] 简记与单调性 对于\\(J:S\\rightarrow\\mathcal{R}\\)，采用\\(TJ\\)记对\\(J\\)应用DP算法： \\[(TJ)(x)=\\min_{u\\in{}U(x)}E\\{g(x,u,w)+\\alpha{}J(f(x,u,w))\\}\\] 我们视\\(T\\)为\\(S\\)上函数\\(J\\)到\\(TJ\\)的映射。 同样，有如下简记： \\[(T_\\mu{}J)(x)=E\\{g(x,\\mu(x),w)+\\alpha{}J(f(x,\\mu(x),w))\\}\\] \\[(T^kJ)(x)=(T(T^{k-1}J))(x),\\quad{}(T^0J)(x)=J(x)\\] 引理 1.1.1: 单调性引理 对于任何函数\\(J:S\\rightarrow{}\\mathcal{R}\\)和\\(J^{&#39;}:S\\rightarrow\\mathcal{R}\\) \\[J(x)\\leq{}J^{&#39;}(x)\\quad{}\\text{for all }x\\in{}S\\] 对于任何稳定策略\\(\\mu:S\\rightarrow{}C\\)： \\[(T^kJ)(x)\\leq{}(T^kJ^{&#39;})(x)\\quad{}\\text{for all }x\\in{}S,k=1,2,...,\\] \\[(T_\\mu^kJ)(x)\\leq{}(T_\\mu^kJ^{&#39;})(x)\\quad{}\\text{for all }x\\in{}S,k=1,2,...,\\] 引理 1.1.2 定义单位函数为\\(e(x)\\equiv1(\\forall{}x\\in{}S)\\)，则对于任何函数\\(J:S\\rightarrow{}\\mathcal{R}\\)，稳定策略\\(\\mu:S\\rightarrow{}C\\)，标量\\(r\\)： \\[(T^k(J+re))(x)=(T^kJ)(x)+\\alpha^kr\\quad{}\\text{for all }x\\in{}S\\] \\[(T_\\mu^k(J+re))(x)=(T_\\mu^kJ)(x)+\\alpha^kr\\quad{}\\text{for all }x\\in{}S\\] 随机的过去依赖策略 命题 1.1.1： Markov策略的合理性 假设策略集合是可数的，初始状态是可数集合上的分布。则每对\\((x_k,u_k)\\)以及与随机的过去依赖策略相对应的每阶段的期望成本，同样可以以随机Markov策略获得。 有界的阶段成本的折扣问题 假设D： 折扣成本-有界的阶段成本 存在标量\\(M\\)使阶段成本\\(g\\)满足 \\[|g(x,u,w)|\\leq{}M,\\quad\\text{for all }(x,u,w)\\in{}S\\times{}C\\times{}D\\] 命题 1.2.1： DP算法的收敛性 对于有界函数\\(J:S\\rightarrow{}\\mathcal{R}\\)，最优成本函数满足： \\[J^*(x)=\\lim_{N\\rightarrow\\infty}(T^NJ)(x)\\quad\\text{for all }x\\in{}S\\] 推论 1.2.1.1 对于稳定策略\\(\\mu\\)，相关联的成本函数满足： \\[J_{\\mu}(x)=\\lim_{N\\rightarrow\\infty}(T_{\\mu}^NJ)(x)\\quad\\text{for all }x\\in{}S\\] 命题 1.2.2：贝尔曼方程 最优成本函数满足： \\[J^*=TJ^*\\] 此外，\\(J^*\\)也是有界成本函数中满足这一方程的唯一解 推论 1.2.2.1 对于稳定策略\\(\\mu\\)，相关联的成本函数满足： \\[J_{\\mu}=T_{\\mu}J_{\\mu}\\] 此外，\\(J_{\\mu}\\)也是有界成本函数中满足这一方程的唯一解 命题 1.2.3：最优的充要条件 稳定策略\\(\\mu\\)是最优的当且仅当对于\\(\\forall{}x\\in{}S\\)，\\(\\mu(x)\\)取得了贝尔曼方程的最小值，即： \\[TJ^*=T_{\\mu}J^*\\] 命题 1.2.4 对于任意两个有界函数\\(J:S\\rightarrow{}\\mathcal{R},J^{&#39;}:S\\rightarrow{}\\mathcal{R}\\)，有： \\[(\\forall{}k)~\\max_{x\\in{}S}|(T^kJ(x)-(T^kJ^{&#39;})(x))|\\leq\\alpha^k\\max_{x\\in{}S}|J(x)-J^{&#39;}(x)|\\]"},{"title":"Hello World","slug":"hello-world","date":"un22fin22","updated":"un33fin33","comments":true,"path":"2019/01/22/hello-world/","link":"","permalink":"https://zs-liu.github.io/2019/01/22/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment"},{"title":"$\\rm \\LaTeX$ Manual","slug":"Latex Manual","date":"un22fin22","updated":"un33fin33","comments":true,"path":"2019/01/22/Latex Manual/","link":"","permalink":"https://zs-liu.github.io/2019/01/22/Latex Manual/","excerpt":"Text","text":"Text Figure Two Column Figure 12345678\\begin&#123;figure&#125;[htbp] \\begin&#123;minipage&#125;[t]&#123;0.5\\linewidth&#125; \\includegraphics[]&#123;&#125; \\end&#123;minipage&#125; \\begin&#123;minipage&#125;[t]&#123;0.5\\linewidth&#125; \\includegraphics[]&#123;&#125; \\end&#123;minipage&#125;\\end&#123;figure&#125; Table Equation Un-Italic Code Display $x+y$ \\(x+y\\) $\\rm x+y$ \\(\\rm x+y\\) $\\text{x+y}$ \\(\\text{x+y}\\) Others Two Column Document 1234567\\documentclass[UTF8,a4paper,14pt,twocolumn]&#123;ctexart&#125;\\begin&#123;document&#125; \\twocolumn[ \\begin&#123;@twocolumnfalse&#125; % something here may cross two column \\end&#123;@twocolumnfalse&#125;]\\end&#123;document&#125; Hyperlink 123\\usepackage[colorlinks,linkcolor=blue]&#123;hyperref&#125;% set this command as the final one\\href&#123;https://www.google.com&#125;&#123;Google&#125;"}]}