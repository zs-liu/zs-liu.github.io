{"meta":{"title":"鲈鱼柳的博客","subtitle":"为学日益，为道日损","description":null,"author":"鲈鱼柳","url":"https://zs-liu.github.io"},"pages":[{"title":"","date":"un66fin66","updated":"un66fin66","comments":true,"path":"404.html","permalink":"https://zs-liu.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 Sorry，Page Not Found 如果网站出现故障，请在下方留言"},{"title":"","date":"un66fin66","updated":"un22fin22","comments":true,"path":"google462fce11ee28ad9d.html","permalink":"https://zs-liu.github.io/google462fce11ee28ad9d.html","excerpt":"","text":"google-site-verification: google462fce11ee28ad9d.html"},{"title":"","date":"un66fin66","updated":"un55fin55","comments":true,"path":"about/index.html","permalink":"https://zs-liu.github.io/about/index.html","excerpt":"","text":"Hi，欢迎来到我的博客。如果想进一步了解我，请查看我的简历。 如需防止任何第三方获取你我的沟通信息，请在此处将你的文本信息加密并且将密文通过电子邮件发送给我。 Hi, welcome to my blog. If you want to know more about me, please check my resume. If you want to secure your contact with me, please encrypt your text here and send me the encrypted text by email."},{"title":"Categories","date":"un55fin55","updated":"un55fin55","comments":true,"path":"categories/index.html","permalink":"https://zs-liu.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tags/index.html","permalink":"https://zs-liu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Garbled Circuits","slug":"Garbled Circuits","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2020/11/16/Garbled Circuits/","link":"","permalink":"https://zs-liu.github.io/2020/11/16/Garbled%20Circuits/","excerpt":"","text":"Introduction Gabled circuits express the function to be computed as an “encrypted” boolean circuit, e.g. with ANDs, ORs, XORs, etc Garbled circuits are more efficient than for 2-party joint processing setups. They get too complex for more parties. In this post, we will look at Yao's Garbled Circuits Given two parties computations, Alice and Bob, they want to compute a function \\(f(x,y)\\) where \\(x\\) is Alice's input and \\(y\\) is Bob's input. After the computation, \\(x\\) and \\(y\\) remain private unless revealed by the function \\(f\\). One Gate Garbled Circuit WLOG, we consider a \\(\\text{AND}\\) gate in this section, i.e., \\(f(x,y)=x\\land y\\). Alice will be the circuit garbler, and Bob will be the circuit evaluator. Denote the input bit of Alice and Bob as \\(a,b\\). Step by step Alice compiles \\(f\\). into an acyclic boolean circuit, composed of logic gates like AND, OR, XOR etc. Alice generates random keys \\(k[A,0]\\), \\(k[A,1]\\), \\(k[B,0]\\), \\(k[B,1]\\), \\(k[C,0]\\), \\(k[C,1]\\), each of which corresponds to a bit of a wire. For example, \\(k[A,0]\\) for wire A, bit \\(0\\); \\(k[C,1]\\) for wire C, bit \\(1\\) Alice garbles the circuit to produce a garbled circuit, essentially each gate will carry out its operation on encrypted input bits producing an encrypted output bit. For example, the AND garbled truth table entry for wire A input \\(1\\) and wire B input \\(0\\), is key \\(k[C,0]\\) doubly-encrypted by \\(k[A,1]\\) and \\(k[B,0]\\) \\[\\text{GT}[1,0]=E_{k[A,1],k[B,0]}(k[C,\\text{AND}(1,0)])=E_{k[A,1],k[B,0]}(k[C,0])\\] Alice randomly permutates the the entries of the Garbled Table, since the truth-table order will leak information that could be used to learn Alice’s input. Alice sends the permutated garbled gate table plus the encrypted bits for its input and a decryption mapping table that allows Bob to map the encrypted output of the circuit to its plaintext. For example, if \\(a=1\\), Alice will send \\(k[A,1]\\) to Bob. Bob won't know the value of \\(a\\) since all wire keys are random The mapping should be \\(k[C,0]\\Rightarrow0\\),\\(k[C,1]\\Rightarrow1\\) Bob runs a 1-from-2 oblivious transfer (OT) protocol with Alice for Bob input bits. Bob will get the keys that corresponds to \\(b\\) from Alice without Alice learning \\(b\\). For example, if \\(b=1\\), Bob will get \\(k[B,1]\\). Bob won't know \\(k[B,0]\\) and Alice won't know \\(b=1\\). Bob doubly decrypts the garbled gate entry to learn the final encrypted result \\(k[C,\\text{AND}(a,b)]\\) and uses the decryption mapping table to map \\(k[C,\\text{AND}(a,b)]\\) to its plaintext bit \\(\\text{AND}(a,b)\\). Bob sends this to Alice. 1-from-2 Oblivious Transfer protocol Here is an illustrative example for 1-from-2 OT. There are many others protocols. What's more, 1-from-2 OT can be extended to k-from-n OT. WLOG, we assume that Alice has two messages \\(m_1,m_2\\) and Bob wants to know \\(m_1\\). Step by step Alice has two messages \\(m_1,m_2\\) Alice generates two public-private key pairs \\(\\text{(Pub1,Priv1)}\\), \\(\\text{(Pub2,Priv2)}\\). Alice \\(\\rightarrow\\) Bob: \\(\\text{Pub1,Pub2}\\) Bob generates a symmetric key \\(k\\) and chooses \\(\\text{Pub1}\\). Bob \\(\\rightarrow\\) Alice: \\(c=E_{\\text{Pub1}}(k)\\) Alice does \\(D_{\\text{Priv1}}(c)=k\\), \\(D_{\\text{Priv2}}(c)=u\\neq k\\) Alice \\(\\rightarrow\\) Bob: \\(c_1=E_k(m_1)\\), \\(c_2=E_u(m_2)\\) Bob does \\(D_k(c_1)=D_k(E_k(m_1))=m_1\\) and \\(D_k(c_2)=D_k(E_u(m_2))=\\text{non sense}\\) Conclusion Bob learns \\(m_1\\) in this example. Alice doesn't know which message Bob knows. Alice's messages need to have a structure that Bob can recognise to distinguish the good message \\(m_1\\) from the gibberish message. Point-and-Permutate p-bit In the Step 7, Bob doubly decrypts the garbled gate entry to learn the final encrypted result. Bob could decrypt each entry if the underlying encryption method makes it obvious when decrypting which is the “correct” ciphertext. This is both inefficient and inelegant. The following trick is to pair a random point-and-permutate bit and its inverse to each of the keys of the wire (the \\(0\\) key and \\(1\\) key). For example, if wire A has keys \\(k[A,0],k[A,1]\\), we would now have \\((k[A,0],\\text{pA})\\), \\((k[A,1],\\text{not pA})\\). Here is the equivalent pseudo-code to show how Alice generates the point-and-permutate garbled table. 12345678for each permutated PGT with input wires A, B and output wire C for a in [0,1] for b in [0,1] x = a xor pbit[A] y = b xor pbit[B] z = GATE(x,y) # e.g. AND, OR, XOR, NAND ... k[C,z] = GATE(k[A,x], k[B,y]) zl = z xor pbit[C] PGT[a][b] = E[k[A,x], k[B,y]](k[C,z], zl) # double encrypt In Step 5, Alice sends both the encrypted bits and the pbit for its input. In Step 6, Bob obtains both his key and the pbit of his key by running OT. As a result, Bob learns \\(k[A,a], \\text{pA or not pA}\\) and \\(k[B,b], \\text{pB or not pB}\\). Then Bob can directly doubly decrypts \\(\\text{PGT[pA][pB]}\\) to learn \\(k[C,z], z^{\\prime}\\). Example For \\(a=0,\\text{pA}=1,b=1,\\text{pB}=0\\) th { display: none; } \\(\\text{PGT[1][1]}\\) \\(E_{k[A,0],k[B,1]}(k[C,0],0)\\) \\(\\text{PGT[1][0]}\\) \\(E_{k[A,0],k[B,0]}(k[C,0],0)\\) \\(\\text{PGT[0][1]}\\) \\(E_{k[A,1],k[B,1]}(k[C,1],1)\\) \\(\\text{PGT[0][0]}\\) \\(E_{k[A,1],k[B,0]}(k[C,0],0)\\) Bob learns \\(k[A,0],1\\) and \\(k[B,1],1\\). He locates and doubly decrypts \\(\\text{PGT[1,1]}\\) with \\(k[A,0],k[B,1]\\) to learn \\(k[C,0], 0\\). Finally, he uses the decryption mapping table to map \\(k[C,0]\\) to its plaintext \\(0\\). Garbled circuit performance Runs in a constant number of rounds. Computation cost dominated by encryption function used. Typically hardware-assisted AES is used. High communication costs - time to transfer circuit, plus time to complete oblivious transfers (later can be done in parallel). Both Alice and Bob could cheat at various stages. Techniques such as zero-knowledge proofs and cut-and-choose can be used but have high overheads."},{"title":"Shamir Secret Sharing and BGW Protocol","slug":"Shamir Secret Sharing and BGW Protocol","date":"un11fin11","updated":"un22fin22","comments":true,"path":"2020/11/09/Shamir Secret Sharing and BGW Protocol/","link":"","permalink":"https://zs-liu.github.io/2020/11/09/Shamir%20Secret%20Sharing%20and%20BGW%20Protocol/","excerpt":"","text":"MPC (Multi-party Computation) Definition Given \\(n\\) parties, design a protocol which computes a public function \\(f\\) over inputs from each party such that the inputs remain private unless they would be revealed by the function anyway. Character Typically, all parties will receive the same output when the calculation finishes MPC is typically considered when parties don’t trust either the others or a 3rd party An MPC protocol can be shown equivalent to a protocol uses a trusted 3rd party There are many setups that can be used for MPC. However, in this post, we'll only take a look at Secret Sharing and its application BGW protocol. Adversarial Models Honest-but-curious, Semi-Honest, Passive: Parties follow protocol, but are interested (i.e. curious) in breaking privacy of other parties. Also know as corrupt parties. Malicious, Byzantine, Active: Parties can deviate from the protocol e.g. lie about inputs, quit protocol early. Secure Type Information theoretically secure: system cannot be broken even if adversary has unlimited computational power. Also known as Perfect security and Unconditional security. Computationally secure: adversary would require an unreasonably large amount of computational power to break the system. Theoretical Limit of Adversary and Security Honest-but-curious adversary, Information theoretically secure: tolerate up to \\(n/2\\) corrupt parties Honest-but-curious adversary, Computationally secure: tolerate up to \\(n-1\\) corrupt parties Malicious adversary, Information theoretically secure: tolerate up to \\(n/3\\) adversaries Malicious adversary, Computationally secure: tolerate up to \\(n/2\\) adversaries (unfair/un-robust protocol can tolerate up to \\(n-1\\) adversaries) Lagrange Interpolation Distribution and Recombination A polynomial \\(P(x)\\) of degree \\(T\\) can be uniquely determined by a set of \\(T + 1\\) (or more) distinct points on the polynomial curve \\(P(x)\\). A polynominal \\(P(x)\\) of degree upto at most \\(N − 1\\) there exist coefficients \\(r = (r_1,\\cdots,r_N )\\) (called the recombination vector) such that: \\[P(0)=\\sum_{i=1}^Nr_iP(i)\\] Lagrange Interpolation Given \\(T\\) points \\((x_1,P(x_1)),\\cdots,(x_n,P(x_n))\\), the unique Lagrange Interpolation Polynomial of degree T is: \\[P(x)=\\sum_{i=1}^{T+1}\\delta_i(x)P(x_i),~\\delta_i(x)=\\prod_{j=1,j\\neq i}^{T+1}\\frac{x-x_j}{x_i-x_j}\\] We also have: \\[P(0)=\\sum_{i=1}^{T+1}\\delta_i(0)P(x_i)=\\sum_{i=1}^{T+1}r_iP(x_i)\\Rightarrow r_i=\\prod_{j=1,j\\neq i}^{T+1}\\frac{x_j}{x_j-x_i}\\] And when \\(x_i=i\\): \\[r_i=\\prod_{j=1,j\\neq i}^{T+1}\\frac{j}{j-i}\\] Secret Share You may have already noticed that Lagrange Interpolation can be used to share(split) secret. For example, if someone have a secret \\(a_0\\), and he randomly choose coefficients \\(a_1,\\cdots,a_T\\), we obtain a polynomial \\(P(x)=a_0+\\sum_{i=1}^Ta_ix^i\\). And if he sends shares \\(P(1),\\cdots,P(N)\\) to \\(N\\) parties, then the only way to recover the secret is perform recombination on at least \\(T+1\\) shares (from at least \\(T+1\\) parties). BGW Protocol From the definition of MPC, we need to calculate a public function \\(f\\). For convenience, we consider \\(f:\\mathcal{N^*}\\rightarrow\\mathcal{N}\\) which is a polynomial over a finite field modulo a prime \\(p\\). Thus, we can only consider two gates(operations): Add(addition) and Mul(multiplication). In this post, we'll only take a look at the honest-but-curious version. Arithmetic over a finite field rules out the possibility of 'guessing' the secret. It makes us impossible to determine the range of the secret with less than \\(T+1\\) shares. See wikipedia for more details. We will look at a three-party example. Three parties are \\((p_1,p_2,p_3)\\) and their secrets(private values) are \\((x_1,x_2,x_3)=(10,20,30)\\). They want to calculate \\((x_1+x_2)\\times x_3\\). They choose prime \\(p=101\\) and degree \\(T=1\\). We also need \\(2T\\leq N-1\\), the reason of which is lied in the Mul gate. Distribution At first, each party needs to distribute it secret into \\(N\\) shares and send them to \\(N\\) parties (also include saving his own share). For \\(p_1\\), he chooses polynomial \\(3x+10\\), and the shares are \\(13, 16, 19\\). For \\(p_2\\), he chooses polynomial \\(2x+20\\), and the shares are \\(22, 24, 26\\). For \\(p_3\\), he chooses polynomial \\(1x+30\\), and the shares are \\(31, 32, 33\\). After distribution \\(p_1\\) has shares \\(13,22,31\\) \\(p_2\\) has shares \\(16,24,32\\) \\(p_3\\) has shares \\(19,26,33\\) Add Gate Suppose we have two secrets \\(a\\) and \\(b\\) which are shared using the polynomials: \\[f(x)=a+f_1x+\\cdots,f_tx^T,g(x)=b+g_1x+\\cdots,g_tx^T\\] Each of our parties has a share \\(a^{(i)}=f(i)\\) and \\(b^{(i)}=g(i)\\). Now consider the polynomial: \\[h(x)=f(x)+g(x)\\] This polynomial provides a sharing of the sum \\(c = a + b\\), and we have \\[c^{(i)}=h(i)=f(i)+g(i)=a^{(i)}+b^{(i)}\\] Let's go into the example. We want to calculate \\(x_1+x_2\\) first. And for each party, he needs to add the share from \\(p_1\\) and the share from \\(p_2\\). \\(p_1\\) calculates \\(a^{(1)}+b^{(1)}=13+22=35\\) \\(p_2\\) calculates \\(a^{(2)}+b^{(2)}=16+24=40\\) \\(p_3\\) calculates \\(a^{(3)}+b^{(3)}=19+26=45\\) And you can see if we combine these values together by using the recombination vector, we have \\([3,-3,1][35,40,45]^\\top=30\\), which equals to the 'true' value \\(x_1+x_2=30\\). Multiplication Gate To compute the Multiplication gate we perform the following four steps: Each party \\(i\\) locally computes \\(d^{(i)}=a^{(i)}\\times b^{(i)}\\) Each party \\(i\\) perform distribution of \\(d^{(i)}\\) (party \\(j\\) receives the sub-share \\(d_{i,j}\\) from party \\(i\\)) by polynomial \\(\\delta_i(x)\\) of degree \\(T\\) Each party \\(j\\) recombination the sub-shares as \\(c^{(j)}=\\sum_{i=1}^Nr_i\\times d_{i,j}\\) So why does this work? Consider the first step; here we are actually computing a polynomial of degree \\(2T\\). Hence, we need to 'reduce' the degree of this polynomial. Thus, we produce \\(\\delta_i(x)\\) in the second step. Consider what happens when we recombine them using the recombination vector, i.e. let: \\[h(x)=\\sum_{i=1}^Nr_i\\delta_i(x)\\] Then we have \\[h(0)=\\sum_{i=1}^Nr_i\\delta_i(0)=\\sum_{i=1}^Nr_id^{(i)}=c\\] \\[h(j)=\\sum_{i=1}^Nr_i\\delta_i(j)=\\sum_{i=1}^Nr_id_{i,j}=c^{(j)}\\] Thus, \\(h(x)\\) is a polynomial which could be used to share the value of the product and underlies the sharing produced in the final step. Let's take a look at the example. \\(p_1,p_2,p_3\\) calculates \\(d^{(1)}=35\\times31=75,d^{(2)}=68,d^{(3)}=71\\) \\(p_1\\) distributes \\(78,81,84\\), \\(p_2\\) distributes \\(70,72,74\\), \\(p_3\\) distributes \\(72,73,74\\) \\(p_1\\) has shares \\(78,70,72\\), \\(p_2\\) has shares \\(81,72,73\\), \\(p_3\\) has shares \\(84,74,74\\) \\(p_1,p_2,p_3\\) recombines as \\(c^{(1)}=[3,-3,1][78,70,72]^\\top=96,c^{(2)}=100,c^{(3)}=3\\) Recombination Each party broadcasts its value, and performs recombination. As the end of the example, each party obtains \\(96,100,3\\) and calculates the result as \\([3,-3,1][96,100,3]^\\top=92\\), which equals to the 'true' value \\((x_1+x_2)\\times x_3=92\\). Reference Smart, Nigel P. Cryptography made simple. Springer, 2016."},{"title":"Differential Privacy","slug":"Differential Privacy","date":"un11fin11","updated":"un11fin11","comments":true,"path":"2020/11/02/Differential Privacy/","link":"","permalink":"https://zs-liu.github.io/2020/11/02/Differential%20Privacy/","excerpt":"","text":"Definition Neighboring dataset For any dataset \\(D,D^{\\prime}\\in\\mathcal{D}\\), \\(D,D^{\\prime}\\) are neighboring datasets if and only if one of the following two statements is true: \\(\\exists x\\in D^{\\prime}\\), s.t. \\(D^{\\prime}=D\\cup\\{x\\}\\) \\(\\exists x\\in D\\), s.t. \\(D=D^{\\prime}\\cup\\{x\\}\\) Two neighboring datasets always have different sizes, i.e. \\(|D_1|-|D_2|=\\pm1\\) Differential privacy Let \\(M:\\mathcal{D}\\rightarrow Y\\) be a randomized mechanism. \\(M\\) is \\(\\varepsilon\\)-differentially private if, for any neighboring datasets \\(D,D^{\\prime}\\in\\mathcal{D}\\) and any \\(S\\subset \\text{img }M\\), we have: \\[\\text{Pr}[M(D)\\in S]\\leq e^{\\varepsilon}\\text{Pr}[M(D^{\\prime})\\in S]\\] Differential privacy guarantees that the result of a certain query on the dataset must be essentially the same irrespective of the presence of any single individual in the dataset. Laplace mechanism Global sensitivity Let \\(f:\\mathcal{D}\\rightarrow \\mathbb{R}^k\\), with \\(f(D)=(f_1(D),f_2(D),\\cdots,f_k(D))\\). The global sensitivity of \\(f\\) is: \\[\\Delta f=\\max_{D_1,D_2}||f(D_1)-f(D_2)||_1=\\max_{D_1,D_2}\\sum_{i=1}^k|f_i(D_1)-f_i(D_2)|\\] where \\(D_1,D_2\\) can be any arbitrary neighboring datasets in \\(\\mathcal{D}~\\). Laplace mechanism For any \\(f:\\mathcal{D}\\rightarrow\\mathbb{R}^k\\), the mechanism \\(M:\\mathcal{D}\\rightarrow\\mathbb{R}\\) defined by \\[M(D)=f(D)+\\text{Lap}(\\Delta f/\\varepsilon)\\] is \\(\\varepsilon\\)-DP. This is called the Laplace mechanism. When \\(\\Delta f\\) is unbounded (or very large), we cannot apply the simple Laplace mechanism without destroying utility. Fortunately, in many cases it is still possible to use other mechanisms. Composition Theorem Let \\(M_1,\\cdots,M_k\\) be such that every \\(M_i\\) is an \\(\\varepsilon_i\\)-DP mechanism. Then \\((M_1,\\cdots,M_k)\\) is an \\(\\sum_{i=1}^k\\varepsilon_i\\)-DP mechanism. This theorem is a corollary from the global sensitivity definition. Actually, we can decide on an \\(\\varepsilon\\), called the privacy budget, which then defines the total number of queries anyone can run on the dataset and the noise added to the query results. For example, 10 queries at \\(\\varepsilon_i=0.1\\) can protect privacy as good as 2 queries at \\(\\varepsilon_i=0.5\\), because they both have budget \\(\\varepsilon=1\\). Optimized mechanism for histograms A histogram consists of queries \\[\\text{COUNT } Q_i: x\\in X_i\\] where \\(\\cap_{i=1}^kX_i=\\varnothing\\) . If we add \\(\\text{Lap}(1/\\varepsilon)\\) noise to each query result, then the whole mechanism is \\(\\varepsilon\\)-DP. This optimization is a corollary from the global sensitivity definiton. Group privacy Any \\(\\varepsilon\\)-DP mechanism \\(M\\) is \\(k\\varepsilon\\)-DL for groups of size \\(k\\). This means that it's \"\\(k\\varepsilon\\) hard\" to determine whether any entire group of \\(k\\) individuals belongs to the dataset, based on the output of \\(M\\). Local differential privacy Introduction In local differential privacy, every user “adds noise” to his own data (locally) and then shares the “privatized” data with the analyst. The analyst can then run any computation on these privatized data. Local differential privacy Let \\(R\\) be a set of responses. A randomized algorithm \\(M:R\\rightarrow Y\\) is \\(\\varepsilon\\)-local differentially private if, for responses \\(r_1,r_2\\in R\\) and any \\(S\\subset \\text{img }M\\), we have: \\[\\text{Pr}[M(r_1)\\in S]\\leq e^{\\varepsilon}\\text{Pr}[M(r_2)\\in S]\\] Example Suppose a professor wants to conduct a survey among students asking the question: “Have you cheated at the exam?”. Every student is asked to answer YES or NO, following these steps: Flip a biased coin, with probability of tails \\(p &gt; 0.5\\) If tails, then respond truthfully If heads, then lie. For this mechanism \\[\\varepsilon=\\log(\\frac{p}{1-p})\\]"},{"title":"Query-based System","slug":"Query-based System","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2020/10/26/Query-based System/","link":"","permalink":"https://zs-liu.github.io/2020/10/26/Query-based%20System/","excerpt":"","text":"Introduction Query-based systems aim at giving researchers and organizations anonymous access to the data without sharing with them the individual-level data This can be through online interfaces, SQL queries, verified algorithms, etc. which would only return aggregated data In this post, we only consider COUNT interface Query size restriction (QSR) Definition A query \\(Q\\) is a logical formula that selects a specific set of rows. Define: \\[Q= C_1\\land \\cdots \\land C_h\\] \\[\\{Q\\}_D=\\{x|x\\in D,Q(x)=\\text{True}\\}\\] where \\(C_i\\) is an expression of the form \"attribute ? value\". QSR imposes that every query such that \\(|\\{Q\\}_D|\\leq t\\) is blocked where \\(t\\) is the threshold made by the data curator. Intersection Attack COUNT Q1: students in DoC AND code with Notepad? COUNT Q2: students in DoC AND born on 1994-09-23 AND code with Notepad? Counting \\(Q_1\\) and \\(Q_2\\) are likely to have answers \\(A_1,A_2&gt;t\\), so they won’t be blocked by the query set size restriction. However, if \\(A_1-A_2=0\\), we will know that the person born on 1994-09-23 doesn’t code with Notepad. Intersection attack uses the answers of multiple queries to learn information about a single individual. It has been shown that detecting any intersection attacks is an NP-hard problem. Unbiased noise addition Bounded noise Bounded noise will leak information if the attacker knows the noise mechanism. For example, noise \\(u\\sim U[-2,2]\\) is added to the above intersection attack query. If we obtain the result \\(\\hat{A_1}=A_1+2=586,\\hat{A_2}=A_2-2=581\\) and we know that only Bob born on that day, we will know \\(A_1=584,A_2=583\\) with certainty because \\(A_1,A_2\\) differs at most \\(1\\). Unbounded noise Centered at 0, as to not introduce biases Large perturbations are possible but unlikely Averaging attack Averaging attack asks the same question several times and takes the average to find the right value. Formally, this can be done in two ways (frequentist and Bayesian): Compute the average of the samples and apply the central limit theorem (CLT). CLT says that this average converges to the mean (i.e. the true value) Use Bayes’ rule with multiple observations. This method immediately gives not only the most likely value (the average), but also a full posterior distribution Assume Gaussian noise \\(N(0,\\sigma^2)\\) is added. We can conclude that if the attacker wants to decide whether Bob is in the dataset and his query number \\(n\\geq 4\\sigma^2z_{\\alpha}^2\\), the attacker will have confidence (probability of making an incorrect prediction) \\(\\alpha\\) . Consistent noise addition If our noises were to be consistent, we wouldn’t learn anything by asking the same question again. Consistent noise can be achieved through: Caching, basically making sure that we cache the noisy result of every query and return this if the same question is asked again Seeded pseudorandom number generator (PRNG): in short, we seed our noise generator to ensure that when the query is the same, the exact same noise is added. The seed could e.g. be the hash of all the parameters of the query Semantic attack Tf the query language we use is expressive enough, there often exist multiple ways to ask the same questions: COUNT Q1: students in DoC AND code with Notepad AND born between 1994-09-23 00:00 and 1994-09-24 00:00? COUNT Q2: students in DoC AND code with Notepad AND born between 1994-09-23 00:01 and 1994-09-24 00:01? When we get a great number of answers, we are able to cancel out the consistent noise by taking average. Diffix's sticky noise Diffix's output of counting \\(Q\\) is: \\[\\text{COUNT }\\tilde{Q}(D)=\\text{COUNT }Q(D)+\\sum_{i=1}^h\\text{static}[C_i]+\\sum_{i=1}^h\\text{dynamic}_Q[C_i]\\] Static noise For each condition \\(C_i\\), the noise value \\(\\text{static}[C_i]\\) is generated by drawing a random value from \\(N(0,1)\\). The random value is generated using a PRNG seeded with: \\[\\text{static_seed}_{C_i}=\\text{hash}(C_i,\\text{salt})\\] Dynamic noise For each condition \\(C_i\\), the noise value \\(\\text{dynamic}_Q[C_i]\\) is generated by drawing a random value from \\(N(0,1)\\). The random value is generated using a PRNG seeded with: \\[\\text{dynamic_seed}_{C_i,Q}=\\text{hash}(\\text{static_seed}_{C_i},\\{Q\\}_D)\\] Bucket suppression Besides sticky noise, Diffix includes another protection. This is called bucket suppression, and it is a more sophisticated version of QSR. The idea is the same: block any query that selects a set of users with size smaller than a certain threshold. However, the threshold is not fixed, but noisy as well: if \\(|\\{Q\\}_D|\\leq1\\), the query gets suppressed; if \\(|\\{Q\\}_D|&gt;1\\), it draws a noisy threshold \\(t\\) from \\(N(4,1/2)\\) by the seed: \\[\\text{threshold_seed}=\\text{hash}(\\{Q\\}_D,salt)\\] Safe? Dynamic noise helps protect against many sophisticated attacks, see here. However, the developers of Diffix do not have a mathematical proof that their system protects against any attack. And they acknowledge that. In October 2018, Aloni Cohen and Kobbi Nissim published a report on their attack on Diffix. This attack is an adaptation of a reconstruction attack, proposed in 2003 by Dinur and Nissim. In October 2018, some researchers from UCL and EPFL published a blogpost about their attack on Diffix, based on a previous paper. There is no technical report available for this attack yet. In April 2018, researchers from Imperial College London designed an attack to infer a user’s attribute with high accuracy, see [here] (https://cpg.doc.ic.ac.uk/blog/aircloak-diffix-signal-is-in-the-noise/). They called it a noise-exploitation attack."},{"title":"Data Pseudonymization and Anonymization","slug":"Data Pseudonymization and Anonymization","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2020/10/19/Data Pseudonymization and Anonymization/","link":"","permalink":"https://zs-liu.github.io/2020/10/19/Data%20Pseudonymization%20and%20Anonymization/","excerpt":"","text":"Pseudonymization To remove all direct identifiers, e.g. name, phone number, address, social security number etc, and just give each person a unique id that cannot be linked to them: Lookup table quickly becomes cumbersome to save it and keep it up-to-date as new data arrives Secret formula easy to store but easy to 'fit' the secret formula by only a few points collisions are an issue Cryptographic hash functions easy to store, easy to compute, no collision, fixed length hash function are not a secret information, thus the attacker is able to build a lookup table by iterating over all possible IDs Cryptographic hash functions with SALT salt is a fixed string of arbitrary length (but long!) that is added to the identifier before hashing it salt must be kept secret md5(\"19477\") =&gt; md5(\"19477youwillneverguesswhatthissaltis\") if the salt is long enough (and secret) it cannot be brute-forced However, even a properly pseudonymized table is not safe. ID Gender DOB Zip-code Sensitive Data f1f333... Male 28-09-1955 4444 **** 93db7... Female 12-03-1959 4334 **** What if I were to know that the person I’m searching for is a man born on 28-09-1955 and his zip-code is 4444? 63% of the US population is unique given a date of birth, zip code, and sex (Golle, 2006). Terminology Sensitive information: A piece of information about an individual (e.g. disease, drug use) we’re trying to protect (but is relevant for the application). Identifier: A piece of information that directly identifies a person (name, address, phone number, ip address, passport number, etc). Quasi-identifier: A piece of information that does not directly identify a person (e.g. nationality, date of birth). But multiple quasi-identifiers taken together could uniquely identify a person. A set of quasi-identifiers could be known to an attacker for a certain individual (auxiliary info). Auxiliary information: Information known to an attacker. Uniqueness Attack Definition Uniqueness w.r.t. \\(A\\): fraction of the dataset that is uniquely identified by the set \\(A\\) of quasi-identifiers. \\(k\\)-anonymity A table is \\(k\\)-anonymous if every record in the table is indistinguishable from at least \\(k-1\\) other records, with respect to every set of quasi-identifiers. This means that even if an attacker knows all possible quasi-identifiers, she cannot identify his target uniquely. An equivalence class is a set of records that have the same values for all the quasi-identifiers. Loss of information \\[H(D)=\\sum_{i=1}^k\\frac{\\#C_i}{N}\\log\\frac{\\#C_i}{N}\\] where \\(N\\) is the amount of rows in the dataset \\(D\\); \\(C_1 ,\\cdots, C_k\\) are the equivalence classes; \\(\\#C_i\\) indicates the number of rows that belong to \\(Ci\\). The higher the entropy, the more information is contained in \\(D\\). Homogeneity Attack Definition A homogeneity attack can take place when individuals in the same equivalence class all have the same sensitive attribute value. ID Gender DOB Zip-code Sensitive Data f1f333... Male 1955 Las Vegas gastritis 34dera... Male 1955 Las Vegas gastritis What if I know that the person I’m searching for is a man, born in 1955, living in Las Vegas? \\(l\\)-diversity An equivalence class is \\(l\\)-diverse if it contains at least \\(l\\) distinct values for the sensitive attributes. A table is \\(l\\)-diverse if every equivalence class is \\(l\\)-diverse. Not enough yet... Definition A semantic attack can take place when sensitive attributes of individuals in an equivalence class are distinct but semantically similar. For example, skin cancer and breast cancer are both cancer. A skewness attack (here it is probabilistic) takes place when the distribution of the sensitive attributes in a class is skewed. In the general population, 99% might test negative for illegal drugs but, in an equivalence class, only 15% test negative. I learned something about people in this class. \\(t\\)-closeness An equivalence class is said to have \\(t\\)-closeness if the distance between the distribution of a sensitive attribute in this class and the distribution of this attribute in the whole table is no more than a threshold \\(t\\). A table is said to have \\(t\\)-closeness if all equivalence classes have \\(t\\)-closeness. Final reminder Anonymization is hard for small data and probably impossible for big data protect a dataset against a whole range of attacks: uniqueness, homogeneity, semantic, skewness, matching (unicity), profiling by anonymizing it once and only once all the while preserving utility (for all current and future uses)"},{"title":"好久不见","slug":"hello","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2020/10/14/hello/","link":"","permalink":"https://zs-liu.github.io/2020/10/14/hello/","excerpt":"","text":"许久不见啦！去年十月开始我忙于申请，然后紧接着又是毕业的相关事宜，以致我许久没有更新。我正在着手写一点关于我在清华四年的东西，希望能尽快发上来。与此同时，我也可能发一些我在IC的笔记（和以前一样）。 另外，我更新了博客的主题，希望大家都能喜欢 :)"},{"title":"Multi Armed Bandit","slug":"Multi Armed Bandit","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/10/28/Multi Armed Bandit/","link":"","permalink":"https://zs-liu.github.io/2019/10/28/Multi%20Armed%20Bandit/","excerpt":"Multi Armed Bandit \\(t=1,2,\\cdots,N\\) \\(k\\) arm at \\(t\\), choose arm \\(i\\), receive reward \\(X_{i,t}\\sim{}D_i\\)(iid unknown distribution) Policy: \\(\\pi=\\{i(t),t=1,2,\\cdots,N\\}\\) Goal: \\(\\max\\mathbb{E}\\{\\sum_{t=1}^NX_{i,t}\\}\\) \\({\\rm OPT}=N\\max_i\\mu_i=N\\mu,\\mu_i=\\mathbb{E}\\{X_{i,t}\\}\\) \\({\\rm Reg}={\\rm OPT}-\\mathbb{E}\\{\\sum_{t=1}^NX_{i,t}|\\pi\\}\\)","text":"Multi Armed Bandit \\(t=1,2,\\cdots,N\\) \\(k\\) arm at \\(t\\), choose arm \\(i\\), receive reward \\(X_{i,t}\\sim{}D_i\\)(iid unknown distribution) Policy: \\(\\pi=\\{i(t),t=1,2,\\cdots,N\\}\\) Goal: \\(\\max\\mathbb{E}\\{\\sum_{t=1}^NX_{i,t}\\}\\) \\({\\rm OPT}=N\\max_i\\mu_i=N\\mu,\\mu_i=\\mathbb{E}\\{X_{i,t}\\}\\) \\({\\rm Reg}={\\rm OPT}-\\mathbb{E}\\{\\sum_{t=1}^NX_{i,t}|\\pi\\}\\) UCB ALG Initialize: play machine arm once Loop: play machine \\[i=\\arg\\max\\bar{x}_j+\\sqrt{\\frac{2\\ln{}n}{n_j}}\\] where \\(\\bar{x}_j={\\rm avg}\\) reward of arm \\(j\\), \\(n_j=\\#\\) of times \\(j\\) is played so far Theorem For all \\(k&gt;1\\), if UCB runs on \\(k\\) arms with distributions \\(P_1,P_2,\\cdots,P_k\\) with support \\([0,1]\\), then the expected regrest after any \\(n\\) is at most: \\[\\delta\\sum_{i,\\mu_i&lt;\\mu}\\frac{\\ln{}n}{\\Delta_i}+(1+\\frac{\\pi^2}{3})\\sum_{j=1}^k\\Delta_j\\] where \\(\\Delta_i=\\mu-\\mu_i\\) Proof. Define \\(C_{t,s}=\\sqrt{\\frac{2\\ln{}t}{s}}\\) \\[T_i(n)=1+\\sum_{t=k+1}^n\\{i(t)=t\\}\\leq{}l+\\sum_{t=k+1}^n\\{i(t)=i,T_i(t-1)\\geq{}l\\}\\] After some calculation, we found that \\[T_i(n)\\leq{}l+\\sum_{t}\\sum_{s=1}^{t-1}\\sum_{s_i=l}^{t-1}\\{\\bar{x}_s^\\star+C_{t,s}\\leq\\bar{x}_i+C_{t,s_i}\\}\\] Also, it's true that \\[\\bar{x}_s^\\star+C_{t,s}\\leq\\bar{x}_i+C_{t,s_i}\\] holds if at least one of following holds \\[\\bar{x}_s^\\star\\leq\\mu-C_{t,s},\\bar{x}_{i,s_i}\\geq\\mu_i+C_{t,s},\\mu&lt;\\mu_i+2C_{t,s}\\] However, the first two probability can be bound by Chernoff bound and the last one is impossible (probability \\(= 0\\)). \\[{\\rm Pr}\\{\\bar{x}_s^\\star\\leq\\mu-C_{t,s}\\}\\leq{}e^{-4\\ln{}t},{\\rm Pr}\\{\\bar{x}_{i,s_i}\\geq\\mu_i+C_{t,s}\\}\\leq{}e^{-4\\ln{}t}\\] Finally, choose \\[l=\\lceil\\frac{8\\ln{}n}{\\Delta_i^2}\\rceil\\] \\[\\mathbb{E}\\{T_i(n)\\}\\leq{}l+\\sum_{t}\\sum_{s=1}^{t-1}\\sum_{s_i=l}^{t-1}t^{-4}=l+1+\\frac{\\pi^2}{3}\\] \\(\\varepsilon\\)-greedy ALG Parameter: \\(c&gt;0,0&lt;d&lt;1\\) Initialize: Define \\[\\varepsilon_n=\\min\\{1,\\frac{ck}{d^2n}\\}\\] Loop: \\(\\forall{}n=1,2,\\cdots\\), let \\(i_n=\\arg\\max_j\\bar{x}_{j,n}\\). Play \\(i_n\\) with probability \\(1-\\varepsilon_n\\) and random with probability \\(\\varepsilon_n\\) Theorem For all \\(k&gt;1\\), if \\(\\varepsilon\\)-greedy runs on \\(k\\) arms with distributions \\(P_1,P_2,\\cdots,P_k\\) with support \\([0,1]\\) and \\(0&lt;d&lt;\\min_{i,\\mu_i&lt;\\mu}\\Delta_i\\). Then, the probability that after \\(n\\geq{}ck/d\\) plays, \\(\\varepsilon\\)-greedy choose a sub-opt \\(j\\) is at most \\[\\mathcal{O}(\\frac{c}{d^2n}+o(\\frac{1}{n}))\\] for \\(n\\rightarrow\\infty\\) and \\(c&gt;5\\). Lower Bound Consider two \\(\\Theta\\) values, i.e., \\(\\Theta=(\\theta_1,\\theta_2,\\cdots,\\theta_k)\\) with \\(\\mu_1&gt;\\mu_2\\geq\\dots\\geq\\mu_k\\); \\(\\Theta^{\\prime}=(\\theta_1,\\theta_2^\\prime,\\cdots,\\theta_k)\\) with \\(\\mu_2^{\\prime}&gt;\\mu_1\\geq\\dots\\geq\\mu_k\\). Here we want to prove that it costs about \\(\\ln{}n\\) time to tell the difference between this two. For a strategy - \\(x_{j,s}\\): reward for \\(j\\) at \\(s\\)-th slot - \\(\\mathbb{P}\\): joint distribution over \\(\\{I_t,x_{j,s}\\}\\) under \\(\\Theta\\) - \\(\\mathbb{P}^{\\prime}\\): joint distribution over \\(\\{I_t,x_{j,s}\\}\\) under \\(\\Theta^\\prime\\) Consider \\(A\\subset\\{T_2(n)=n_2\\}\\) \\[\\mathbb{P}(A)=\\int_A\\prod_{s=1}^{n_2}\\frac{d\\mathbb{P}_{\\theta_2^\\prime}}{d\\mathbb{P}_{\\theta_2}}(x_{2,s})d\\mathbb{P}=\\int_A\\exp\\{\\sum_{s=1}^{n_2}\\ln\\frac{d\\mathbb{P}_{\\theta_2^\\prime}}{d\\mathbb{P}_{\\theta_2}}(x_{2,s})d\\mathbb{P}\\}=\\int_Ae^{-L_{n_2}}d\\mathbb{P}\\] \\[\\mathbb{P}^\\prime(A)\\geq{}e^{-C_n}\\mathbb{P}(A)\\] Thompson Sampling ALG Here we only consider Bernoulli Bandits with \\(N=2\\), because the overall analysis is really complex. \\(X_1,X_2\\in\\{0,1\\}\\), \\(\\mu_1\\geq\\mu_2\\) Initialize: for each \\(i\\), \\(S_i=F_i=0\\) Loop: \\(t=1,2,\\cdots,d_0\\) For each \\(i\\), sample \\(\\theta_i(t)\\sim{\\rm Beta}(S_i+1,F_i+1)\\) Play \\(i(t)=\\arg\\max\\theta_i(t)\\), observe reward \\(r_t\\) If \\(r_t=1\\), \\(S_{i(t)}+=1\\), else \\(F_{i(t)}+=1\\) Theorem In this \\(2\\) Bernoulli Bandits, TS achieves \\[\\mathbb{E}\\{R(T)\\}=\\mathcal{O}(\\frac{\\ln{}T}{\\Delta}+\\frac{1}{\\Delta^3})\\] where \\(\\Delta=\\mu_\\max-\\mu_\\min\\) Proof. Denote \\(j_0=\\#\\) of plays of arm \\(1\\) until arm \\(2\\) is played \\(L=24\\ln{}T/\\Delta^2\\) times. Let \\(t_j=\\) time of the \\(j^{\\rm th}\\) play of arm \\(1\\) (\\(t_0=0\\)). Also, let \\(y_j=t_j-t_{j-1}-1\\), i.e., the interval play time. Denote \\(S(j)=\\#\\) of success in the first \\(j\\) plays of arm \\(1\\) The expected \\(\\#\\) of play of arm \\(2\\): \\[\\mathbb{E}\\{K_2(T)\\}\\leq{}L+\\mathbb{E}\\{\\sum_{j=j_0}^{T-1}y_j\\}\\] Denote \\(X(j,s,y)\\) to be the \\(\\#\\) of attempts before we get a sample from \\({\\rm Beta}(s+1,j-s+1)\\) to exceed \\(y\\) It can be seen that \\(X(j,s,y)\\) is Geo with success probability \\(1-F_{s+1,j-s+1}^{\\rm beta}(y)\\) Lemma 1 For all non-negative \\(j\\) an d \\(s\\leq{}j\\) and \\(y\\in[0,1]\\) \\[\\mathbb{E}\\{X(j,s,y)\\}=\\frac{1}{F_{j+1,y}^{\\rm beta}(s)}-1\\] Consider the \\(\\#\\) of steps before event \\(\\{\\theta_1(t)&gt;\\mu_2+\\Delta/2\\}\\) first happens Given \\(S(j)\\), this has the same distribution as \\(X(j,S(j),\\mu_2+\\Delta/2)\\) \\(y_j\\) can be larger than this \\(\\#\\) iff at some \\(t\\in(t_j,t_{j+1})\\), \\(\\theta_2(t)&gt;\\mu_2+\\Delta/2\\) Thus: \\[\\mathbb{E}\\{y_j\\}\\leq\\mathbb{E}\\{\\min(X(j,S(j),\\mu_2+\\Delta/2),T)\\}+\\mathbb{E}\\{\\sum_{t=t_j+1}^{t_{j+1}-1}T\\cdot{}I\\{\\theta_2&gt;\\mu_2+\\Delta/2\\}I\\{j&gt;j_0\\}\\}\\] Sum it up from \\(j=j_0\\) to \\(T-1\\) and define event: \\[E_2(t)=\\{\\theta_2(t)\\leq\\mu_2+\\Delta/2\\lor{}K_2(t)\\leq{}L\\}\\] Lemma 2 \\[\\mathbb{P}(E_2(t))\\geq1-\\frac{2}{T^2}\\] Lemma 3 Consider any \\(y&lt;\\mu_1\\), let \\(\\Delta^1=\\mu_1y\\), let \\(R=\\frac{\\mu_1(1-y)}{y(1-\\mu_1)}&gt;1\\) and \\(D=D(y||\\mu_1)=y\\ln\\frac{y}{\\mu_1}+(1-y)\\ln\\frac{1-y}{1-\\mu_1}\\), we have: \\[\\mathbb{E}\\{\\mathbb{E}\\{\\min(X(j,S(j),y),T)|S(j)\\}\\}\\leq\\frac{16}{T}\\quad{\\rm for }\\quad{}j\\geq\\frac{4\\ln{}T}{(\\Delta^1)^2}\\] Substitute Lemma 2 and Lemma 3 into the sum-up result (which we omit) and the inequality \\[\\mathbb{E}\\{K_2(T)\\}\\leq{}L+\\mathbb{E}\\{\\sum_{j=j_0}^{T-1}y_j\\}\\] we have (skip lots lots of calculation): \\[\\mathbb{E}\\{K_2(T)\\}\\leq{}\\frac{40\\ln{}T}{\\Delta^2}+\\frac{48}{\\Delta^4}+18\\] \\[\\mathbb{E}\\{R(T)\\}=\\mathcal{O}(\\frac{\\ln{}T}{\\Delta}+\\frac{1}{\\Delta^3})\\] Non Stochastic: ALG EXP3 \\(K\\) actions \\(X(i)=(X_i(1),X_i(2),\\cdots),X_i(t)\\in[0,1]\\) ALG \\(=i_1,i_2,\\cdots,\\) \\(G_A(T)=\\sum_{t=1}^TX_{i_t}(t)\\) Regret \\(=\\max_j\\sum_{t=1}^TX_j(t)-G_A(T)\\) ALG EXP3 Parameter: Real \\(\\gamma\\in[0,1]\\) Initialize: \\(w_i(1)=1,\\forall{}i=1,2,\\cdots,K\\) Loop: for each \\(t=1,2,\\cdots\\) Set \\[P_i(t)=(1-\\gamma)\\frac{w_i(t)}{\\sum{}w_i(t)}+\\frac{\\gamma}{K}\\] Draw \\(i_t\\) randomly \\(\\sim{}P_1(t),\\cdots,P_K(t)\\) Get reward \\(X_{i_t}\\in[0,1]\\) For \\(j=1,2,\\cdots,k\\) \\[\\hat{X}_j(t)=\\left\\{\\begin{aligned}&amp;\\frac{X_j(t)}{P_j(t)}&amp;j=i_t\\\\&amp;0&amp;\\text{otherwise}\\end{aligned}\\right.\\quad\\quad{}w_j(t+1)=w_j(t)\\exp(\\frac{\\gamma\\hat{X}_j(t)}{K})\\] Theorem For any \\(K\\) and any \\(\\gamma\\in[0,1]\\), \\[G_\\max-\\mathbb{E}\\{G_{\\rm exp3}\\}\\leq(e-1)\\gamma{}G_\\max+\\frac{K\\ln{}K}{\\gamma}\\] for any reward assignments Corollary Assume \\(g\\geq{}G_\\max\\) and \\(\\gamma=\\min\\{1,\\sqrt{\\frac{K\\ln{}K}{(e-1)g}}\\}\\), then \\[G_\\max-\\mathbb{E}\\{G_{\\rm exp3}\\}\\leq2.63\\sqrt{gK\\ln{}K}\\]"},{"title":"Heavy Traffic Analysis","slug":"Heavy Traffic","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/09/23/Heavy Traffic/","link":"","permalink":"https://zs-liu.github.io/2019/09/23/Heavy%20Traffic/","excerpt":"Introduction Single Hop System, \\(L\\) Queues \\(Q_l(t=1)=(Q_l(t)+A_l(t)-S_l(t))^+=Q_l(t)+A_l(t)-S_l(t)+U_l(t)\\) \\(U_l(t)=\\max\\{0,S_l(t)-A_l(t)-Q_l(t)\\}\\)","text":"Introduction Single Hop System, \\(L\\) Queues \\(Q_l(t=1)=(Q_l(t)+A_l(t)-S_l(t))^+=Q_l(t)+A_l(t)-S_l(t)+U_l(t)\\) \\(U_l(t)=\\max\\{0,S_l(t)-A_l(t)-Q_l(t)\\}\\) \\(~\\) Lemma 1 For an aperiodic and irreducible Markov Chain, over a countable state-space, suppose \\(Z:\\mathcal{X}\\rightarrow{}R\\) is a non-negative Lyapunor function. Define \\[\\Delta(Z(X))=[Z(X[t+1])-Z(X[t])]I(X(t)=x)\\] Suppose \\(\\exists\\eta&gt;0\\) and \\(k&lt;\\infty\\) s.t. \\[\\mathbb{E}\\{\\Delta(Z(X))|X(t)=x\\}\\leq-\\eta\\text{ for all }X\\in\\mathcal{X},Z(X)\\geq{}k\\] \\(\\exists{}D\\leq\\infty\\) s.t. \\[Pr\\{|\\Delta(Z(X))\\leq0\\}=1,~\\forall{}X\\in\\mathcal{X}\\] Then \\(\\exists\\theta^*&gt;0\\) and \\(c^*&lt;\\infty\\) s.t. \\[\\lim\\sup_{t\\rightarrow\\infty}\\mathbb{E}\\{\\exp\\{\\theta^*Z(X[t])\\}\\}\\leq{}c^*\\] If also \\(X(t)\\) is positive recurrent, then \\(Z(X(t))\\) converges in distribution to \\(\\bar{Z}\\) with \\(\\mathbb{E}\\{\\exp\\{\\theta^*\\bar{Z}\\}\\}\\leq{}c^*\\) Join the Shortest Queue (JSQ) \\(A_{\\Sigma}(t),S_l(t)\\) iid \\(A_\\Sigma(t)\\in[0,A_\\max],S_l(t)\\in[0,S_\\max]\\) \\(\\lambda_\\Sigma=\\mathbb{E}\\{A_\\Sigma(t)\\},\\sigma_\\Sigma^2=D\\{A_\\Sigma(t)\\}\\) \\(\\mu_l=\\mathbb{E}\\{S_l(t)\\},v_l^2=D\\{S_l(t)\\}\\) \\(\\mu_\\Sigma=\\sum_l\\mu_l,v_\\Sigma^2=\\sum_lv_l^2\\) And the policy is \\(A(t)=(A_1(t),\\cdots,A_L(t))\\) \\[A(t)=\\text{RAND}\\{\\arg\\min_{A\\geq0,\\sum_lA_l=A_\\Sigma(t)}&lt;A,Q(t)&gt;\\}\\] Conclusion \\(\\lambda_\\Sigma&gt;\\mu_\\Sigma\\), unstable \\(\\lambda_\\Sigma&lt;\\mu_\\Sigma\\), JSQ stablizes that system \\(\\{Q(t)\\}\\) converges in distribution to a random variable \\(\\bar{Q}\\) whose all moments are bounded, i.e. \\(\\mathbb{E}\\{||\\bar{Q}||^r\\}=M_r\\) Analysis of Stability \\(Z(X)=V(Q)=||Q||\\) \\(W(Q)=||Q||^2\\) \\[e=\\mu_\\Sigma-\\lambda_\\Sigma,\\frac{e}{L}=\\mu_l-\\lambda_l\\] \\[\\begin{split}\\mathbb{E}\\{\\Delta(W(Q))|Q\\}&amp;=\\mathbb{E}\\{||Q(t+1)||^2-||Q(t)||^2~|Q\\}\\\\&amp;=\\mathbb{E}\\{||Q+A-S+U||^2-||Q||^2~|Q\\}\\\\&amp;=\\mathbb{E}\\{||Q+A-S||^2+2&lt;Q+A_S,U&gt;+||U||^2-||Q||^2~|Q\\}\\\\&amp;\\leq{}\\mathbb{E}\\{||Q+A-S||^2-||Q||^2~|Q\\}\\\\&amp;=\\mathbb{E}\\{2&lt;Q,A-S&gt;+||A-S||^2~|Q\\}\\\\&amp;=\\mathbb{E}\\{2&lt;Q,A-S&gt;|Q\\}+k_1\\\\&amp;=2&lt;Q,\\mathbb{E}\\{A|Q\\}-\\lambda&gt;-&lt;Q,\\mu-\\lambda&gt;+k_1\\\\&amp;=&lt;Q,\\mathbb{E}\\{A|Q\\}-\\lambda&gt;-\\frac{2e}{L}&lt;Q,1&gt;+k_1\\\\&amp;=2\\mathbb{E}\\{A_\\Sigma|Q\\}Q_\\min-2&lt;Q,\\lambda&gt;-\\frac{2e}{L}||Q||_1+k_1\\\\&amp;=2\\lambda_\\Sigma{}Q_\\min-2\\sum_l\\lambda_lQ_l-\\frac{2e}{L}||Q||+k_1\\\\&amp;=-2\\sum_l\\lambda_l(Q_l-Q_\\min)-\\frac{2e}{L}||Q||+k_1\\\\&amp;\\leq\\frac{2e}{L}||Q||+k_1\\end{split}\\] \\[\\begin{split}\\mathbb{E}\\{\\Delta{}V(Q)|Q(t)=Q\\}=&amp;\\mathbb{E}\\{||Q(t+1)||-||Q(t)||~|Q(t)=Q\\}\\\\\\leq&amp;\\frac{1}{2||Q||}\\mathbb{E}\\{||Q(t+1)||^2-||Q(t)||^2~|Q(t)=Q\\}\\\\\\leq&amp;\\frac{e}{L}+\\frac{k_1}{2||Q||}\\end{split}\\] Lower Bound Consider \\(Q_\\Sigma(t)\\) and \\(S_\\Sigma(t)=\\sum{}S_l(t)\\) which is a single server, obviously: \\[Q_\\Sigma(t)\\leq\\sum_lQ_l(t)\\] \\(\\alpha(t)&lt;\\alpha_\\max\\) \\(\\beta(t)&lt;\\beta_\\max\\) \\(\\Phi(t+1)=[\\Phi(t)+\\alpha(t)-\\beta(t)]^+\\) Lemma 2 Suppose \\(\\{\\alpha^e(t)\\}\\) satisfies \\(e=\\beta-\\alpha^e\\). Let the queue process \\(\\{\\Phi^e(t)\\}\\). Then \\(\\{\\Phi^e(t)\\}\\) is a positive recurrent Markov Chain and \\(\\{\\Phi^e(t)\\}\\rightarrow{}\\bar{\\Phi}^e\\) with \\(\\mathbb{E}\\{||\\bar{\\Phi}^e||\\}\\leq{}M_r\\) \\[\\mathbb{E}\\{\\bar{\\Phi}^e\\}\\geq\\frac{\\xi^e}{2e}-B_1\\] where \\(\\xi^e=\\sigma_{\\alpha^e}^2+\\sigma_\\beta^2+e^2,B_1=\\frac{\\beta_\\max}{2}\\) In the heavy traffic limit, \\(e\\rightarrow0\\) \\[\\lim_{e\\rightarrow0}\\inf{}e\\mathbb{E}\\{||\\bar{\\Phi}^e||\\}\\geq\\frac{\\xi}{2}\\] \\[\\begin{split}\\Phi(t+1)=\\Phi(t)+\\alpha(t)-\\beta(t)+\\chi(t),\\chi(t)=-\\max\\{0,\\beta-\\alpha-\\Phi\\}\\end{split}\\] \\[\\begin{split}W(\\Phi)=||\\Phi||^2\\end{split}\\] \\[\\begin{split}\\mathbb{E}\\{\\Delta{}W(\\Phi)|\\Phi(t)\\}&amp;=\\mathbb{E}\\{(\\Phi+\\alpha-\\beta)^2+2(\\Phi+\\alpha-\\beta)\\chi+\\chi^2-\\Phi^2|\\Phi)\\}\\\\&amp;=\\mathbb{E}\\{(\\alpha-\\beta)^2|\\Phi\\}+2\\Phi{}\\mathbb{E}\\{(\\alpha-\\beta)|\\Phi\\}-\\mathbb{E}\\{\\chi^2|\\Phi\\}\\end{split}\\] In steady state, \\(\\mathbb{E}\\{\\Delta{}W\\}=0\\), take expectation of both sides: \\[\\mathbb{E}\\{(\\alpha-\\beta)^2\\}+2\\mathbb{E}\\{\\Phi{}\\}e-\\mathbb{E}\\{\\chi^2\\}=0\\] Consider \\(\\mathbb{E}\\{\\chi^2\\}\\leq{}e\\beta_\\max\\) State Space Collapse Let \\(\\vec{c}&gt;0\\) is a vector with unit form. Let \\(Q_{\\parallel}=&lt;\\vec{c},\\vec{Q}&gt;\\vec{c}\\), \\(Q_{\\perp}=Q-Q_{\\parallel}\\) Lemma 3 Define Lyapunov function \\[V_\\perp(Q)=||Q_\\perp||,W(Q)=||Q||^2,W_{\\parallel}(Q)=||Q_\\parallel||^2\\] \\[\\Delta{}V_\\perp(Q)=V_\\perp(Q(t+1))-V_\\perp(Q(t))\\] Then: \\[\\Delta{}V_\\perp(Q)\\leq\\frac{1}{2||Q_\\perp||}(\\Delta{}W(Q)-\\Delta{}W_\\parallel(Q))\\] \\[|\\Delta{}V_\\perp(Q)|\\leq2\\sqrt{L}\\max(A_\\max,S_\\max)\\] \\[\\begin{split}\\Delta{}V_\\perp(Q)&amp;=V_\\perp(Q(t+1))-V_\\perp(Q(t))\\\\&amp;=\\sqrt{V_\\perp(Q(t+1))^2}-\\sqrt{V_\\perp(Q(t))^2}\\\\&amp;\\leq\\frac{1}{2||Q_\\perp(t)||}(||Q_\\perp(t+1)||^2-||Q_\\perp(t)||^2)\\\\&amp;=\\frac{1}{2||Q_\\perp(t)||}(\\Delta{}W(Q)-\\Delta{}W_{\\parallel}(Q))\\end{split}\\] \\[\\begin{split}|\\Delta{}V_\\perp(Q)|&amp;=|||Q_\\perp(t+1)||-||Q_\\perp(t)|||\\\\&amp;\\leq||Q_\\perp(t+1)-Q_\\perp(t)||\\\\&amp;\\leq||Q(t+1)-Q(t)||+||Q_\\parallel(t+1)-Q_\\parallel(t)||\\\\&amp;\\leq2||Q(t+1)-Q(t)||\\\\&amp;\\leq2\\sqrt{L}\\max||Q_l(t+1)-Q_l(t)||\\end{split}\\] \\[\\vec{c}=\\vec{1}\\cdot\\frac{1}{\\sqrt{L}}\\] \\[Q_\\parallel^e=\\frac{Q_\\Sigma}{L}\\vec{1},Q_\\perp=[Q_l-\\frac{1}{L}Q_\\Sigma]_{l=1}^L,Q_\\Sigma=\\sum_lQ_l\\] Proposition Consider \\(\\bar{Q}^e\\) under JSQ with \\(\\{A_\\Sigma^e\\}_t\\), and \\(e=\\mu_\\Sigma-\\lambda_\\Sigma^e\\). Then, for any \\(\\delta\\in(0,\\mu_\\max)\\), \\(\\exists{}\\) a sequence of \\(\\{N_r\\}\\), s.t. \\[\\mathbb{E}\\{||\\bar{Q}_\\perp^2||^r\\}\\leq{}N_r,\\forall{}e\\in(0,(\\mu_\\min-\\delta)L)\\] \\[\\begin{split}\\mathbb{E}\\{\\Delta{}W(Q)|Q\\}&amp;=\\mathbb{E}\\{||Q(t+1)||^2-||Q(t)||^2|Q\\}\\\\&amp;\\leq{}\\mathbb{E}\\{||Q+A-S||^2-||Q||^2|Q\\}\\\\&amp;=2\\mathbb{E}\\{&lt;Q,A-S&gt;|Q\\}+K_1\\end{split}\\] \\[\\begin{split}\\mathbb{E}\\{&lt;Q,A-S&gt;|Q\\}&amp;=&lt;Q,\\mathbb{E}\\{A|Q\\}-\\lambda&gt;-&lt;Q,\\mu-\\lambda&gt;\\\\&amp;=\\mathbb{E}\\{A_\\Sigma|Q\\}Q_\\min-&lt;Q,\\lambda&gt;-\\frac{e}{\\sqrt{L}}&lt;Q,c&gt;\\\\&amp;=\\lambda_\\Sigma{}Q_\\min-\\sum_l\\lambda_lQ_l-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\\\&amp;=-\\sum_l\\lambda_l(Q_l-Q_\\min)-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\\\&amp;\\leq\\lambda_\\min\\sum_l|Q_l-Q_\\min|-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\\\&amp;=-||Q-Q_\\min\\cdot\\vec{1}||\\cdot\\lambda_\\min-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\\\&amp;\\leq-||Q-Q_\\min\\cdot\\vec{1}||\\lambda_\\min-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\\\&amp;\\leq-||Q-\\frac{1}{L}Q_\\Sigma\\cdot\\vec{1}||\\cdot\\lambda_\\min-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\\\&amp;\\leq-\\delta||Q_\\perp||-\\frac{e}{\\sqrt{L}}||Q_\\parallel||\\end{split}\\] \\[\\begin{split}\\mathbb{E}\\{\\Delta{}W_\\parallel(Q)|Q\\}&amp;=\\mathbb{E}\\{&lt;c,Q(t+1)&gt;^2-&lt;c,Q(t)&gt;^2|Q\\}\\\\&amp;=\\mathbb{E}\\{&lt;c,Q+A-S+u&gt;^2-&lt;c,Q&gt;^2|Q\\}\\\\&amp;\\cdots\\rm{here~I~omit~something}\\cdots\\\\&amp;\\geq-\\frac{2e}{\\sqrt{L}}||Q_\\parallel||-K_2\\end{split}\\] Max Weight Assumptions: \\(\\vec{S}(t)=(S_l(t))\\in\\vec{S}\\) \\(A_l(t)\\) independent over \\(l\\) \\(A_l(t)\\) iid over \\(t\\) \\(A_l(t)\\leq{}A_\\max,S_l(t)\\leq{}S_\\max\\) Policy: \\[\\vec{S}(t)=\\text{RAND}\\{\\arg\\max_{s\\in\\vec{S}}&lt;Q(t),S&gt;\\}\\] Capacity Region \\[\\mathcal{R}=\\text{conv}(\\vec{S})\\] Assume finite size and non-negative of \\(\\vec{S}\\), then: \\[\\mathcal{R}=\\{r\\geq0,&lt;c^{(k)},r&gt;\\leq{}b^{(k)}\\}\\] \\[\\mathcal{H}^{(k)}=\\{r:&lt;c^{(k)},r&gt;=b^{(k)}\\}\\] \\[\\mathcal{F}^{(k)}=\\mathcal{H}^{(k)}\\cap{}\\mathcal{R}\\] Lemma 4 if \\(\\lambda\\notin\\mathcal{R}\\), it is unstable \\(\\lambda\\in\\text{int}\\mathcal{R}\\), \\(\\{Q_l(t)\\}\\rightarrow\\bar{Q}\\) with \\(\\mathbb{E}\\{||Q||^r\\}\\leq{}M_r\\) Lower Bound Define: \\[e^{(k)}=\\min_{r\\in\\mathcal{H}}||\\lambda-r||\\] \\[\\lambda^{(k)}=\\lambda+e^{(k)}c^{(k)}\\] \\[K_\\lambda=\\{K\\in\\{1,\\cdots,k\\},\\lambda^{(k)}\\in\\mathcal{R}\\}\\] \\[K_\\lambda^\\circ=\\{k\\in{}K_\\lambda,\\lambda^{(k)}\\in\\rm{int}\\mathcal{F}^{(k)}\\}\\] \\[\\alpha^{(k)}(t)=&lt;c^{(k)},A(t)&gt;,\\beta^{(k)}(t)=b^{(k)}\\] \\[Q_\\parallel^{(k)}=&lt;c^{(k)},Q(t)&gt;c^{(k)},Q_\\perp=Q-Q_\\parallel^{(k)}\\] Lemma 5 Assume \\(\\lambda\\in\\rm{int}\\mathcal{R}\\) with set \\(e^{(k)}\\). Then under MW, for each \\(k\\in{}K_\\lambda^\\circ\\), \\(\\exists{}\\) finite \\(\\{N_r^{(k)}\\}_{k=1,2,\\cdots}\\) s.t. \\[\\mathbb{E}\\{||\\bar{Q}_\\perp^{(k)}||^r\\}\\leq{}N_r^{(k)}\\] for all \\(e&gt;0\\), each Face \\(k\\) belonging to \\(K_\\lambda^\\circ\\) as \\(e\\rightarrow0\\) and each \\(r=1,2,\\cdots\\) Further proof is available in the reference. Reference [1] Eryilmaz A, Srikant R. Asymptotically tight steady-state queue length bounds implied by drift conditions[J]. Queueing Systems, 2012, 72(3-4): 311-359."},{"title":"Max Weight, Back Pressure and Utility Maximization","slug":"Max Weight, Back Pressure and Utility Maximization","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/09/16/Max Weight, Back Pressure and Utility Maximization/","link":"","permalink":"https://zs-liu.github.io/2019/09/16/Max%20Weight,%20Back%20Pressure%20and%20Utility%20Maximization/","excerpt":"Queue Control Problem \\(A_1(t),A_2(t),Q_1(t),Q_2(t),\\mu_1(t),\\mu_2(t)\\) \\(A_i(t)\\) is iid, \\(\\mathbb{E}\\{A_i(t)\\}=\\lambda_i\\), \\(A_i(t)\\in[0,A_{\\max}]\\) \\(S_i(t)\\) is condition of link \\(i\\), \\(S_i(t)\\in\\{0,1\\}\\) \\(P_{xy}=P_r\\{S_1(t)=x,S_2(t)=y\\}\\) At every time, serve \\(Q_1(t)\\) or \\(Q_2(t)\\) \\(\\mu_i(t)=1\\text{ if } S_i(t)=1,Q_i(t)&gt;0\\quad0\\text{ otherwise}\\) Goal: stabilize both queues \\(~\\)","text":"Queue Control Problem \\(A_1(t),A_2(t),Q_1(t),Q_2(t),\\mu_1(t),\\mu_2(t)\\) \\(A_i(t)\\) is iid, \\(\\mathbb{E}\\{A_i(t)\\}=\\lambda_i\\), \\(A_i(t)\\in[0,A_{\\max}]\\) \\(S_i(t)\\) is condition of link \\(i\\), \\(S_i(t)\\in\\{0,1\\}\\) \\(P_{xy}=P_r\\{S_1(t)=x,S_2(t)=y\\}\\) At every time, serve \\(Q_1(t)\\) or \\(Q_2(t)\\) \\(\\mu_i(t)=1\\text{ if } S_i(t)=1,Q_i(t)&gt;0\\quad0\\text{ otherwise}\\) Goal: stabilize both queues \\(~\\) Linear Program \\[\\max~e\\] \\[\\begin{split}s.t.~\\lambda_1+e&amp;\\leq{}P_{10}+P_{11}\\theta\\\\\\lambda_2+e&amp;\\leq{}P_{01}+(1-\\theta)P_{11}\\end{split}\\] optimal value \\(e^\\star&gt;0\\) LP policy is obvious If we know all \\(\\lambda_i\\) and \\(P_{xy}\\) ALG Max-Weight Policy is simple \\(S(t)=\\{1,0\\}\\), serve \\(Q_1\\) \\(S(t)=\\{0,1\\}\\), serve \\(Q_2\\) \\(S(t)=\\{1,1\\}\\), serve \\(\\max\\{Q_1,Q_2\\}\\) Without any infomation \\[L(t)=\\frac{1}{2}Q_1^2(t)+\\frac{1}{2}Q_2^2(t)\\] \\[Q_i(t+1)=(Q_i(t)-\\mu_i(t)+A_i(t))^+\\] \\[\\begin{split}\\Delta(t)&amp;=\\mathbb{E}\\{L(t+1)-L(t)|Q(t)\\}\\\\&amp;=\\frac{1}{2}\\mathbb{E}\\{Q_1^2(t+1)-Q_1^2(t)+Q_2^2(t+1)-Q_2^2(t)|Q(t)\\}\\\\&amp;\\leq{}B-\\mathbb{E}\\{Q_1(t)[\\mu_1(t)-A_1(t)]+Q_2(t)[\\mu_2(t)-A_2(t)]|Q(t)\\}\\\\&amp;=B+\\lambda_1Q_1(t)+\\lambda_2Q_2(t)-\\mathbb{E}\\{Q_1(t)\\mu_1(t)+Q_2(t)\\mu_2(t)|Q(t)\\}\\\\&amp;\\leq{}B+\\lambda_1Q_1(t)+\\lambda_2Q_2(t)-Q_1(t)[P_{10}+P_{11}\\theta]-Q_2(t)[P_{01}+P_{11}(1-\\theta)]\\\\&amp;=B-eQ_1(t)-eQ_2(t)\\end{split}\\] Capacity Region \\(\\lambda=(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)\\) Def: The capacity region \\(\\Lambda\\) is the closure of \\(\\lambda\\) under which exists an algorithm that can stabilize the system Claim: \\(\\Lambda=\\{(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)\\text{ s.t. }e(\\lambda)\\geq0\\}\\) Corollary: Max-weight stabilizes the system whenever \\(\\exists{}e\\geq0\\text{ s.t. } \\lambda+e\\in\\Lambda\\) So, in some way, Max-weight is \"Throughput Optimal\" Caratheodory's Theorem Let \\(X\\) be a subset of \\(R^d\\). If \\(x\\in\\text{conv}(X)\\), then \\(\\exists~d+1\\) points in \\(X\\) (i.e. \\(x_1,\\dots,x_{d+1}\\)), s.t. \\[x=\\sum_{i=1}^{d+1}\\alpha_ix_i,\\alpha_i\\geq0,\\sum_{i=1}^{d+1}\\alpha_i=1\\] Multi-Hop \\(V=\\{1,2,\\dots,7\\}\\) \\(E\\) are linkes \\(A_n^{(k)(t)}\\) is packages entering \\(n\\) for destination \\(k\\), \\(\\mathbb{E}\\{A_n^{(k)}(t)\\}=\\lambda_n^{(k)}\\in[0,A_{\\max}]\\) \\(S(t)=(S_{nm}(t),[n,m]\\in{}E)\\), \\(S_{n,m}\\in\\{0,1\\}\\), iid \\(\\mu_{nm}(t)=1\\text{ if }S_{nm}(t)=1\\text{ and use link }[n,m]\\quad0\\text{ otherwise}\\) No interference Queueing: \\(Q_n^{(k)}(t)\\) \\(Q_n^{(k)}(t+1)\\leq{}(Q_n^{(k)}(t)-\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t))^+{}+\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)+A_n^{(k)}(t)\\) \\(Q_n^{(0)}(t)=0\\) Goal: stabilize all queues Back Pressure At time \\(t\\), observe \\(Q_n^{(k)}(t)\\) for all \\(n,k\\) For each \\([n,m]\\), define weight \\(W_{nm}(t)=\\max_k[Q_n^{(k)}(t)-Q_m^{(k)}(t)]^+\\) and let \\(k^\\star=\\arg\\max{}W_{nm}^{(k)}(t)\\) If \\(W_{nm}(t)&gt;0\\), \\(\\mu_{nm}^{(k)}(t)=\\mu_{nm}(t)\\text{ if }k=k^\\star\\quad0\\text{ otherwise}\\) If \\(W_{nm}(t)=0\\), \\(\\mu_{nm}^{(k)}(t)=0\\) Analysis \\(Q(t)=(Q_n^{(k)}(t)),L(Q(t))=\\frac{1}{2}\\sum_{n,k}Q_n^{(k)}(t)^2\\) \\[\\begin{split}\\Delta(t)&amp;=\\mathbb{E}\\{L(t+1)-L(t)|Q(t)\\}\\\\&amp;\\leq{}B-\\mathbb{E}\\{\\sum_{n,k}Q_n^{(k)}(t)[\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t)-\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)-A_n^{(k)}(t)]|Q(t)\\}\\\\&amp;=B-\\sum_{n,m,k}\\mathbb{E}\\{\\mu_{nm}^{(k)}(t)[Q_n^{(k)}(t)-Q_m^{(k)}(t)]|Q(t)\\}+\\sum_{n,k}Q_n^{(k)}(t)\\lambda_n^{(k)}\\\\&amp;\\leq{}B-e\\sum_{nm}Q_n^{(k)}(t)\\end{split}\\] \\[\\max{}e\\] \\[\\begin{split}s.t.~\\lambda_n^{(k)}+e+\\sum_{[a,n]}f_{an}^{(k)}&amp;\\leq\\sum_{[n,m]}f_{nm}^{(k)}\\\\\\sum_kf_{nm}^{(k)}&amp;\\leq\\mu_{nm}\\\\f_{nm}^{(k)}&amp;\\geq0\\end{split}\\] Interference Problem \\[\\max~\\sum_{n,m,k}\\mu_{nm}^{(k)}(t)[Q_n^{(k)}(t)-Q_m^{(k)}(t)]^+\\] \\[s.t.~\\sum_k\\mu_{nm}^{(k)}(t)\\leq\\mu_{nm}(I(t),S(t)),I(t)\\text{ feasible}\\] Utility Maximization in Networks \\(A_n^{(k)}(t)\\): # of pkts arrivals to \\(n\\) for \\(k\\) at t \\(R_n^{(k)}(t)\\): # of pkts admitted (admission control), \\(0\\leq{}R_n^{(k)}(t)\\leq{}R_{\\max}\\) \\(S(t)=(S_{nm}(t),[n,m]\\in{}E)\\), \\(S_{n,m}\\in\\{0,1\\}\\), iid \\(Q_n^{(k)}(t+1)\\leq{}(Q_n^{(k)}(t)-\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t))^+{}+\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)+R_n^{(k)}(t)\\) \\(\\bar{r}_n^{(k)}=\\lim_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\{R_n^{(k)}(t)\\}\\) \\(g_n^{(k)}(\\cdot)\\) is utility function which is usually concave increasing Goal: \\[\\max\\sum_{n,k}g_n^{(k)}(\\bar{r}_n^{(k)}),\\text{ s.t. stability}\\] Lyapunov function \\[L(Q(t))=\\frac{1}{2}\\sum_{n,k}Q_n^{(k)}(t)^2\\] \\[\\begin{split}\\Delta(t)&amp;\\leq{}B-\\mathbb{E}\\{\\sum_{n,k}Q_n^{(k)}(t)[\\sum_{[n,m]\\in{}E}\\mu_{nm}^{(k)}(t)-\\sum_{[a,n]\\in{}E}\\mu_{an}^{(k)}(t)-R_n^{(k)}(t)]|Q(t)\\}\\\\\\text{subtract from both sides the term}&amp; \\\\ &amp;V\\sum_{n,k}\\mathbb{E}\\{g_n^{(k)}(R_n^{(k)}(t))|Q_n(t)\\}\\\\\\Delta(t)-V\\sum_{n,k}\\mathbb{E}\\{g_n^{(k)}(R_n^{(k)}(t))|Q_n(t)\\}&amp; \\leq{}B-\\sum_{n,k}\\mathbb{E}\\{Vg_n^{(k)}(R_n^{(k)}(t))-Q_n^{(k)}(t)R_n^{(k)}(t)|Q(t)\\}\\\\&amp;-\\sum_{n,m,k}\\mathbb{E}\\{\\mu_{nm}^{(k)}(t)[Q_n^{(k)}(t)-Q_m^{(k)}(t)]|Q(t)\\}\\end{split}\\] Cross-Longer Control At time \\(t\\), observe \\(S(t)\\) and \\(Q(t)\\) Admission control (Application): Choose \\(R_n^{(k)}(t)\\) \\[\\max{}Vg_n^{(k)}(R_n^{(k)}(t))-Q_n^{(k)}(t)R_n^{(k)}(t),\\text{ s.t. }0\\leq{}R_n^{(k)}(t)\\leq{}R_{\\max}\\] Scheduling and Routing (Transport): Define \\(W_{nm}(t)=\\max_k[Q_n^{(k)}(t)-Q_m^{(k)}(t)]^+\\), choose \\[\\mu_{nm}^{(k)}(t)=\\mu_{nm}(t)\\text{ if }k=k^\\star\\quad0\\text{ otherwise}\\] Resource Allocation (Physical): Choose \\(\\mu(t)\\) \\[\\max\\sum_{n,m}W_{nm}(t)\\mu_{nm}(t),\\text{ s.t. }\\mu(t)\\text{ is feasible}\\] Queue Independent Policy \\[\\begin{split}\\max&amp;\\sum_{n,k}g_n^{(k)}(r_n^{(k)})\\\\\\text{s.t. }r_n^{(k)}+\\sum_{a,n}f_{an}^{(k)}&amp;\\leq{}f_{nm}^{(k)}\\\\\\sum_kf_{nm}^{(k)}&amp;\\leq\\mu_{nm}\\\\\\mu=(\\mu_{nm},[n,m]\\in{}E)&amp;\\in\\sum_{S}\\beta_S\\text{conv}(\\Gamma_S)\\end{split}\\] \\(r_n^{(k)\\star}\\), \\(f_{nm}^{(k)\\star}\\), \\(\\mu_{nm}^{(k)\\star}\\) refers to a stationary randomized policy \\[\\Rightarrow{}\\Delta-V\\sum_{n,k}\\mathbb{E}\\{g_n^{(k)}(R_n^{(k)}(t))|Q_n(t)\\}\\leq{}B-V\\cdot\\text{opt}\\] Sum over \\(t=0,\\cdots,T-1\\), take expectation \\[\\begin{split}\\mathbb{E}\\{L(t)\\}-\\mathbb{E}\\{L(0)\\}-\\sum_tV\\sum_{n,k}\\mathbb{E}\\{g_n^{(k)}(R_n^{(k)}(t))|Q(t)\\}&amp;\\leq{}BT-VT\\cdot{}\\text{opt}\\\\\\sum_tV\\sum_{n,k}\\mathbb{E}\\{g_n^{(k)}(R_n^{(k)}(t))|Q(t)\\}&amp;\\geq{}-BT+VT\\cdot{}\\text{opt}-\\mathbb{E}\\{L(0)\\}\\end{split}\\] If consider \\(g_n^{(k)}(\\cdot)\\) as concave increasing, we can use Jensen Inequality to put the expectation sign inside. Utility Performance \\[\\sum_{n,k}g_n^{(k)}(r_n^{(k)}(t))\\geq{}\\text{opt}-\\frac{B}{V}\\] Delay Bound Assume \\(r_n^{(k)\\star}\\geq{}e&gt;0\\) \\[r_n^{(k)&#39;}=r_n^{(k)\\star}-e,~f_{nm}^{(k)&#39;}=f_{nm}^{(k)\\star}\\] \\[\\begin{split}\\Delta(t)-V\\mathbb{E}\\{\\sum_{n,k}g_n^{(k)}(R_n^{(k)}(t))|Q(t)\\}&amp;\\leq{}B-e\\sum_{n,k}Q_n^{(k)}(t)\\\\\\Delta(t)&amp;\\leq{}B+Vg_{\\max}-e\\sum_{n,k}Q_n^{(k)}(t)\\\\\\bar{Q}(t)&amp;\\leq\\frac{B+Vg_{\\max}}{e}\\sim\\mathcal{O}(V)\\end{split}\\]"},{"title":"Introduction to Queue and Lyapunov Analysis","slug":"Introduction to Queue and Lyapunov Analysis","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/09/09/Introduction to Queue and Lyapunov Analysis/","link":"","permalink":"https://zs-liu.github.io/2019/09/09/Introduction%20to%20Queue%20and%20Lyapunov%20Analysis/","excerpt":"Queue Definition Arrival: \\(A(t)\\) Cumulative arrival: \\[X[t_1,t_2]=\\int_{t_1}^{t_2}A(t)dt\\] Service: \\(\\mu(t)\\) Cumulative departure: \\[Y[t_1,t_2]=\\int_{t_1}^{t_2}Y(t)dt\\leq\\int_{t_1}^{t_2}\\mu(t)dt\\] \\(Y(t)=Q(t)~if~Q(t)&gt;0\\quad0~otherwise\\) \\(Q(t)=X[0,t]-Y[0,t],Q(0)=0\\)","text":"Queue Definition Arrival: \\(A(t)\\) Cumulative arrival: \\[X[t_1,t_2]=\\int_{t_1}^{t_2}A(t)dt\\] Service: \\(\\mu(t)\\) Cumulative departure: \\[Y[t_1,t_2]=\\int_{t_1}^{t_2}Y(t)dt\\leq\\int_{t_1}^{t_2}\\mu(t)dt\\] \\(Y(t)=Q(t)~if~Q(t)&gt;0\\quad0~otherwise\\) \\(Q(t)=X[0,t]-Y[0,t],Q(0)=0\\) Interval and Conserving Def: An interval \\(I=[t_1,t_2]\\) is a busy period if \\(Y(t)&gt;0,\\forall{}t\\in{}I\\) and \\(Y(t_1^-)=Y(t_2^+)=0\\) Def: A work conserving single server system is one where \\(Y(t)=\\mu(t)\\) wherever \\(Q(t)&gt;0\\) If we start from an empty system, and \\(Q(t)&gt;0\\), then \\[\\exists{}t^* \\text{ with } Q(t^*)=0 \\text{ s.t. } Q(t)=X[t^*,t]-\\int_{t^*}^t\\mu(t)dt\\] Stability Def: \\(X[0,t]\\) has a rate \\(\\lambda\\) if \\(\\lim_{t\\rightarrow\\infty}\\frac{X[0,t]}{t}=\\lambda\\) w.p.1 Def: \\(\\mu(t)\\) has rate \\(\\mu\\) if \\(\\lim_{t\\rightarrow\\infty}\\frac{\\int_0^t\\mu(t)}{t}=\\mu\\) w.p.1 Def: \\(Q(t)\\) is rate stable if \\(\\lim_{t\\rightarrow\\infty}\\frac{Q(t)}{t}=0\\) w.p.1 Def: \\(Q(t)\\) is mean rate stable if \\(\\lim_{t\\rightarrow\\infty}\\frac{E[Q(t)]}{t}=0\\) Theorem-Rate Stability Suppose \\(Q(t)=X[0,t]-Y[0,t]\\), and \\(X[0,t]\\) has rate \\(\\lambda\\) and \\(\\mu(t)\\) has rate \\(\\mu\\), then \\(Q(t)\\) is rate stable if and only if \\(\\lambda\\leq\\mu\\) \\[Q(t)\\text{ stable } \\rightarrow\\lambda\\leq\\mu\\] \\[\\frac{Q(t)}{t}=\\frac{X[0,t]}{t}-\\frac{Y[0,t]}{t}\\geq\\frac{X[0,t]}{t}-\\frac{\\int_0^t\\mu(t)}{t}\\] \\[Q(t)\\text{ stable } \\leftarrow\\lambda\\leq\\mu\\] Little's law \\[Q_{av}=\\lim_{t\\rightarrow\\infty}\\frac{1}{t}\\int_{\\tau=0}^tQ(\\tau)d\\tau,D_{av}=\\lim_{k\\rightarrow\\infty}\\frac{1}{k}\\sum_{k=1}^kD_k\\] \\[\\Rightarrow{}Q_{av}=\\lambda{}D_{av}\\] Lyapunov Analysis Definition Denote \\(\\vec{Q}(t)=(Q_1(t),\\dots,Q_k(t))\\), Def \\(L(\\vec{Q}(t))\\) is a Lyapunov-function if \\(L(\\vec{Q}(t))\\geq0\\) \\(L(0)=0\\) Define one-slot conditional Lyapunov drift \\[\\Delta(t)=E\\{L(\\vec{Q}(t+1))-L(\\vec{Q}(t))|\\vec{Q}(t)\\}\\] Theorem Suppose \\(L(\\vec{Q}(t))\\) is a Lyapunov function and satisifes \\(\\Delta(t)\\leq{}B-e\\sum_kQ_k(t)\\) and \\(L(\\vec{Q}(0))&lt;\\infty\\), then \\(e&gt;0\\rightarrow{}Q(t)\\) is strongly stable \\[\\lim_{T\\rightarrow\\infty}\\sup\\frac{1}{T}\\sum_{t=0}^{T-1}\\sum_kE\\{Q_k(t)\\}\\leq\\frac{B}{e}\\] \\(e\\geq0\\) and \\(L(\\vec{Q})=\\sum_k{}w_kQ_k\\), then \\[\\lim_{t\\rightarrow\\infty}\\frac{E\\{Q_k(t)\\}}{t}=0,\\forall{}k\\] Proof is made by list the inequality from \\(t=0\\) to \\(t=T\\) and sum them up. For the first one, divide by \\(T\\) then is obvious. For the second one, put exception into square, the left side is \\(O(\\sqrt{T})\\). Example \\(A(t), \\mu(t), \\sim{}Bernoulli~\\lambda,\\mu\\) \\[Q(t+1)=(Q(t)-\\mu(t)+A(t))^+,L(Q(t))=\\frac{1}{2}Q^2(t)\\] \\[\\Delta(t)=\\frac{1}{2}E\\{Q^2(t+1)-Q^2(t)|Q(t)\\}\\leq\\cdots=E\\{\\frac{1}{2}(\\mu(t)-A(t))^2-Q(t)(\\mu(t)-A(t))|Q(t)\\}\\leq\\frac{1}{2}-Q(t)(\\mu-\\lambda)\\] \\[\\bar{Q}(t)\\leq\\frac{1}{2(\\mu-\\lambda)}\\]"},{"title":"Optimal Transport","slug":"Optimal Transport","date":"un66fin66","updated":"un00fin00","comments":true,"path":"2019/05/25/Optimal Transport/","link":"","permalink":"https://zs-liu.github.io/2019/05/25/Optimal%20Transport/","excerpt":"Wasserstein Distance Definition Consider, general functions \\(f\\) and \\(g\\), the Wasserstein distance is \\[\\min_{\\text{all map }T}\\{\\sum_{\\text{all movements of }T}\\text{distance moved}\\times\\text{amount moved}\\}\\] For \\(f:X\\rightarrow{}R^+,g:Y\\rightarrow{}R^+\\), the distance can be formulated as \\[W_p(f,g)=\\left(\\inf_{T\\in\\mathcal{M}}\\int{}|x-T(x)|^pf(x)dx\\right)^{1/p}\\] where \\(\\mathcal{M}\\) is the set of all maps that rearrange the distribution \\(f\\) into \\(g\\).","text":"Wasserstein Distance Definition Consider, general functions \\(f\\) and \\(g\\), the Wasserstein distance is \\[\\min_{\\text{all map }T}\\{\\sum_{\\text{all movements of }T}\\text{distance moved}\\times\\text{amount moved}\\}\\] For \\(f:X\\rightarrow{}R^+,g:Y\\rightarrow{}R^+\\), the distance can be formulated as \\[W_p(f,g)=\\left(\\inf_{T\\in\\mathcal{M}}\\int{}|x-T(x)|^pf(x)dx\\right)^{1/p}\\] where \\(\\mathcal{M}\\) is the set of all maps that rearrange the distribution \\(f\\) into \\(g\\). Quadratic Wasserstein distance: \\(p=2\\) \\[W_2^2(f,g)=\\inf_{T\\in\\mathcal{M}}\\int{}|x-T(x)|^pf(x)dx\\] Kantorovich Problem Definition \\[\\inf_\\gamma\\left\\{\\int_{X\\times{}Y}c(x,y)d\\gamma|\\gamma\\geq0,\\gamma\\in\\Pi(\\mu,\\nu)\\right\\}\\] where \\(\\Pi(\\mu,\\nu)=\\{\\gamma\\in\\mathcal{P}(X\\times{}Y)|(P_X)\\#\\gamma=\\mu,(P_Y)\\#\\gamma=\\nu\\}\\), \\(P_X\\) and \\(P_Y\\) are two projections Kantorovich Dual Problem Consider \\(\\varphi\\in{}L^1(\\mu)\\) and \\(\\psi\\in{}L^1(\\nu)\\), the Kantorovich dual problem is formulated as the following: \\[\\sup_{\\varphi,\\psi}\\left(\\int_X\\varphi{}d\\mu+\\int_Y\\psi{}d\\nu\\right)\\] subject to \\(\\varphi(x)+\\psi(y)\\leq{}c(x,y)\\), for any \\((x,y)\\in{}X\\times{}Y\\). Note that this dual formulation is a linear optimization problem which is solvable by linear programming. Dynamic Formulation The Benamou-Brenier formula identifies the squared quadratic Wasserstein metric between \\(\\mu\\) and \\(\\nu\\) by: \\[W_2^2(\\mu,\\nu)=\\inf\\int_0^1\\int|v(t,x)|^2\\rho(t,x)dxdt\\] where infimum is taken among all the Borel fields \\(v(t,x)\\) that transports \\(\\mu\\) to \\(\\nu\\) continuously in time, satisfying the zero flux condition on the boundary: \\[\\begin{split}\\frac{\\partial\\rho}{\\partial{}t}+\\nabla(v\\rho)&amp;=0\\\\\\text{subject to }\\rho(0,x)=d\\mu,\\rho(1,x)&amp;=d\\nu\\end{split}\\] Monge-Ampere Equation \\[\\left\\{\\begin{split}&amp;det(D^2u(x))=f(x)/g(\\nabla{}u(x))\\\\&amp;\\nabla{}u:X\\rightarrow{}Y\\\\&amp;u\\text{ is convex}\\end{split}\\right.\\] The optimal map is \\(\\nabla{}u\\). Thus, the square of the quadratic Wasserstein distance has the form: \\[W_2^2(f,g)=\\int{}|x-\\nabla{}u(x)|^2f(x)dx\\]"},{"title":"Proximal Mapping","slug":"Proximal Mapping","date":"un44fin44","updated":"un00fin00","comments":true,"path":"2019/04/25/Proximal Mapping/","link":"","permalink":"https://zs-liu.github.io/2019/04/25/Proximal%20Mapping/","excerpt":"Closed Closed set A set \\(\\mathcal{C}\\) is closed if it contains its boundary: \\[x^k\\in\\mathcal{C},x^k\\rightarrow\\bar{x}\\Rightarrow\\bar{x}\\in\\mathcal{C}\\] the intersection of closed sets is closed the union of a finite number of closed sets is closed inverse under linear mapping \\[\\{x|Ax\\in\\mathcal{C}\\}\\] is closed if \\(\\mathcal{C}\\) is closed","text":"Closed Closed set A set \\(\\mathcal{C}\\) is closed if it contains its boundary: \\[x^k\\in\\mathcal{C},x^k\\rightarrow\\bar{x}\\Rightarrow\\bar{x}\\in\\mathcal{C}\\] the intersection of closed sets is closed the union of a finite number of closed sets is closed inverse under linear mapping \\[\\{x|Ax\\in\\mathcal{C}\\}\\] is closed if \\(\\mathcal{C}\\) is closed Closed function a function is closed if its epigraph is a closed set or if all its sublevel set is a closed set If \\(f\\) is continuous and \\(dom~f\\) is closed, then \\(f\\) is closed If \\(f\\) is continuous and \\(dom~f\\) is open, then \\(f\\) is closed iff it converges to \\(\\infty\\) along every sequence converging to a boundary point of \\(dom~f\\) Properties sublevel sets: \\(f\\) is closed iff all its subsevel sets are closed minimum: if \\(f\\) is closed with bounded sublevel sets then it has a minimizer Conjugate function Conjugate functions: recall the conjugate of a function \\(f\\) is always closed and convex The indicator function of convex set \\(\\mathcal{C}\\): conjugate is support function of \\(\\mathcal{C}\\) Norm: conjugate is indicator of unit dual norm ball Second conjugate \\[f^{**}(x)=\\sup_{y\\in{}dom~f^*}(x^\\top{}y-f^*(y)))\\] \\(f^{**}\\) is closed and convex \\(f^{**}\\leq{}f(x)\\) if \\(f\\) is closed and convex, then \\(f^{**}\\leq{}f(x)\\) Calculus rules Separable sum \\[f(x_1,x_2)=g(x_1)+h(x_2),f^*(y_1,y_2)=g^*(y_1)+h^*(y_2)\\] Scalar multiplication \\[\\alpha{}&gt;0,f(x)=\\alpha{}g(x),f^*(y)=\\alpha{}g^*(y/\\alpha{})\\] Addition to affine function \\[f(x)=g(x)+a^\\top+b,f^*(y)=g^*(y-a)-b\\] Infimal convolution \\[f(x)=\\inf_{u+v=x}(g(u)+h(v)),f^*(y)=g^*(y)+h^*(y)\\] Proximal mapping Definition: the proximal mapping of a closed convex function \\(f\\) is: \\[\\text{prox}_f(x)=\\arg\\min_u\\left(f(u)+\\frac{1}{2}||u-x||_2^2\\right)\\] Example \\[u=\\text{prox}_f(x)\\Leftrightarrow{}0\\in\\partial{}f(u)+u-x\\Leftrightarrow{}x-u\\in\\partial{}f(u)\\Leftrightarrow{}f(z)\\geq{}f(u)+(x-u)^\\top(z-u)\\] If \\(f(x)=\\delta_C(x)\\), \\(C\\) is closed and convex \\(\\Rightarrow{}(x-u)^\\top(z-u)\\leq0\\) Calculus rules \\[f(x)=\\lambda{}g(x/\\lambda)\\Rightarrow{}\\text{prox}_f(x)=\\lambda\\text{prox}_{\\lambda^{-1}g}(x/\\lambda)\\] Proof \\[\\text{prox}_f(x)=\\arg\\min_y\\{f(y)+\\frac{1}{2}||y-x||^2\\}=\\arg\\min_{y,z}\\{\\lambda{}g(z)+\\frac{1}{2}||\\lambda{}z-x||_2^2\\left|z=\\frac{y}{\\lambda}\\}\\right.\\] \\[L(y,z;\\mu)=\\lambda{}g(z)+\\frac{1}{2}||\\lambda{}z-x||_2^2+&lt;\\lambda{}z-y,\\mu&gt;\\] \\[\\partial_yL=0,\\partial_zL=0\\Rightarrow{}z=\\text{prox}_{\\lambda^{-1}g}(x/\\lambda)\\] Moreau decomposition \\[x=\\text{prox}_f(x)+\\text{prox}_{f^*}(x)\\quad{}\\text{for all }x\\] Composition with affine mapping for general \\(A\\), prox-operator of \\(f\\) does not follow easily from prox-operator of \\(g\\) however, if \\(AA^\\top=(1/\\alpha)I\\) \\[f(x)=g(Ax+b)\\Rightarrow{}\\text{prox}_f(x)=x-\\alpha{}A^\\top(Ax+b-\\text{prob}_{\\alpha^{-1}g}(Ax+b))\\] Projections Affine sets \\(C=\\{x|Ax=b\\}\\), with \\(A\\in{}R^{p\\times{}n}\\) and \\(\\textbf{rank}(A)=p\\) \\[P_C(x)=x+A^\\top(AA^\\top)^{-1}(b-Ax)\\] inexpensive if \\(p\\ll{}n\\), or \\(AA^\\top=I\\) Simple polyhedral sets Halfspace \\(C=\\{x|a^\\top{}x\\leq{}b\\}\\) \\[P_C(x)=x+\\frac{b-a^\\top{}x}{||a||_2^2}a\\quad\\text{if }a^\\top{}x&gt;b,\\quad{}P_C(x)=x\\quad\\text{if }a^\\top{}x\\leq{}b\\] Probability simplex \\(C=\\{x|\\textbf{1}^\\top{}x=1,x\\succeq0\\}\\) \\[P_C(x)=(x-\\lambda\\textbf{1})_{+}\\] where \\(\\lambda\\) is the solution of the equation \\[\\textbf{1}^\\top(x-\\lambda\\textbf{1})_{+}=\\sum_{i=1}^n\\max\\{0,x_k-\\lambda\\}=1\\] Support function, Norm and Distance Support function conjugate of support function of closed convex set is indicator function \\[f(x)=\\sup_{y\\in{}C}x^\\top{}y,f^*(y)=\\delta_C(y)\\] prox-operator of support function follows from Moreau decomposition \\[\\text{prox}_{tf}(x)=x-t\\text{prox}_{t^{-1}f^*}(x/t)=x-tP_C(x/t)\\] Norm conjugate of norm is indicator function of dual norm \\[f(x)=||x||,f^*(y)=\\delta_B(y),B=\\{y|~||y||_*\\leq1\\}\\] prox-operator of norm follows from Moreau decomposition \\[\\text{prox}_{tf}(x)=-P_{tB}(x)\\] Euclidean distance to a set \\[d(x)=\\inf_{y\\in{}C}||x-y||_2\\] \\[\\text{prox}_{td}(x)=\\left\\{\\begin{aligned}&amp;x+\\frac{t}{d(x)}(P_C(x)-x)&amp;d(x)\\geq{}t\\\\&amp;P_C(x)&amp;~\\text{otherwise}\\end{aligned}\\right.\\]"},{"title":"Subgradient Method","slug":"Subgradient Method","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2019/04/16/Subgradient Method/","link":"","permalink":"https://zs-liu.github.io/2019/04/16/Subgradient%20Method/","excerpt":"Subgradient \\(g\\) is a subgradient of a convex function \\(f\\) at \\(x\\in{}dom~f\\) if \\[f(y)\\geq{}f(x)+g^\\top(y-x)\\quad\\text{for all }y\\in{}dom~f\\]","text":"Subgradient \\(g\\) is a subgradient of a convex function \\(f\\) at \\(x\\in{}dom~f\\) if \\[f(y)\\geq{}f(x)+g^\\top(y-x)\\quad\\text{for all }y\\in{}dom~f\\] Properties \\(f(x)+g^\\top(y-x)\\) is a global lower bound on \\(f(y)\\) \\(g\\) defines non-vertical supporting hyperplane to \\(epi~f\\) at \\((x, f (x))\\) if \\(f\\) is convex and differentiable, then \\(\\nabla{}f(x)\\) is a subgradient of \\(g\\) at \\(x\\) Subdifferential the subdifferential \\(\\partial{}f(x)\\) of \\(f\\) at \\(x\\) is the set of all subgradients: \\[\\partial{}f(x)=\\{g|g^\\top(y-x)\\leq{}f(y)-f(x),\\forall{}y\\in{}dom~f\\}\\] Monotonicity the subdifferential of a convex function is a monotone operator: \\[(u-v)^\\top(x-y)\\geq0,\\text{for all }x,y,u\\in\\partial{}f(x),v\\in\\partial{}f(y)\\] Directional Derivative Introduction \\[f(x+d)=f(x)+\\nabla{}f(x)^\\top{}d\\] \\[\\text{If }\\nabla{}f(x)^\\top{}d&lt;0,d\\text{ is a descent direction}\\] Definition The directional derivative of \\(f\\) at \\(x\\) in the direction \\(y\\) is \\[f^{&#39;}(x;y)=\\lim_{a\\searrow{}0}\\frac{f(x+\\alpha{}y)-f(x)}{\\alpha}=\\lim_{t\\rightarrow\\infty}(tf(x+\\frac{y}{t})-tf(x))\\] \\(f^{&#39;}(x;y)\\) is the right derivative of \\(g(\\alpha)=f(x+\\alpha{}y)\\) at \\(\\alpha=0\\) Proof:Existence \\[h(\\alpha)=\\frac{f(x+\\alpha{}y)-f(x)}{\\alpha}\\] \\[x+\\alpha_1{}y=x+\\frac{\\alpha_1}{\\alpha_2}x-\\frac{\\alpha_1}{\\alpha_2}x+\\frac{\\alpha_1}{\\alpha_2}\\alpha_2y=(1-\\frac{\\alpha_1}{\\alpha_2})x+\\frac{\\alpha_1}{\\alpha_2}(x+\\alpha_2y)\\] \\[f(x+\\alpha_1{}y)\\leq(1-\\frac{\\alpha_1}{\\alpha_2})f(x)+\\frac{\\alpha_1}{\\alpha_2}f(x+\\alpha_2y)\\text{ (convex)}\\] \\[h(\\alpha_1)\\leq{}h(\\alpha_2)\\] Proof:Homogeneous \\[\\text{If }x\\in\\text{int dom}f\\Rightarrow{}\\text{dom}f^{&#39;}(x,y)=R^n\\] \\[\\text{Let }\\beta=\\alpha\\lambda,\\text{ it is easy to prove}\\] Directional derivative and subgradients For convex \\(f\\) and \\(x\\in\\text{int dom}f\\): \\[f^{&#39;}(x;y)=\\sup_{g\\in\\partial{}f(x)}g^\\top{}y\\] generalize \\(f^{&#39;}(x;y)\\) for differentiable functions implies that \\(f^{&#39;}(x;y)\\) exists for all \\(x\\in\\text{int dom}f\\), all \\(y\\) Proof \\[f^{&#39;}(x;y)=\\lim_{a\\searrow{}0}\\frac{f(x+\\alpha{}y)-f(x)}{\\alpha}\\geq\\lim_{a\\searrow{}0}\\frac{}{}\\] \\[\\text{Let }\\hat{g}\\in\\partial_yf^{&#39;}(x;y)\\] \\[\\lambda{}f^{&#39;}(x;v)=f^{&#39;}(x,\\lambda{}v)\\geq{}f^{&#39;}(x;y)+\\hat{g}^\\top(\\lambda{}v-y)\\]\\[\\Leftrightarrow{}\\lambda(f^{&#39;}(x,v)-\\hat{g}^\\top{}v)\\geq{}f^{&#39;}(x;y)-\\hat{g}^\\top{}y\\] \\[\\Leftrightarrow{}^{\\lambda\\rightarrow\\infty}f^{&#39;}(x,v)\\geq\\hat{h}^\\top{}v\\] \\[\\Leftrightarrow{}f(x,v)\\geq{}f(x)+f^{&#39;}(x,v)\\geq{}f(x)+\\hat{g}^\\top{}v\\Rightarrow{}\\hat{g}\\in\\partial{}f(x)\\] \\[\\Leftrightarrow{}^{\\lambda\\rightarrow0}f^{&#39;}(x,y)\\leq{}g^\\top{}y\\] Steepest descent direction Steepest descent direction at \\(x\\in\\text{int dom} f\\) is \\[\\Delta{}x_{nsd}=\\arg\\min_{||y||_2\\leq1}f^{&#39;}(x;y)\\] \\[\\text{(P) }\\min_yf^{&#39;}(x;y),s.t.||y||_2\\leq1\\] \\[\\text{(D) }\\max_g-||g||_2,s.t.g\\in\\partial{}f(x)\\] Proof if \\(f\\) is convex, \\(f(y)&lt;f(x),g\\in\\partial{}f(x)\\), then for small \\(t&gt;0\\): \\[||x-tg-y||_2^2=||x-y||_2^2-2tg^\\top(x-y)+t^2||g||_2^2\\leq||x-y||_2^2-2t(f(x)-f(y))+t^2||g||_2^2&lt;||x-y||_2^2\\] \\[f^\\star(x^\\star)=\\min{}f(x),x_{k+1}=x_k-tg_k,g_k\\in\\partial{}f(x_k)\\] \\[f(x_k)&gt;f(x^\\star),\\text{but }||x_{k+1}-x^\\star||&lt;||x_k-x^\\star||\\] Lemma 1 Let \\(x_0\\in{}int~dom~f\\), then \\[f \\text{ is convex }\\Rightarrow{}f\\text{ is continuous at }x_0\\] Proof \\[g(x)=f(x_0+x)-f(x_0), g(0)=0, g\\text{ is convex}, 0\\in~int~dom~g\\] \\[\\exists\\alpha~s.t.~x_0+\\alpha{}y_i\\in~dom~f\\] \\[\\{y_1,\\dots,y_{2n}\\}=\\{e_1,\\dots,e_n,-e_1,\\dots,-e_n\\}\\] ...... Subgradient Method to minimize a nondifferentiable convex function \\(f\\): choose \\(x^{(0)}\\) and repeat \\[x^{(k)}=x^{(k-1)}-t_kg^{(k-1)},k=1,2,\\dots\\] \\(g^{(k-1)}\\) is any subgradient of \\(f\\) at \\(x^{(k−1)}\\) Assumptions \\(f\\) has finite optimal value \\(f^\\star\\), minimizer \\(x^\\star\\) \\(f\\) is convex, \\(dom~f = R^n\\) \\(f\\) is Lipschitz continuous with constant \\(G &gt; 0\\) \\[|f(x)-f(y)|\\leq{}G||x-y||_2\\quad\\forall{}x,y\\] this is equivalent to \\(||g||_2\\leq{}G\\) for all \\(x\\) and \\(g\\in\\partial{}f(x)\\) Analysis the subgradient method is not a descent method the key quantity in the analysis is the distance to the optimal set with \\(x^+=x^{(i)},x=x^{(i-1)},g=g^{(i-1)},t=t_i\\): \\[||x^+-x^\\star||_2^2=||x-tg-x^\\star||_2^2\\] \\[=||x-x^\\star||_2^2-2tg^\\top(x-x^\\star)+t^2||g_2^2||\\leq||x-x^\\star||_2^2-2t(f(x)-f^\\star)+t^2||g_2^2||\\] combine inequalities for \\(i=1,\\dots,k\\), and define \\(f^{(k)}_{\\text{best}}=\\min_{0\\leq{}i\\leq{}k}f(x^{(i)})\\) \\[(f^{(k)}_{\\text{best}}-f^\\star)\\leq\\frac{||x^{(0)}-x^\\star||_2^2+\\sum_{i=1}^kt_i^2||g^{(i-1)}||_2^2}{2\\sum_{i=1}^kt_i}\\] Fixed step size: \\(t_i=t\\) \\[f^{(k)}_{\\text{best}}-f^\\star\\leq\\frac{||x^{(0)}-x^\\star||_2^2}{2kt}+\\frac{G^2t}{2}\\] does not guarantee convergence of \\(f^{(k)}_{\\text{best}}\\) Fixed step length: \\(t_i=s/||g^{(i-1)}||_2\\) \\[f^{(k)}_{\\text{best}}-f^\\star\\leq\\frac{G||x^{(0)}-x^\\star||_2^2}{2ks}+\\frac{Gs}{2}\\] does not guarantee convergence of \\(f^{(k)}_{\\text{best}}\\) Diminishing step size: \\(t_i\\rightarrow0\\) \\(f^{(k)}_{\\text{best}}\\) converges to \\(f^\\star\\) currently we don't know the speed of convergence large \\(t_i\\) may converge fast at first, but then smaller \\(t_i\\) should be used Optimal step size when \\(f^\\star\\) is known \\[t_i=\\frac{f(x^{(i-1)})-f^\\star}{||g^{(i-1)}||_2^2}\\] applying recursively (with \\(||x^{(0)}-x^\\star||_2\\leq{}R\\) and \\(||g^{(i)}||_2\\leq{}G\\)) gives \\[f^{(k)}_{\\text{best}}-f^\\star\\leq\\frac{GR}{\\sqrt{k}}\\]"},{"title":"故障模式影响及危害分析","slug":"故障模式影响及危害分析","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/04/15/故障模式影响及危害分析/","link":"","permalink":"https://zs-liu.github.io/2019/04/15/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%BC%8F%E5%BD%B1%E5%93%8D%E5%8F%8A%E5%8D%B1%E5%AE%B3%E5%88%86%E6%9E%90/","excerpt":"概述 定义 Failure Mode, Effects and Criticality analysis 归纳分析方法：分析系统中每个设备所有可能的故障模式及对 系统造成的所有可能影响，并按每个故障模式的严重程度及发 生概率予以分类。 一种自下而上的归纳分析方法 FMEA和CA","text":"概述 定义 Failure Mode, Effects and Criticality analysis 归纳分析方法：分析系统中每个设备所有可能的故障模式及对 系统造成的所有可能影响，并按每个故障模式的严重程度及发 生概率予以分类。 一种自下而上的归纳分析方法 FMEA和CA 目的 在设计、生产和使用阶段发现影响系统可靠性的各薄弱环节 对薄弱环节提出改进措施，提高系统可靠性水平 步骤 系统定义 确定系统中进行FMECA的设备范围 系统功能任务的描述 确定系统及设备的故障判据 FMEA 代码：采用统一的编码体系对每个设备的每个故障模式进行编码 设备或功能标识：记录被分析设备或功能的名称标识 功能：简要描述设备的主要功能 故障模式：故障的表现形式，每一个产品有可能具有多种故障模式 损坏型、退化型、松脱型、失调型、堵塞或渗漏型、功能型等 故障原因 直接原因：导致设备功能故障的设备自身的那些物理、化学或生物变化过程等 间接原因：其他设备故障、环境因素和人为因素等外部原因 任务阶段与工作方式 任务可能包含多个阶段，设备可能需要参与一个或多个阶段 工作方式：可替换、有冗余等 故障影响 局部影响：某设备的故障模式对自身和与其所在约定层次相同的其他设备的使用、功能或状态的影响 高一层次影响：某设备的故障模式对其所在约定层次的高一层次设备的使用、功能或状态的影响。 最终影响：指系统中某设备的故障模式对初始约定层次设备的使用、功能或状态的影响。 严酷度等级 严酷度：设备故障模式所产生后果的严重程度 严酷度等级：应考虑故障造成的最坏潜在后果，根据最终可能出现的人员伤亡、系统损坏或经济损失的程度等来确定。 故障检测方法 包括事前检测与事后检测 目视检查、离机检测、原位测试等手段 补偿措施 设计补偿措施 设备发生故障时，能继续工作的冗余设备 安全或保险装置(如监控及报警装置) 可替换的工作方式(如备用或辅助设备) 可以消除或减轻故障影响的设计或工艺改进 操作人员补偿措施 特殊的使用和维护规程，尽量避免或预防故障的发生 一旦出现某故障后操作人员应采取的最恰当的补救措施 CA 危害性分析CA：按每个故障模式的严重程度及其发生概率所产生的综合影响对系统中的设备分级 分析方法 风险优先数法 危害性矩阵法 风险优先数法 \\[RPN=OPR\\times{}ESR\\] \\(OPR\\)：发生概率等级 \\(ESR\\)：影响严酷度等级 危害性矩阵法 故障模式频数比 \\(\\alpha\\)：设备某故障模式占其全部故障模式的百分比率 故障影响概率 \\(\\beta\\)：某故障模式发生后，导致确定的严酷度等级的最终影响的条件概率 故障模式危害度：评价设备单一故障模式危害性 \\[C_m(j)=\\alpha\\beta\\lambda{}pt\\] 设备危害度：评价设备的危害性 \\[C_r(j)=\\sum{}C_{mi}(j)\\] 结果呈现 可靠性关键设备清单 RPN 值大于某规定值的设备 危害性矩阵中落在某一规定区域之内的设备 严重故障模式清单 严酷度为 Ⅰ、Ⅱ 类的故障模式 故障影响严重程度被评为 9-10 分的故障模式 单点故障模式清单 某一设备或故障模式发生后将直接导致系统故障 小结 FMECA 中各故障模式的相关数据是定量化分析的基础 FMECA 是静态的、单一因素的分析方法，在动态方面还很不完善"},{"title":"LP, SOP and SOCP","slug":"LP, SDP and SOCP","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2019/04/02/LP, SDP and SOCP/","link":"","permalink":"https://zs-liu.github.io/2019/04/02/LP,%20SDP%20and%20SOCP/","excerpt":"Linear Programming \\[\\text{(P) }\\min_xc^Tx,s.t.Ax=b,x\\geq0\\] \\[\\text{(D) }\\min_yb^Ty,s.t.A^Ty+s=c,s\\geq0\\] Strong duality \\[c^Tx=b^T\\Leftrightarrow{}x^Ts=0\\Leftrightarrow{}x_is_i=0\\]","text":"Linear Programming \\[\\text{(P) }\\min_xc^Tx,s.t.Ax=b,x\\geq0\\] \\[\\text{(D) }\\min_yb^Ty,s.t.A^Ty+s=c,s\\geq0\\] Strong duality \\[c^Tx=b^T\\Leftrightarrow{}x^Ts=0\\Leftrightarrow{}x_is_i=0\\] KKT system in LP Primal feasibility \\[Ax=b\\land{}x\\geq0\\] Dual feasibility \\[A^Ty+s=c\\land{}s\\geq0\\] Complementarity \\[x_is_i=0\\] Algebraic Characterization Define \\(x\\circ{}s=(x_1s_1,\\dots,x_ns_n)^\\top\\) and \\[L_x:y\\mapsto(x_1y_1,\\dots,x_ny_n)^\\top,\\text{ i.e. }L_x={\\rm Diag}(x)\\] Semidefinite programming (SDP) \\[\\text{(P) }\\min\\langle{}C_1,X_1\\rangle{}+\\dots+\\langle{}C_n,X_n\\rangle{}\\] \\[s.t.~\\langle{}A_{i1},X_1\\rangle{}+\\dots+\\langle{}A_{in},X_n\\rangle{}=b_i,X_i\\succeq0\\] \\[\\text{(D) }\\max{}b^\\top{}y\\] \\[s.t.~A_{1i}b_1+\\dots+A_{ni}b_n+S_i=C_i,S_i\\succeq0\\] Strong duality \\[\\langle{}X,S\\rangle{}=0\\Leftrightarrow{}X\\succeq0\\land{}S\\succeq0\\land\\frac{XS+SX}{2}=0\\] Second order cone programming (SOCP) \\[\\text{(P) }\\min{}c^\\top{}x,s.t.Ax=b,x\\succeq_{\\mathcal{Q}}0\\] \\[\\text{(Q) }\\min{}b^\\top{}y,s.t.A^\\top{}y+s=c,s\\succeq_{\\mathcal{Q}}0\\] Example \\[x\\in{}S,\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_n\\] \\[(\\lambda_1+\\dots+\\lambda_K)(X)\\] \\[\\Leftrightarrow\\] \\[\\min_{Y,t}Tr(Y)+Kt,~s.t.tI+Y\\succeq{}X,Y\\succeq0\\] SDP Relaxation Consider QCQP \\[\\min~x^TA_0x+2b_0^Tx+c_0\\text{ assume }A_i\\in{}S^n\\] \\[\\text{s.t. }x^TA_ix+2b_i^Tx+c_i\\leq0,i=1,\\dots,m\\] Max Cut For graph \\((V,E)\\) and weights \\(w_{ij}=w_{ji}\\geq0\\), the maxcut problem is: \\[(Q)~\\max_x\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-x_ix_j),~\\text{ s.t. }x_i\\in\\{-1,1\\}\\] where \\(x_i=-1\\) means \\(x\\) in set \\(X_1\\) and \\(x_i=1\\) means \\(x\\) in set \\(X_2\\). And its relaxation is: \\[(P)~\\max_{v_i\\in{}R^n}\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-v_iv_j),~\\text{ s.t. }||v_i||_2=1\\] The equivalent SDP of \\((P)\\) is : \\[(SDP)~\\max_{X\\in{}S^n}\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-X_{ij}),~\\text{ s.t. }X_{ii}=1,X\\succeq0\\] where \\(X=V^TV=(v_1,\\dots,v_n)^T(v_1,\\dots,v_n)\\). Add \\(rank(X)=1\\), then \\(X=xx^T\\Rightarrow{}x_i^2=1\\), which means that it is equivalent to the original problem. Greedy Way Every time we pick most distant one, then \\[Z^*\\geq\\frac{1}{2}Z_{opt}\\] Rounding Procedure Generate a vector \\(r\\) uniformly distributed on the unit sphere, i.e. \\(||r||_2=1\\) Set \\(x_i=1(v_i^Tr\\geq0),-1(\\text{otherwise})\\) Theoretical Result Let \\(W\\) be the objective function value of \\(x\\) and \\(E(W)\\) be the expected value. Then \\[E(W)=\\frac{1}{\\pi}\\sum_{i&lt;j}w_{ij}\\arccos(v_i^Tv_j)\\] Goemans and Williamson showed: \\[E(W)\\geq\\alpha\\frac{1}{2}\\sum_{i&lt;j}w_{ij}(1-v_i^Tv_j),\\alpha=\\min_{0\\leq\\theta\\leq\\pi}\\frac{2}{\\pi}\\frac{\\theta}{1-\\cos\\theta}&gt;0.878\\] SDP Representablity A set \\(X\\subseteq{}R^n\\) is SDP-representable (or SDP-Rep for short) if it can be expressed linearly as the feasible region of an SDP: \\[X=\\{x|(\\exists{}u\\in{}R^k)(\\exists{}A_i,B_j,C\\in{}R^{m\\times{}m})\\sum_ix_iA_i+\\sum_ju_jB_j+C\\succeq0\\}\\] A function \\(f(x)\\) is SDP-Rep if its epigraph is SDP-representable: \\[\\text{epi}(f)=\\{(x_0,x)|f(x)\\leq{}x_0\\}\\] If \\(X\\) is SDP-Rep, then \\(\\min_{x\\in{}X}c^Tx\\) is an SDP If \\(f(x)\\) is SDP-Rep, then \\(\\min_xf(x)\\) is an SDP 'Calculus' of SDP-Rep Sets If \\(X\\) and \\(Y\\) are SDP-Rep then so are Minkowski sum \\(X+Y\\) Intersection \\(X\\cap{}Y\\) Affine pre-image \\(A^{-1}(X)\\) if \\(A\\) is affine Affine map \\(A(X)\\) if \\(A\\) is affine Cartesian Product \\(X\\times{}Y=\\{(x,y)|x\\in{}X,y\\in{}Y\\}\\) Proof of Affine map \\[Y=\\{Fx+f|F\\in{}R^{m\\times{}n},x\\in{}X\\}\\] \\[m\\geq{}n,rank(F)=n\\] \\[y=Fx+f\\Rightarrow{}x=(F^TF)^{-1}F^T(y-f)=F^+(y-f)\\] \\[A(F^+(y-f))+B(u)+C\\geq0\\Rightarrow{}A(F^+y)+B(u)+C-A(F^+f)\\geq0\\] \\[m\\leq{}n,rank(F)=m\\] \\[y=Fx\\Rightarrow{}y=[F_1,F_2][x_1,x_2]^T\\Rightarrow{}x_1=F^{-1}(y-F_2x_2)\\] 'Calculus' of SDP-Rep Functions If functions \\(f_i,i=1,\\dots,m\\) and \\(g\\) are SDP-Rep. Then the following are SDP-Rep: nonnegative sum \\(\\sum\\alpha_if_i\\) for \\(\\alpha_i\\geq0\\) maximum \\(\\max_if_i\\) composition \\(g(f_1(x),\\dots,f_m(x))\\) Legendre transform \\(f^*(y)=\\max_xy^Tx-f(x)\\) Positive Polynomials The set of nonnegative polynomials of a given degree forms a proper cone \\[P_n=\\{(p_0,\\dots,p_n)|(\\forall{}t\\in{}I)p_o+p_1t+\\dots+p_nt^n&gt;0\\}\\] The cone of positive polynomials is SDP-Rep"},{"title":"可修复系统的可靠性分析","slug":"可修复系统的可靠性分析","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/04/01/可修复系统的可靠性分析/","link":"","permalink":"https://zs-liu.github.io/2019/04/01/%E5%8F%AF%E4%BF%AE%E5%A4%8D%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%88%86%E6%9E%90/","excerpt":"维修性特征量 维修分布函数/维修度 产品从故障开始到修理完毕经历的时间 \\(Y\\) \\[M(t)=P(Y\\leq{}t)\\] 修复率 尚未修复的产品在单位时间内修复完成 \\[\\mu(t)=\\frac{m(t)}{1-M(t)}\\] 平均修复时间 \\[MTTR=\\int_0^\\infty{}tm(t)dt=\\int_0^\\infty{}tdM(t)\\]","text":"维修性特征量 维修分布函数/维修度 产品从故障开始到修理完毕经历的时间 \\(Y\\) \\[M(t)=P(Y\\leq{}t)\\] 修复率 尚未修复的产品在单位时间内修复完成 \\[\\mu(t)=\\frac{m(t)}{1-M(t)}\\] 平均修复时间 \\[MTTR=\\int_0^\\infty{}tm(t)dt=\\int_0^\\infty{}tdM(t)\\] 可用性 也称有效性，综合反映可靠性和维修性，即可维修产品使用效率的广义可靠性 规定条件包括工作条件和维修条件 特征量 瞬时可用度 可维修产品在某时刻具有或维持功能的概率 瞬时可用度常用于理论分析 平均可用度 \\[\\bar{A}(t_1,t_2)=\\frac{1}{t_2-t_1}\\int_{t_1}^{t_2}A(t)dt\\] 稳态可用度 \\[A=A(\\infty)=\\lim_{t\\rightarrow\\infty}A(t)=\\frac{MTBF}{MTBF+MTTR}\\] 当可靠度函数和维修度函数均是指数函数： \\[A=\\frac{\\mu}{\\mu+\\lambda}\\] 预防维修与事后维修 预防维修 计划性的维修活动，最常用的形式是定期检修，使设备总是 保持良好的工作状态 按照规定程序，假定起始时间为 \\(0\\)，检修时间间隔为 \\(T\\)，则产品工作时间 \\(t=jT+\\tau\\) 理想预防维修系统的可用度 平均预防维修时间 \\(T_p\\) ；平均事后维修时间 \\(T_f\\) 系统平均停工时间 \\(MDT=R(T)T_p+(1-R(T))T_f\\) 系统平均工作时间 \\(MUT=\\int_0^TR(t)dt\\) \\[A=\\frac{MUT}{MUT+MDT}\\] 事后维修 系统投入运行后，一旦发生故障，立即开始修理 系统只存在运行状态 \\(S\\) 和失效状态 \\(F\\) 状态转移关系 \\[P_F(t+\\Delta{}t)=P_S(t)\\lambda(t)\\Delta{}t+P_F(t)[1-\\mu(t)\\Delta{}t]\\] \\[P_F(t)=\\exp\\{-Q(t)\\}\\int_0^t\\exp\\{Q(t)\\}\\lambda(t)dt,Q(t)=\\int_0^t[\\lambda(t)+mu(t)]dt\\] 马尔科夫模型求解可修复系统可靠性 马尔科夫过程 \\[P(X(t_n)=x_n|X(t_1)=x_1,\\dots,X(t_{n-1})=x_{n-1})=P(X(t_n)=x_n|X(t_{n-1})=x_{n-1})\\] 随机过程 \\(\\{X(t),t\\geq0\\}\\) 是连续时间和离散状态空间 \\(S=\\{0,1,2,\\dots,n\\}\\) 的马尔科夫过程，若满足如下条件（即转移密度为常数），则称此过程为齐次马尔科夫过程： \\[P(X(t+\\Delta{}t)=j|X(t)=i)\\equiv{}p_{ij}(\\Delta{}t)\\] 由归一化条件，显然有： \\[p_{ii}+\\sum_{j\\neq{}i}p_{ij}=1\\] 转移率 \\[q_{ij}=\\lim_{\\Delta{}t\\rightarrow0}\\frac{p_{ij}(\\Delta{}t)}{\\Delta{}t},q_{i}=\\lim_{\\Delta{}t\\rightarrow0}\\frac{1-p_{ii}(\\Delta{}t)}{\\Delta{}t}\\] 由全概率条件，显然有： \\[q_i=\\sum_{j\\neq{}i}q_{ij}\\] 转移率矩阵为： \\[a_{ij}=q_{ij}~(i\\neq{}j)~\\text{or}~-q_i~(i=j)\\] 利用转移率矩阵求解系统可靠性参数 状态概率：系统在 \\(t\\) 时刻处于状态 \\(i\\) 的概率 可用度：瞬时可用度、稳态可用度 系统可靠度、平均首次故障时间 状态频率、状态持续时间 系统状态概率 令 \\[P(t)=[p_1(t),p_2(t),\\dots,p_n(t)]\\] 则有： \\[P^{&#39;}(t)=P(t)A\\] 等式两边使用拉普拉斯变换 \\[sP^*(s)-P(0)=P^*(s)A\\] \\[P^*(s)=P(0)(sI-A)^{-1}\\] 然后进行分式分解（\\(s_i\\) 为矩阵 \\(sI-A\\) 第 \\(i\\) 个特征值） \\[P^*_j(s)=\\sum_{i=0}^Nb_{ji}(s-s_i)^{-1}\\] 从而可以得到原函数 \\[P_j(t)=\\sum_{i=0}^Nb_{ji}\\exp(s_it)\\] 系统可用度 系统的瞬时可用度 \\(A(t)\\) \\[A(t)=\\sum_{j\\in{}W}p_j(t)\\] 系统稳态可用度 \\(A(\\infty)\\) \\[(\\pi_0,\\dots,\\pi_n)A=(0,\\dots,0),\\pi_0+\\dots+\\pi_n=1\\] \\[A(\\infty)=\\sum_{j\\in{}W}\\pi_j\\] 系统可靠度与平均首次故障前时间MTTFF 状态频率和持续时间"},{"title":"Duality","slug":"Duality","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2019/03/26/Duality/","link":"","permalink":"https://zs-liu.github.io/2019/03/26/Duality/","excerpt":"Lagrange dual problem Lagrangian \\[\\min{}f_0(x),s.t.f_i(x)\\leq0,h_i(x)=0\\] variable \\(x\\in{}R^n\\), domain \\(\\mathcal{D}\\), optimal value \\(p^\\star\\) \\[L:R^n\\times{}R^m\\times{}R^p\\rightarrow{}R,dom~L=\\mathcal{D}\\times{}R^m\\times{}R^p\\] \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\]","text":"Lagrange dual problem Lagrangian \\[\\min{}f_0(x),s.t.f_i(x)\\leq0,h_i(x)=0\\] variable \\(x\\in{}R^n\\), domain \\(\\mathcal{D}\\), optimal value \\(p^\\star\\) \\[L:R^n\\times{}R^m\\times{}R^p\\rightarrow{}R,dom~L=\\mathcal{D}\\times{}R^m\\times{}R^p\\] \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] Lagrange dual function \\[g:R^m\\times{}R^p\\rightarrow{}R\\] \\[g(\\lambda,\\nu)=\\inf_{x\\in{}D}L(x,\\lambda,\\nu)=\\inf_{x\\in{}D}(f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x))\\] lower bound property: if \\(\\lambda\\geq0\\), then \\(g(\\lambda,\\nu)\\leq{}p^\\star\\) Lagrange dual problem \\[\\max{}g(\\lambda,\\nu),s.t.\\lambda\\succeq0\\] optimal value \\(d^\\star\\) Weak and strong duality weak duality: \\(d^\\star\\leq{}p^\\star\\) always holds (for convex and nonconvex problems) strong duality: \\(d^\\star=p^\\star\\) (usually) holds for convex problems Slater's constraint qualification \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }f_i(x)\\leq0,Ax=b\\] strong duality holds if it is strictly feasible, i.e., \\[\\exists{}x\\in{}\\text{relint }D:~f_i(x)&lt;0,Ax=b\\] there exist many other types of constraint qualifications Geometric interpretation Omit Complementary slackness Assume strong duality holds, \\(x^\\star\\) is primal optimal, \\((\\lambda^\\star,\\nu^\\star)\\) is dual optimal - \\(x^\\star\\) minimizes \\(L(x,\\lambda^\\star,\\nu^\\star)\\) - \\(\\lambda_i^\\star{}f_i(x^\\star)=0\\) for \\(i=1,\\dots,m\\) KKT conditions primal constraints: \\(f_i(x)\\leq0,i=1,\\dots,m,h_i(x)=0,i=1,\\dots,p\\) dual constraints: \\(\\lambda\\succeq0\\) complementary slackness: \\(\\lambda_if_i(x)=0\\) gradient of Lagrangian with respect to \\(x\\) vanishes: \\[\\nabla{}f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla{}f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla{}h_i(x)=0\\] KKT conditions for convex problem If \\(\\tilde{x},\\tilde{\\lambda},\\tilde{\\nu}\\) satisfy KKT for a convex problem, then they are optimal. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda, \\nu\\) that satisfy KKT conditions. Perturbation and sensitivity analysis \\[f_i(x)\\leq0\\Rightarrow{}f_i(x)\\leq{}u_i\\] \\[h_i(x)=0\\Rightarrow{}h_i(x)=v_i\\] \\[g(\\lambda,\\nu)\\Rightarrow{}g(\\lambda,\\nu)-u^T\\lambda-v^T\\nu\\] \\[p^\\star(u,v)\\geq{}g(\\lambda^\\star,\\nu^\\star)-u^T\\lambda^\\star-v^T\\nu^\\star=p^\\star(0,0)-\\lambda^\\star-\\nu^\\star\\] Local sensitivity If \\(p^\\star(u,v)\\) is differentiable at \\((0,0)\\), then \\[\\lambda_i^\\star=-\\frac{\\partial{}p^\\star(0,0)}{\\partial{}u_i},\\nu_i^\\star=-\\frac{\\partial{}p^\\star(0,0)}{\\partial{}v_i}\\] Duality and problem reformulations Introducing new variables and equality constraints \\[\\text{minimize }f_0(Ax+b)\\] dual function is constant we have strong duality, but dual is quite useless \\[\\Rightarrow\\text{minimize }f_0(y)\\text{, subject to }Ax+b-y=0\\] Implicit constraints \\[\\text{minimize }c^Tx\\text{, subject to }Ax=b,-1\\preceq{}x\\preceq{}1\\] \\[\\Rightarrow\\text{minimize }f_0(x)=c^Tx(-1\\preceq{}x\\preceq{}1),\\infty(\\text{otherwise})\\text{, subject to }Ax=b\\] Problems with generalized inequalities \\[\\text{minimize }f_0(x)\\text{, subject to }f_i(x)\\preceq{}_{K_i}0,h_i(x)=0\\] Semidefinite program \\[\\text{minimize }c^Tx,\\text{, subject to }x_1F_1+\\dots+x_nF_n\\preceq{}G\\] Lagrange multiplier is matrix \\(Z\\in{}S^k\\) Lagranigian \\[L(x,Z)=c^Tx+tr(Z(x_1F_1+\\dots+x_nF_n-G))\\] dual function \\[g(Z)=\\inf_xL(x,Z)=-tr(GZ)(\\text{for }tr(F_iZ)+c_i=0),-\\infty(\\text{otherwise})\\]"},{"title":"Simplex Algorithm","slug":"Simplex Algorithm","date":"un55fin55","updated":"un00fin00","comments":true,"path":"2019/03/22/Simplex Algorithm/","link":"","permalink":"https://zs-liu.github.io/2019/03/22/Simplex%20Algorithm/","excerpt":"Convert LP to a Standard Form \\[\\min{}c^\\top{}x,\\text{subject to }Ax=b\\land{}x\\geq0\\] For Inequality \\(x+y\\geq{}a\\rightarrow{}x+y-z=a,z\\geq0\\) \\(x+y\\leq{}a\\rightarrow{}x+y+s=a,s\\geq0\\) Unrestricted \\(\\rightarrow{}x=y-z,y\\geq0,z\\geq0\\)","text":"Convert LP to a Standard Form \\[\\min{}c^\\top{}x,\\text{subject to }Ax=b\\land{}x\\geq0\\] For Inequality \\(x+y\\geq{}a\\rightarrow{}x+y-z=a,z\\geq0\\) \\(x+y\\leq{}a\\rightarrow{}x+y+s=a,s\\geq0\\) Unrestricted \\(\\rightarrow{}x=y-z,y\\geq0,z\\geq0\\) Preview of Simplex Algorithm Basic solutions \\[Ax=b,A\\in{}R^{m\\times{}n},x\\in{}R^n,n\\geq{}m\\] Set \\(n-m\\) variables to \\(0\\) and solving for the remaining \\(m\\) variables. The columns for the remaining \\(m\\) variables are linear independent. \\[\\text{Total number: }C_n^m\\] Basic feasible solutions Feasible region for any LP problem is a convex set If a LP has an optimal solution, there must be an extreme point of the feasible region that is optimal Proof \\[\\text{Suppose }x^\\star=\\sum_{i=1}^n\\alpha_ix_i\\text{ is an optimal solution}\\] \\[c^\\top{}x^\\star=c^\\top{}\\sum_{i=1}^n\\alpha_ix_i&gt;\\sum_{i=1}^n\\alpha_ic^\\top{}x^\\star=c^\\top{}x^\\star\\] Adjacent Basic Feasible Solutions Two basic feasible solutions are said to be adjacent if their sets of basic variables have \\(m-1\\) basic variables in common General Description of the Simplex Algorithm Find an initial bfs of LP Change bfs to its adjacent bfs Recalculate by Gaussian elimination For maximum, until the first row is all nonnegative; For minimum, until the first row is all nonpositive Example Standard form: \\[\\max{}z=3x_1+5x_2\\] \\[\\begin{split} \\text{subject to }\\quad{}x_1+s_1&amp;=8\\\\ 2x_2+s_2&amp;=12\\\\ 3x_1+4x_2+s_3&amp;=36\\\\ x_1,x_2,s_1,s_2,s_3&amp;\\geq0 \\end{split} \\label{p1s}\\] Choose \\((s_1,s_2,s_3)\\) as BFS, we have table: \\(z\\) \\(x_1\\) \\(x_2\\) \\(s_1\\) \\(s_2\\) \\(s_3\\) rhs Basic Variable 1 -3 -5 0 0 0 0 \\(z=0\\) 0 1 0 1 0 0 8 \\(s_1=8\\) 0 0 2 0 1 0 12 \\(s_2=12\\) 0 3 4 0 0 1 36 \\(s_3=36\\) Choose \\((s_1,x_2,s_3)\\) as BFS, we have table: \\(z\\) \\(x_1\\) \\(x_2\\) \\(s_1\\) \\(s_2\\) \\(s_3\\) rhs Basic Variable 1 0 0 0 2.5 0 30 \\(z=30\\) 0 1 0 1 0 0 8 \\(s_1=8\\) 0 0 1 0 0.5 0 6 \\(x_2=6\\) 0 3 0 0 -2 1 12 \\(s_3=12\\) Choose \\((s_1,x_2,x_1)\\) as BFS, we have table: \\(z\\) \\(x_1\\) \\(x_2\\) \\(s_1\\) \\(s_2\\) \\(s_3\\) rhs Basic Variable 1 0 0 0 0.5 0 42 \\(z=42\\) - - - - - - - - 0 0 1 0 0.5 0 12 \\(x_2=6\\) 0 1 0 0 -2/3 1/3 4 \\(x_1=4\\) So the final result is \\[x_1=4,x_2=6,z_{max}=42\\] Degeneracy An LP is degenerate if it has at least one basic feasible solution in which a basic feasible variable is equal to \\(0\\) pivoting may not improve the objective value simplex method may end up in cycles Two BFS Methods Big M Method \\[\\min{}f(x),\\text{ subject to }Ax=b(b\\succeq0)\\] \\[\\Rightarrow\\min{}f(x)+M^\\top{}a,\\text{ subject to }Ax+Ia=b\\] Introduce \\(a_i\\) to every row which has a negative coefficient (on \\(s\\) or \\(z\\)), then we can get bfs easily \\[(x=0,a=b)\\] If we can't reduce all parameters of \\(a\\) to 0, then the original problem is infeasible Two-Phase Method \\[\\min{}f(x),\\text{ subject to }Ax=b(b\\succeq0)\\] \\[\\Rightarrow\\min{}I^\\top{}a,\\text{ subject to }Ax+Ia=b,a\\succeq0\\] If \\(a^\\star\\neq0\\), then the original problem is infeasible"},{"title":"Convex Optimization Problems","slug":"Convex Optimization Problems","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2019/03/19/Convex Optimization Problems/","link":"","permalink":"https://zs-liu.github.io/2019/03/19/Convex%20Optimization%20Problems/","excerpt":"Optimization problem in standard form \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }f_i(x)\\leq0\\quad{}\\land{}\\quad{}Ax=b\\]","text":"Optimization problem in standard form \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }f_i(x)\\leq0\\quad{}\\land{}\\quad{}Ax=b\\] Theorem: Optimal and locally optimal options Any locally optimal point of a convex problem is (globally) optimal. #### Proof Suppose \\(x\\) is locally optimal, but there exists a feasible \\(y\\) with \\(f_0(y)&lt;f_0(x)\\), for \\(z\\) in the small neighbour of \\(x\\), contradiction must exist Theorem \\(x\\) is optimal if and only if it is feasible and \\[\\nabla{}f_0(x)^T(y-x)\\geq0\\quad\\text{for all feasible }y\\] Proof \\[f_0(y)\\geq{}f_0(x)+\\nabla{}f_0(x)^T(y-x)\\geq{}f_0(x)\\] \\[\\text{Suppose }\\exists{}y,~s.t.~\\nabla{}f_0(x)^T(y-x)&lt;0\\] \\[g(t)=f(x+y(y-x)),g^{&#39;}(0)=\\nabla{}f_0(x)^T(y-x)&lt;0\\] \\[\\exists{}\\bar{t},g(\\bar{t})&lt;g(0)=f_0(x)\\] Also, we can view this with Taylor expansion Equivalent convex problems Eliminating equality constrains \\[Ax=b\\rightarrow{}x=Fz+x_0~\\text{for some }z\\] Introducing equality constraints \\[y_i=A_ix+b\\] Introducing slack variables \\[a_i^Tx\\leq{}b_i\\rightarrow{}a_i^Tx+s_i=b_i\\land{}s_i\\geq0\\] Quasiconvex optimization If \\(f_0\\) is quasiconvex, there exists a family of functions \\(\\phi_t\\) such that: \\(\\phi_t(x)\\) is convex in \\(x\\) for fixed \\(t\\) t-sublevel set of \\(f_0\\) is 0-sublevel set of \\(\\phi_t\\), i.e. \\[f_0(x)\\leq{}t\\leftrightarrow{}\\phi_t(x)\\leq0\\] Linear program (LP) \\[\\text{minimize }c^Tx+d\\] \\[\\text{subject to }Gx\\leq{}h\\land{}Ax=b\\] convex problem with affine objective and constraint functions feasible set is a polyhedron Linear fractional program \\[\\text{minimize }f_0(x)\\] \\[\\text{subject to }Gx\\leq{}h\\land{}Ax=b\\] \\[f_0(x)=\\frac{c^Tx+d}{c^Tx+f}, dom~f_0(x)=\\{c^Tx+f&gt;0\\}\\] a quasiconvex optimization problem; can be solved by bisection also equivalent to the LP (variables \\(y\\), \\(z\\)) \\[\\text{minimize }c^Ty+dz\\] \\[\\text{subject to }Gy\\leq{}hz,Ay=bz,e^Ty+fz=1,z\\geq0\\] Proof \\[\\forall{}x\\in{}X_1,y=\\frac{x}{e^Tx+f},z=\\frac{1}{e^Tx+f}\\rightarrow{}(y,z)\\in{}X_2\\rightarrow{}p_1^*\\geq{}p_2^*\\] \\[\\forall{}(y,z)\\in{}X_2, z\\neq0,x=\\frac{y}{z}\\in{}X_1\\] \\[\\forall{}(y,z)\\in{}X_2, z=0,\\text{Choose }x_0\\in{}X_1,x_0+ty\\in{}X_1(t\\geq0)\\] \\[\\lim_{t\\rightarrow\\infty}\\frac{c^T(x_0+ty)+d}{e^T(x_0+ty)+f}=c^Ty,p_2^*\\geq{}p_1^*\\] Quadratic program (QP) \\[A\\in{}R^{m\\times{}n},A=U\\Sigma{}V^T,\\Sigma=diag(\\lambda_1,...,\\lambda_r,0,...)\\] \\[A^\\dagger=V\\Sigma^\\dagger{}U^T,\\Sigma^\\dagger=diag(1/\\lambda_1,...,1/\\lambda_r,0,...)\\] Quadratically constrained quadratic program (QCQP) \\[\\text{minimize }(1/2)x^TP_0x+q_0^Tx+r_0\\] \\[\\text{subject to }(1/2)x^TP_ix+q_i^Tx+r_i\\leq0, Ax=b\\] \\(P_i\\in{}S_{+}^{n}\\) objective and constraints are convex quadratic Least squares \\[\\text{minimize }||Ax-b||_2^2\\] analytical solution \\(x^\\star=A^\\dagger{}b\\) Robust linear program \\[\\text{minimize }c^Tx\\] \\[\\text{subject to }prob(a_i^Tx\\leq{}b_i)\\geq\\eta,i=1,...,m\\] Semidefinite program (SDP) \\[\\text{minimize }c^Tx\\] \\[\\text{subject to }x_1F_1+x_2F_2+\\dots+x_nF_n+G\\preceq0,Ax=b\\] inequality constraint is called linear matrix inequality (LMI) includes problems with multiple LMI constraints"},{"title":"不可修复系统的可靠性分析","slug":"不可修复系统的可靠性分析","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/03/18/不可修复系统的可靠性分析/","link":"","permalink":"https://zs-liu.github.io/2019/03/18/%E4%B8%8D%E5%8F%AF%E4%BF%AE%E5%A4%8D%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%88%86%E6%9E%90/","excerpt":"可靠性逻辑框图 系统可靠性 在元件故障数据和系统结构已知的情况下，预测系统的可靠性 硬件可靠性/人员操作可靠性/软件可靠性","text":"可靠性逻辑框图 系统可靠性 在元件故障数据和系统结构已知的情况下，预测系统的可靠性 硬件可靠性/人员操作可靠性/软件可靠性 系统可靠性框图 描述系统与其组成单元之间的故障逻辑关系，以系统工程图为基础。 方框:单元或功能 逻辑关系:功能布局 连线:系统功能流程的方向 节点(节点可以在需要时才加以标注) 输入节点:系统功能流程的起点 输出节点:系统功能流程的终点 中间节点 典型不可修系统的可靠性分析 可靠性分析假设 系统及组成单元只有故障与正常两种状态 不同方框表示的不同单元的故障概率是相互独立的 不考虑输入错误引起的系统故障 假设系统的整个软件是完全可靠的 假设人员操作是完全可靠的。 串联系统 \\[S=x_1\\cap{}x_2\\cap{}\\dots\\cap{}x_n\\] \\[\\bar{S}=\\bar{x}_1\\cup\\bar{x}_2\\cup\\dots\\cup\\bar{x}_n\\] \\[R_S(t)=\\prod_{i=1}^nR_i(t)\\] 当各单元的寿命分布均为指数分布时，系统的寿命也服从指数分布 串联系统中提高可靠度最小的设备的可靠性，对系统可靠性的提高贡献最大 并联系统 \\[S=x_1\\cup{}x_2\\cup{}\\dots\\cup{}x_n\\] \\[\\bar{S}=\\bar{x}_1\\cap\\bar{x}_2\\cap\\dots\\cap\\bar{x}_n\\] \\[R_S(t)=1-\\prod_{i=1}^n(1-e^{-\\lambda_it})\\] \\[MTTF_S=\\sum_{i=1}^n(i\\lambda)^{-1}\\] 三并联到四并联基本上可以满足要求 并联系统中提高可靠度最大的设备的可靠性，对系统可靠性的提高贡献最大 \\(k/n\\)表决系统 \\[R_S(t)=\\sum_{i=0}^{n-k}C_n^i[1-R(t)]^i[R(t)]^{n-i}\\] \\[\\text{For exponential distribution, }MTTF_S=\\sum_{i=k}^n\\frac{1}{i\\lambda}\\] 储备系统 组成系统的各单元只有一个单元工作，当工作单元故障时，通过转换装置接到另一个单元继续工作，直到所有单元都故障时系统才故障，称为非工作贮备系统(又可称为旁联系统) 卷积方法 \\[f_{12}(t)=f_1(t)*f_2(t)=\\int_0^tf_2(t-t_1)f_1(t)dt_1\\] \\(f_1(t)=\\lambda_1e^{-\\lambda_1t},f_2(t)=\\lambda_2e^{-\\lambda_2t}\\) \\[L[f_{12}(t)]=L[f_1(t)*f_2(t)]=L[f_1(t)]\\cdot{}L[f_2(t)]=\\frac{\\lambda_1}{s+\\lambda_1}\\frac{\\lambda_2}{s+\\lambda_2}\\] \\[\\Rightarrow{}f_{12}(t)=\\frac{\\lambda_1\\lambda_2}{\\lambda_1-\\lambda_2}(e^{-\\lambda_2t}-e^{-\\lambda_1t})\\] 复合事件概率法 两设备的失效可认为是独立的 计算储备系统时需考虑两者产生的非独立性（在时间上） \\[R_{12}(t)=R_1(t)+\\int_0^tR_2(t-t_1)f_1(t_1)dt_1\\] 考虑冗余设备的备用失效 \\(R_2^{-}(\\cdot)\\) 考虑不完全切换 \\(R_{SW}\\) \\[R_{12}(t)=R_1(t)+R_{SW}\\int_0^tR_2(t-t_1)R_2^{-}(t_1)f_1(t_1)dt_1\\] 网络系统 全概率分解法分析复杂系统可靠性 \\[R_S(t)=P(S)=P(x)P(S|x)+P(\\bar{x})P(S|\\bar{x})\\] \\(S(x)\\) 表示把网络 \\(S\\) 中设备 \\(x\\) 的两端节点合成一个节点而产生的新网络 \\(S(\\bar{x})\\) 表示把网络 \\(S\\) 中设备 \\(x\\) 去掉(即两个端点之间不存在经由 \\(x\\) 的联系)而产生的新网络 \\[R_S(t)=P(S)=P(x)P(S(x))+P(\\bar{x})P(S(\\bar{x}))\\] 选用分解弧的原则 对任意无向弧可以作为分解弧 与输入节点与输出节点相连的弧可以作为分解弧 对任意有向弧，其两端点的任何一个只有流入或流出的弧，可以作为分解弧，若弧的两端都有流入和流出的弧不可作为分解弧（因为不可以引入原本不存在的通路） 最小路集/最小割集法分析系统可靠性 结构函数 \\[Y=\\varphi(X)=\\varphi(x_1,x_2,\\dots,x_n)\\] \\[\\bar{\\varphi}(X)=1-\\varphi(\\overline{1-X})\\] \\[C_1(X)=\\{i|x_i=1\\},C_0(X)=\\{i|x_i=0\\}\\] 路集中任何一个设备变成失效则系统失效，此路集为最小路集 割集中任何一个设备变成成功则系统成功，此割集为最小割集 单调关联系统的表示 设最小路集为 \\(p_1,\\dots,p_m\\)，最小割集为 \\(k_1,\\dots,k_m\\) \\[\\varphi(x)=\\bigcup_{j=1}^m\\bigcap_{i\\in{}p_j}x_i=\\bigcap_{j=1}^l\\bigcup_{i\\in{}k_j}x_i\\] 最小路集/最小割集求解 联络矩阵法 \\[C:\\{C_{ij}=x\\text{ if arc }x\\text{ exists between }i,j\\text{; }0\\text{ otherwise}\\}\\] \\[C^r=C\\times{}C^{r-1}\\] 如果研究 \\(I\\) 到 \\(L\\) 的可靠性 只需求出\\(C^2,C^3,\\dots,C^{n-1}\\)中第 \\(L\\) 列 \\(C^{n-1}\\)只需求出第 \\(I\\) 行 布尔行列式 给定联络矩阵 \\(C\\) ，令 \\(D=C+I\\) 删去输入节点列，输出节点行，得到 \\(S\\) \\(|S|\\) 展开并且各项取正 最小割集 \\(S\\) 最小割集为 \\(\\bar{S}\\) 最小路集 利用最小路集/最小割集求解系统可靠度 精确解 最小路集/最小割集一般相交（可以直接用容斥原理求解），可对其进行“不交化”，得到相互独立的最小路集/最小割集 \\[K_1\\cup{}K_2=K_1+\\bar{K}_1K_2\\] 近似解 区间估计：当设备可靠度较高，容斥原理展式的首项或前两项起主要作用 点估计 特殊情况：三角形与星形"},{"title":"Convex Functions","slug":"Convex Functions","date":"un44fin44","updated":"un00fin00","comments":true,"path":"2019/03/07/Convex Functions/","link":"","permalink":"https://zs-liu.github.io/2019/03/07/Convex%20Functions/","excerpt":"Basic properties and examples Definition \\(f:R^n\\rightarrow{}R\\) is convex if dom\\(f\\) is a convex set and \\[f(\\theta{}x+(1-\\theta)y)\\leq\\theta{}f(x)+(1-\\theta)f(y)\\] for all \\(x,y\\in\\) dom\\(f\\), \\(0\\leq\\theta\\leq1\\)","text":"Basic properties and examples Definition \\(f:R^n\\rightarrow{}R\\) is convex if dom\\(f\\) is a convex set and \\[f(\\theta{}x+(1-\\theta)y)\\leq\\theta{}f(x)+(1-\\theta)f(y)\\] for all \\(x,y\\in\\) dom\\(f\\), \\(0\\leq\\theta\\leq1\\) strictly convex if we choose \\(&lt;\\) instead of \\(\\leq\\) Examples on \\(R\\), \\(R^n\\) and \\(R^{m\\times{}n}\\) Restriction of a convex function to a line \\(f:R^n\\rightarrow{}R\\) is convex if and only if the function \\(g:R\\rightarrow{}R\\) \\[g(t)=f(x+tv), dom g=\\{\\}\\] #### Proof \\[\\forall{}t_1,t_2,x+t_1v\\in{}dom~f,x+t_2v\\in{}dom~f\\] \\[g(\\theta{}t_1+(1-\\theta)t_2)=f(x+(\\theta{}t_1+(1-\\theta{})t_2)v)\\] \\[=f(\\theta(x+t_1v)+(1-\\theta)(x+t_2v))\\] \\[\\leq{}\\theta{}f(x+t_1v)+(1-\\theta)f(x+t_2v)\\] \\[let~x_1,x_2\\in{}dom~f, de\\!{}fine~x=x_1,v=x_2-x_1,g(t)=f(x+tv)\\] \\[f(\\theta{}x_1+(1-\\theta)x_2)=f(x_1+(1-\\theta)(x_2-x_1))=g(1-\\theta)\\] \\[\\leq{}\\theta{}g(0)+(1-\\theta)g(1)=\\theta{}f(x_1)+(1-\\theta)f(x_2)\\] Example \\(f:S^n\\rightarrow{}R\\) with \\(f(X)=\\log\\det{}X\\), dom \\(f=S^n_{++}\\) ### First-order condition Differentiable \\(f\\) with convex domain is convex iff \\[f(y)\\geq{}f(x)+\\nabla{}f(x)^T(y-x)\\quad\\forall{}x,y\\in{}dom~f\\] #### Proof \\[t&gt;0,~f(x+t(y-x))\\leq{}(1-t)f(x)+tf(y)\\] \\[f(y)\\geq{}f(x)+\\frac{f(x+t(y-x))-f(x)}{t}\\] \\[f(y)\\geq{}f(x)+f^{&#39;}(x)(y-x)\\] \\[f(x)\\geq{}f^{&#39;}(z)(x-z),f(y)\\geq{}f^{&#39;}(z)(y-z)\\] \\[let~z=\\theta{}x+(1-\\theta{})y\\] \\[\\theta{}f(x)+(1-\\theta)f(y)\\geq{}f(z)\\] ### Second-order condition Twice differentiable \\(f\\) with convex domain is convex iff \\[\\nabla^2f(x)\\succeq0\\quad\\forall{}x\\in{}dom~f\\] if \\(\\nabla^2f(x)\\succ0\\) for all \\(x\\in{}dom~f\\), then \\(f\\) is strictly convex ### Epigraph and sublevel set \\(\\alpha\\)-sublevel set of \\(f:R^n\\rightarrow{}R\\) \\[C_\\alpha=\\{x\\in{}dom~f|f(x)\\leq\\alpha\\}\\] sublevel sets of convex functions are convex (converse is false) epigraph of \\(f:R^n\\rightarrow{}R\\) \\[epi~f=\\{(x,t)\\in{}R^{n+1}|x\\in{}dom~f,f(x)\\leq{}t\\}\\] \\(f\\) is convex iff \\(epi~f\\) is a convex set ## Operations that preserve convexity ### Positive weighted sum &amp; composition with affine function nonnegative multiple sum composition with affine function Pointwise Maximum if \\(f_1,...,f_m\\) are convex, then \\[f(x)=\\max\\{f_1(x),...,f_m(x)\\}\\] is convex ### Pointwise Supremum if \\(f(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\[g(x)=\\sup_{y\\in\\mathcal{A}}f(x,y)\\] is convex ### Theorem Let \\(f:R^n\\rightarrow{}R\\) convex and \\(dom~f:R^n\\), then \\[f(x)=\\sup\\{g(x)|g~is~af\\!{}fine,g(z)\\leq{}f(z),\\forall{}z\\}\\] #### Proof \\(\\geq\\), obvious \\(\\leq\\) \\[(x,f(x))\\in{}bd~epi~f\\] \\[\\exists{}(a,b)\\neq0~s.t.~a^Tz+bt\\leq{}a^Tx+bf(x)\\] \\[a^Tz+b(f(z)+s)\\leq{}a^Tx+bf(x)\\] \\[b\\leq0,\\text{otherwise }bs\\rightarrow{}\\infty\\] \\[b=0,z=x+a\\rightarrow{}a=0,\\text{contradiction}\\] \\[b&lt;0,s=0\\rightarrow{}f(z)\\geq{}f(x)+\\frac{a^T(x-z)}{b}=g(z)\\] ### Jensen's inequality if \\(f\\) is convex, then\\[f(Ez)\\leq{}Ef(z)\\] #### Proof \\[Let~x_0=\\int_{\\Omega}x\\rho{}(x)dx\\] \\[\\exists{}a,b~s.t.~ax+b\\leq{}f(x)~\\text{and}~ax_0+b=f(x_0)\\] \\[\\int_{\\Omega}f(x)\\rho{}(x)dx\\geq{}\\int_{\\Omega}(ax+b)\\rho(x)dx=ax_0+b=f(x_0)\\] ### Composition with scalar functions composition of \\(g:R^n\\rightarrow{}R\\) and \\(h:R\\rightarrow{}R\\) \\[f(x)=h(g(x))\\] \\(f\\) is convex if \\(g\\) convex, \\(h\\) convex, \\(\\tilde{h}\\) nondecreasing \\(g\\) concave, \\(h\\) convex, \\(\\tilde{h}\\) nonincreasing Minimization if \\(f(x,y)\\) is convex in \\((x,y)\\) and \\(C\\) is a convex set, then\\[g(x)=\\inf_{y\\in{}C}f(x,y)\\] is convex ### Perspective the perspective of a function \\(f:R^n\\rightarrow{}R\\) is the function \\(g:R^n\\times{}R\\rightarrow{}R\\) \\[g(x,t)=tf(x/t),~dom~g=\\{(x,t)|x/t\\in{}dom~f,t&gt;0\\}\\] \\(g\\) is convex if \\(f\\) is convex #### Proof use epi graph \\[(x,t,s)\\in{}epi~g\\] \\[s\\geq{}tf(x/t)\\] \\[s/t\\geq{}f(x/t)\\] \\[(x/t,s/t)\\in{}epi~f\\] ## Conjugate function the conjugate of a function \\(f\\) is \\[f^*(y)=\\sup_{x\\in{}dom~f}(y^Tx-f(x))\\] \\(f^*\\) is convex (even if \\(f\\) is not) \\(f(x)+f(y)\\geq{}x^Ty\\) \\(f\\) is convex and closed (\\(epi~f\\) is closed), then \\(f^{**}=f\\) Proof \\[f(x)\\geq{}x^Ty-f(y)\\] \\[f(x)\\geq{}\\sup_{y\\in{}dom~f^*}(y^Tx-f(x))=f^{**}(x)\\] \\[epi~f\\subseteq{}epi~f^{**}\\] \\[Let~(x,f^{**}(x))\\notin{}epi~f\\] \\[\\exists{}[a,b]\\neq0,~s.t.[a,b][z-x,t-f^{**}(x)]^T\\leq{}c&lt;0,\\forall(z,t)\\in{}epi~f\\] \\[t=f(z)+s,s\\geq0\\] \\[b&lt;0,a^T(z-x)+b(f(z)-f^{**}(x)+s)\\leq{}c\\] \\[\\text{Define }y=-\\frac{a}{b},~y^Tz-f(z)-s-y^Tx+f^{**}(x)\\leq-\\frac{c}{b}\\] \\[\\text{Let }s=0,\\text{make }\\sup\\] \\[b=0,Let~y\\in{}dom~f^*~and~\\varepsilon&gt;0\\] \\[[a+\\varepsilon{}\\hat{y},-\\varepsilon][z-x,t-f^{**}(x)^T\\] \\[=a^T(z-x)+\\varepsilon\\hat{y}^T(z-x)-\\varepsilon(t-f^{**}(x))\\] \\[\\leq{}c+\\varepsilon(f^*(\\hat{y})+f^{**}(x)-\\hat{y}^Tx)=\\tilde{c}&lt;0\\] #### Example \\[f(X)=\\log\\det{}X,X\\in{}S_+^n\\] \\[f^*(Y)=\\sup_{X&gt;0}\\{&lt;X,Y&gt;-f(X)\\}\\] \\[Y\\geq0,f^*(Y)=+\\infty\\] \\[Y&lt;0,Y-(X^{*})^{-1}=0\\rightarrow{}f^*(Y)=Tr(I)-\\log\\det{}Y^{-1}\\] \\[Y \\ngeq \\&amp;\\nleq0\\rightarrow{}f^*(Y)=+\\infty(\\exists{}u,s.t.Yu=\\lambda{}u,\\text{choose }X=tuu^T,t&gt;0)\\] ## Quasiconvex functions ### Definition \\(f:R^n\\rightarrow{}R\\) is quasiconvex if dom \\(f\\) is convex and its sublevel sets are all convex ### Modified Jensen inequality \\[0\\leq\\theta\\leq1\\rightarrow{}f(\\theta{}x+(1-\\theta)y)\\leq\\max\\{f(x),f(y)\\}\\] #### Proof \\[\\alpha=f(x)&gt;f(y)\\rightarrow{}S_\\alpha=\\{x|f(x)\\leq\\alpha\\}\\] \\[[x,y]\\subset{}S_\\alpha\\rightarrow{}f([x,y])\\leq\\alpha\\] ### First-order condition Differentiable \\(f\\) with convex domain is quasiconvex iff \\[f(y)\\leq{}f(x)\\rightarrow\\triangledown{}f(x)^T(y-x)\\leq0\\] #### Proof \\[g(t)=f(x+t(y-x))\\rightarrow{}g^{&#39;}(0)=\\triangledown{}f(x)^T(y-x)\\leq0\\] \\[\\text{Assume } z\\in[x,y],f(z)&gt;f(x)\\&amp;{}f(z)&gt;f(y)\\] \\[\\triangledown{}f(z)^T(x-z)\\leq0~\\&amp;~\\triangledown{}f(z)^T(y-z)\\leq0\\] \\[\\triangledown{}f(z)=0\\] \\(z_0\\) in the neighbour of \\(z\\), \\(\\triangledown{}f(z)\\neq0,f(z_0)&gt;f(x),f(z_0)&gt;f(y)\\), contradict ### Second-order condition Differentiable \\(f\\) with convex domain is quasiconvex iff \\[f^{&#39;}(x)=0\\rightarrow{}f^{&#39;&#39;}(x)\\geq0\\] #### Proof \\[\\exists{}c\\in[a,b],f^{&#39;}(c)=0\\land{}f^{&#39;&#39;}(c)&lt;0\\] \\[\\exists{}\\varepsilon{}f(c)&gt;f(c-\\varepsilon),f(c)&gt;f(c+\\varepsilon)\\] \\[\\exists{}\\delta&gt;0,f(c)-\\delta&gt;f(c-\\varepsilon),f(c)+\\delta&gt;f(c+\\varepsilon)\\] \\[c-\\varepsilon,c+\\varepsilon\\in\\{x|f(x)\\leq{}f(c)-\\delta\\},c\\notin\\{x|f(x)\\leq{}f(c)-\\delta\\}\\] \\(\\{x|f(x)\\leq{}f(c)-\\delta\\}\\) is not convex, contradict \\[f^{&#39;}\\neq0\\rightarrow{}f\\uparrow{}or\\downarrow\\rightarrow{}f\\in\\text{quasiconvex}\\] \\[d=\\sup\\{c|f^{&#39;}(c)=0\\}\\] \\[\\forall{}x\\geq{}d,f{&#39;}(x)\\geq0\\rightarrow{}f(x)\\geq{}f(d)\\] \\[\\forall{}x\\leq{}d,f{&#39;}(x)\\leq0\\rightarrow{}f(x)\\leq{}f(d)\\] ## Log-concave and Log-convex functions ### Definition A positive function \\(f\\) is log-concave if \\(\\log{}f\\) is concaveA positive function \\(f\\) is log-convex if \\(\\log{}f\\) is convex powers:\\(x^a\\) on \\(R_{++}\\) is log-convex for \\(a\\leq0\\), log-concave for \\(a\\geq0\\) many common probability densities are log-concave cumulative Gaussian distribution function is log-concave Second-order condition Twice differentiable \\(f\\) with convex domain is log-concave if and only if \\[\\forall{}x,f(x)\\triangledown^2f(x)\\preceq\\triangledown{}f(x)\\triangledown{}f(x)^T\\] ### Properties product of log-concave functions is log-concave sum of log-concave functions is not always log-concave if \\(f:R^n\\times{}R^m\\rightarrow{}R\\) is log-concave, then \\[g(x)=\\int{}f(x,y)dy\\] is log-concave (not easy to show) convolution \\(f*g\\) of log-concave functions \\(f,g\\) is log-concave \\[(f*g)(x)=\\int{}f(x-y)g(y)dy\\]"},{"title":"基本参数与参数估计基础","slug":"基本参数与参数估计基础","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2019/03/04/基本参数与参数估计基础/","link":"","permalink":"https://zs-liu.github.io/2019/03/04/%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0%E4%B8%8E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80/","excerpt":"基本参数 可靠度与不可靠度 \\(n(t)\\):在\\(0\\sim{}t\\)时刻的工作时间内产品的累计失效数 \\(N_0\\):\\(t=0\\)时在规定条件下进行工作的产品数 \\[Reliability(t)=R(t)=\\frac{N_0-n(t)}{N_0}\\] \\[Fallibility(t)=F(t)=\\frac{n(t)}{N_0}\\]","text":"基本参数 可靠度与不可靠度 \\(n(t)\\):在\\(0\\sim{}t\\)时刻的工作时间内产品的累计失效数 \\(N_0\\):\\(t=0\\)时在规定条件下进行工作的产品数 \\[Reliability(t)=R(t)=\\frac{N_0-n(t)}{N_0}\\] \\[Fallibility(t)=F(t)=\\frac{n(t)}{N_0}\\] 失效率 \\[\\lambda(t)=\\lim_{\\Delta{}t\\rightarrow0}\\frac{P(t&lt;T\\leq{}t+\\Delta{}t|T&gt;t)}{\\Delta{}t}=\\frac{F^{&#39;}(t)}{1-F(t)}=-\\frac{R^{&#39;}(t)}{R(t)}\\] \\[\\hat{\\lambda}(t)=\\frac{n(t+\\Delta{}t)-n(t)}{\\Delta{}t(N_0-n(t))}\\] 单位:\\(1Fit=10^{-9}/h\\) \\(R(t)=\\exp\\{-\\int_0^t\\lambda(t)dt\\}\\) \\(f(t)=\\lambda(t)\\exp\\{-\\int_0^t\\lambda(t)dt\\}\\) 浴盆曲线 大多数产品的失效率随时间的变化曲线形似浴盆 由于产品失效机理的不同，失效率随时间的变化大致可以分为三个阶段：早期故障、偶然故障和耗损故障 平均寿命 平均故障前时间(MTTF, Mean Time To Failure) 设\\(n\\)个不可修复产品在同样条件下进行试验，测得其全部故障时间为\\(t_1,t_2,...,t_n\\)，则其平均故障前时间为: \\[T_{TF}=\\frac{1}{n}\\sum_{i=1}^nt_i\\] 平均故障间隔时间(MTBF, Mean Time Between Failure) 设\\(1\\)个可修复产品在使用过程中发生了\\(n\\)次故障，每次故障修复后又重新投入使用，测得其每次工作持续时间为\\(t_1,t_2,...,t_n\\)，则其平均故障间隔时间为： \\[T_{BF}=\\frac{1}{n}\\sum_{i=1}^nt_i\\] 平均寿命 \\[\\theta=\\int_0^\\infty{}tf(t)dt=\\int_0^\\infty{}R(t)dt\\] 可靠寿命 \\[t_R=R^{-1}(r)\\] 常用的可靠性定量指标 指数分布:\\(\\lambda=const\\) \\(R(t)=\\exp\\{-\\lambda{}t\\}\\) \\(f(t)=\\lambda\\exp\\{-\\lambda{}t\\}\\) \\(\\theta=MTTF=\\lambda^{-1}\\) 常用分布 对数正态分布 \\(T\\)的对数\\(\\ln{}T\\)服从正态分布，则\\(T\\)服从对数正态分布，即\\(X=\\ln{}T\\sim{}N(\\mu,\\sigma^2)\\) \\[f(t)=\\frac{1}{\\sigma{}t\\sqrt{2\\pi}}\\exp\\{-\\frac{(\\ln{}t-\\mu)^2}{2\\sigma^2}\\}\\] 均值:\\(E(T)=\\exp\\{\\mu+\\frac{\\sigma^2}{2}\\}\\) 方差:\\(D(T)=\\exp\\{2(\\mu+\\frac{\\sigma^2}{2})\\}(e^{\\sigma^2}-1)\\) 威布尔分布 \\[f(t)=\\frac{m}{t_0}(t-\\gamma)^{m-1}\\exp\\{-(t-\\gamma)\\frac{m}{t_0}\\}\\] 均值:\\(E(T)=\\gamma+t_0^{\\frac{1}{m}}\\Gamma(1+\\frac{1}{m})\\) 方差:\\(D(T)=t_0^{\\frac{2}{m}}[\\Gamma(1+\\frac{2}{m})-\\Gamma^2(1+\\frac{1}{m})]\\) 参数估计基础 截尾实验 无替换定数截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到出现事先指定的\\(r\\)个产品失效停止试验。 总试验时间:\\[T_r=\\sum_{i=1}^rt_i(n-r)t_r\\] 点估计:\\(\\lambda=r/T_r\\) 区间估计:\\(2\\lambda{}T_r\\sim{}\\chi^2(2r)\\) 有替换定数截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到规定失效数\\(r\\)时停止试验。\\(r\\)个失效样品的失效时间记为:\\(t_1\\leq{}t_2\\leq\\dots\\leq{}t_r\\) 总试验时间:\\(T_r=nt_r\\) 无替换定时截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到出现事先规定的时间\\(t_0\\)停止试验，共有\\(r\\)个失效 总试验时间:\\[T_0=\\sum_{i=1}^rt_i(n-r)t_0\\] 点估计:\\(\\lambda=r/T_0\\) 有替换定时截尾试验 随机抽取\\(n\\)件产品进行寿命试验，试验到出现事先规定的时间\\(t_0\\)停止试验，共有\\(r\\)个失效 总试验时间:\\(T_0=nt_0\\) 贝叶斯估计简介 略"},{"title":"无限阶段折扣问题","slug":"无限阶段折扣问题","date":"un44fin44","updated":"un00fin00","comments":true,"path":"2019/01/24/无限阶段折扣问题/","link":"","permalink":"https://zs-liu.github.io/2019/01/24/%E6%97%A0%E9%99%90%E9%98%B6%E6%AE%B5%E6%8A%98%E6%89%A3%E9%97%AE%E9%A2%98/","excerpt":"引言——成本总和的最小化 无限阶段问题的成本总和 定义非时变离散时间动态系统： \\[x_{k+1}=f(x_k,u_k,w_k)\\] \\[x_k\\in{}S,u_k\\in{}U(x_k)\\subset{}C,w_k\\sim{}P(\\cdot{}|x_k,u_k)\\]","text":"引言——成本总和的最小化 无限阶段问题的成本总和 定义非时变离散时间动态系统： \\[x_{k+1}=f(x_k,u_k,w_k)\\] \\[x_k\\in{}S,u_k\\in{}U(x_k)\\subset{}C,w_k\\sim{}P(\\cdot{}|x_k,u_k)\\] 给定初始状态\\(x_0\\)，成本函数\\(g(\\cdot,\\cdot,\\cdot):S\\times{}C\\times{}D\\rightarrow\\mathcal{R}\\)和折扣因子\\(\\alpha\\)，现需要找到\\(\\pi=\\{\\mu_0,\\mu_1,...\\},\\mu_k\\in{}U(x_k)\\)，使得如下定义的成本最小： \\[J_\\pi(x_0)=\\lim_{N\\rightarrow{}\\infty}E_{w_k}\\{\\sum_{k=0}^{N-1}\\alpha^kg(x_k,\\mu_k(x_k),w_k)\\}\\] 对于所有合法的\\(\\pi\\)构成的集合记为\\(\\Pi\\)，则记最优成本函数为： \\[J^*(x)=\\min_{\\pi\\in\\Pi}J_\\pi(x)\\quad{}x\\in{}S\\] 如果对于所有初始状态，其最优策略相同，则称该问题为稳定问题，该最优策略为最优稳定策略： \\[J_\\mu(x)=J^*(x)\\] 有限阶段问题的DP算法 对于\\(N\\)个阶段的成本问题，DP算法为： \\[J_{N-k}(x)=\\min_{u\\in{}U(x)}E\\{\\alpha^{N-k}g(x,u,w)+J_{N-k+1}(f(x,u,w))\\}\\] \\[J_N(x)=\\alpha^NJ(x)\\] 重新作如下记号： \\[V_k(x)=\\frac{J_{N-k}(x)}{\\alpha^{N-k}},\\quad{}V_0(x)=J(x)\\] \\[V_{k+1}(x)=\\min_{u\\in{}U(x)}E\\{g(x,u,w)+\\alpha{}V_k(f(x,u,w))\\}\\] 简记与单调性 对于\\(J:S\\rightarrow\\mathcal{R}\\)，采用\\(TJ\\)记对\\(J\\)应用DP算法： \\[(TJ)(x)=\\min_{u\\in{}U(x)}E\\{g(x,u,w)+\\alpha{}J(f(x,u,w))\\}\\] 我们视\\(T\\)为\\(S\\)上函数\\(J\\)到\\(TJ\\)的映射。 同样，有如下简记： \\[(T_\\mu{}J)(x)=E\\{g(x,\\mu(x),w)+\\alpha{}J(f(x,\\mu(x),w))\\}\\] \\[(T^kJ)(x)=(T(T^{k-1}J))(x),\\quad{}(T^0J)(x)=J(x)\\] 引理 1.1.1: 单调性引理 对于任何函数\\(J:S\\rightarrow{}\\mathcal{R}\\)和\\(J^{&#39;}:S\\rightarrow\\mathcal{R}\\) \\[J(x)\\leq{}J^{&#39;}(x)\\quad{}\\text{for all }x\\in{}S\\] 对于任何稳定策略\\(\\mu:S\\rightarrow{}C\\)： \\[(T^kJ)(x)\\leq{}(T^kJ^{&#39;})(x)\\quad{}\\text{for all }x\\in{}S,k=1,2,...,\\] \\[(T_\\mu^kJ)(x)\\leq{}(T_\\mu^kJ^{&#39;})(x)\\quad{}\\text{for all }x\\in{}S,k=1,2,...,\\] 引理 1.1.2 定义单位函数为\\(e(x)\\equiv1(\\forall{}x\\in{}S)\\)，则对于任何函数\\(J:S\\rightarrow{}\\mathcal{R}\\)，稳定策略\\(\\mu:S\\rightarrow{}C\\)，标量\\(r\\)： \\[(T^k(J+re))(x)=(T^kJ)(x)+\\alpha^kr\\quad{}\\text{for all }x\\in{}S\\] \\[(T_\\mu^k(J+re))(x)=(T_\\mu^kJ)(x)+\\alpha^kr\\quad{}\\text{for all }x\\in{}S\\] 随机的过去依赖策略 命题 1.1.1： Markov策略的合理性 假设策略集合是可数的，初始状态是可数集合上的分布。则每对\\((x_k,u_k)\\)以及与随机的过去依赖策略相对应的每阶段的期望成本，同样可以以随机Markov策略获得。 有界的阶段成本的折扣问题 假设D： 折扣成本-有界的阶段成本 存在标量\\(M\\)使阶段成本\\(g\\)满足 \\[|g(x,u,w)|\\leq{}M,\\quad\\text{for all }(x,u,w)\\in{}S\\times{}C\\times{}D\\] 命题 1.2.1： DP算法的收敛性 对于有界函数\\(J:S\\rightarrow{}\\mathcal{R}\\)，最优成本函数满足： \\[J^*(x)=\\lim_{N\\rightarrow\\infty}(T^NJ)(x)\\quad\\text{for all }x\\in{}S\\] 推论 1.2.1.1 对于稳定策略\\(\\mu\\)，相关联的成本函数满足： \\[J_{\\mu}(x)=\\lim_{N\\rightarrow\\infty}(T_{\\mu}^NJ)(x)\\quad\\text{for all }x\\in{}S\\] 命题 1.2.2：贝尔曼方程 最优成本函数满足： \\[J^*=TJ^*\\] 此外，\\(J^*\\)也是有界成本函数中满足这一方程的唯一解 推论 1.2.2.1 对于稳定策略\\(\\mu\\)，相关联的成本函数满足： \\[J_{\\mu}=T_{\\mu}J_{\\mu}\\] 此外，\\(J_{\\mu}\\)也是有界成本函数中满足这一方程的唯一解 命题 1.2.3：最优的充要条件 稳定策略\\(\\mu\\)是最优的当且仅当对于\\(\\forall{}x\\in{}S\\)，\\(\\mu(x)\\)取得了贝尔曼方程的最小值，即： \\[TJ^*=T_{\\mu}J^*\\] 命题 1.2.4 对于任意两个有界函数\\(J:S\\rightarrow{}\\mathcal{R},J^{&#39;}:S\\rightarrow{}\\mathcal{R}\\)，有： \\[(\\forall{}k)~\\max_{x\\in{}S}|(T^kJ(x)-(T^kJ^{&#39;})(x))|\\leq\\alpha^k\\max_{x\\in{}S}|J(x)-J^{&#39;}(x)|\\]"},{"title":"$\\rm \\LaTeX$ Manual","slug":"Latex Manual","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2019/01/22/Latex Manual/","link":"","permalink":"https://zs-liu.github.io/2019/01/22/Latex%20Manual/","excerpt":"Text","text":"Text Figure Two Column Figure 12345678\\begin&#123;figure&#125;[htbp] \\begin&#123;minipage&#125;[t]&#123;0.5\\linewidth&#125; \\includegraphics[]&#123;&#125; \\end&#123;minipage&#125; \\begin&#123;minipage&#125;[t]&#123;0.5\\linewidth&#125; \\includegraphics[]&#123;&#125; \\end&#123;minipage&#125;\\end&#123;figure&#125; Table Cross Row Table 12345678910\\usepackage&#123;multirow&#125;\\begin&#123;table&#125;[htbp] \\centering \\begin&#123;tabular&#125;&#123;|c|c|&#125; \\hline \\multirow&#123;2&#125;&#123;*&#125;&#123;Cross It&#125;&amp;N\\\\ &amp;N\\\\ \\hline \\end&#123;tabular&#125;\\end&#123;table&#125; Newline in Table 123456789\\usepackage&#123;makecell&#125;\\begin&#123;table&#125;[htbp] \\centering \\begin&#123;tabular&#125;&#123;c&#125; \\hline \\makecell&#123;First Line\\\\Second Line&#125;\\\\ \\hline \\end&#123;tabular&#125;\\end&#123;table&#125; Equation Un-Italic Code Display $x+y$ \\(x+y\\) $\\rm x+y$ \\(\\rm x+y\\) $\\text&#123;x+y&#125;$ \\(\\text{x+y}\\) Others Two Column Document 1234567\\documentclass[UTF8,a4paper,14pt,twocolumn]&#123;ctexart&#125;\\begin&#123;document&#125; \\twocolumn[ \\begin&#123;@twocolumnfalse&#125; % something here may cross two column \\end&#123;@twocolumnfalse&#125;]\\end&#123;document&#125; Hyperlink 123\\usepackage[colorlinks,linkcolor=blue]&#123;hyperref&#125;% include this package as the final one\\href&#123;https://www.google.com&#125;&#123;Google&#125; Code Display 1234567891011121314\\usepackage&#123;listings&#125;\\lstset&#123;numbers=left, numberstyle=\\tiny, basicstyle=\\small\\ttfamily, stringstyle=\\color&#123;purple&#125;, keywordstyle=\\color&#123;red!25!green!33!blue!100&#125;\\bfseries, commentstyle=\\color&#123;olive&#125;, %directivestyle=\\color&#123;blue&#125;, frame=shadowbox, framerule=0pt, backgroundcolor=\\color&#123;red!0.5!green!0.5!blue!0.5!&#125;, rulesepcolor=\\color&#123;red!20!green!20!blue!20&#125;&#125;\\begin&#123;lstlisting&#125;[language=&#123;[ANSI]C&#125;]\\end&#123;lstlisting&#125; or a more convenient way: 123456\\usepackage&#123;listings&#125;\\usepackage[cache=false]&#123;minted&#125;\\begin&#123;minted&#125;[frame=single, tabsize=4]&#123;js&#125;print(&quot;\\n&quot;)\\end&#123;minted&#125;\\mintinline&#123;python&#125;&#123;print(&quot;\\n&quot;)&#125; Tight List 1\\providecommand&#123;\\tightlist&#125;&#123;%\\setlength&#123;\\itemsep&#125;&#123;0pt&#125;\\setlength&#123;\\parskip&#125;&#123;0pt&#125;&#125; Insert Author Info 12345\\usepackage[colorlinks,linkcolor=blue]&#123;hyperref&#125;\\hypersetup&#123; pdftitle=&#123;None&#125;, pdfauthor=&#123;Nobody&#125;&#125;"},{"title":"Hello World","slug":"hello-world","date":"un22fin22","updated":"un44fin44","comments":true,"path":"2019/01/22/hello-world/","link":"","permalink":"https://zs-liu.github.io/2019/01/22/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment"}]}